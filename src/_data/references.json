{
  "elhage2022toy": {
    "title": "Toy Models of Superposition",
    "authors": "Elhage, N., Hume, T., Olsson, C., et al.",
    "year": 2022,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2022/toy_model/index.html"
  },
  "olsson2022context": {
    "title": "In-context Learning and Induction Heads",
    "authors": "Olsson, C., Elhage, N., Nanda, N., et al.",
    "year": 2022,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"
  },
  "elhage2021mathematical": {
    "title": "A Mathematical Framework for Transformer Circuits",
    "authors": "Elhage, N., Nanda, N., Olsson, C., et al.",
    "year": 2021,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2021/framework/index.html"
  },
  "bricken2023monosemanticity": {
    "title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning",
    "authors": "Bricken, T., Templeton, A., Batson, J., et al.",
    "year": 2023,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
  },
  "conmy2023ioi": {
    "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
    "authors": "Conmy, A., Mavor-Parker, A. N., Lynch, A., et al.",
    "year": 2023,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2304.14997"
  },
  "vaswani2017attention": {
    "title": "Attention Is All You Need",
    "authors": "Vaswani, A., Shazeer, N., Parmar, N., et al.",
    "year": 2017,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/1706.03762"
  },
  "alammar2018illustrated": {
    "title": "The Illustrated Transformer",
    "authors": "Alammar, J.",
    "year": 2018,
    "venue": "Blog post",
    "url": "https://jalammar.github.io/illustrated-transformer/"
  },
  "heimersheim2024patching": {
    "title": "How to Use and Interpret Activation Patching",
    "authors": "Heimersheim, S., Nanda, N.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2404.15255"
  },
  "nanda2023attribution": {
    "title": "Attribution Patching: Activation Patching At Industrial Scale Using Differential Calculus",
    "authors": "Nanda, N.",
    "year": 2023,
    "venue": "Blog post",
    "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching"
  },
  "wang2022ioi": {
    "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small",
    "authors": "Wang, K., Variengien, A., Conmy, A., et al.",
    "year": 2022,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2211.00593"
  },
  "olah2020zoom": {
    "title": "Zoom In: An Introduction to Circuits",
    "authors": "Olah, C., Cammarata, N., Schubert, L., et al.",
    "year": 2020,
    "venue": "Distill",
    "url": "https://distill.pub/2020/circuits/zoom-in/"
  },
  "bereska2024review": {
    "title": "Mechanistic Interpretability for AI Safety -- A Review",
    "authors": "Bereska, L., Gavves, E.",
    "year": 2024,
    "venue": "TMLR",
    "url": "https://arxiv.org/abs/2404.14082"
  },
  "nostalgebraist2020logitlens": {
    "title": "interpreting GPT: the logit lens",
    "authors": "nostalgebraist",
    "year": 2020,
    "venue": "Blog post",
    "url": "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"
  },
  "belrose2023tunedlens": {
    "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens",
    "authors": "Belrose, N., Ostrovsky, I., McKinney, L., et al.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2303.08112"
  },
  "hewitt2019structural": {
    "title": "A Structural Probe for Finding Syntax in Word Representations",
    "authors": "Hewitt, J., Manning, C. D.",
    "year": 2019,
    "venue": "NAACL",
    "url": "https://aclanthology.org/N19-1419/"
  },
  "voita2020mdl": {
    "title": "Information-Theoretic Probing with Minimum Description Length",
    "authors": "Voita, E., Titov, I.",
    "year": 2020,
    "venue": "EMNLP",
    "url": "https://arxiv.org/abs/2003.12298"
  },
  "elazar2021amnesic": {
    "title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
    "authors": "Elazar, Y., Ravfogel, S., Jacovi, A., Goldberg, Y.",
    "year": 2021,
    "venue": "TACL",
    "url": "https://aclanthology.org/2021.tacl-1.10/"
  },
  "belinkov2022probing": {
    "title": "Probing Classifiers: Promises, Shortcomings, and Advances",
    "authors": "Belinkov, Y.",
    "year": 2022,
    "venue": "Computational Linguistics",
    "url": "https://aclanthology.org/2022.cl-1.7/"
  },
  "chan2022causalscrubbing": {
    "title": "Causal Scrubbing: A Method for Rigorously Testing Interpretability Hypotheses",
    "authors": "Chan, L., Garriga-Alonso, A., Goldowsky-Dill, N., et al.",
    "year": 2022,
    "venue": "Redwood Research",
    "url": "https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing"
  },
  "templeton2024scaling": {
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "authors": "Templeton, A., Conerly, T., Marcus, J., et al.",
    "year": 2024,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
  },
  "rajamanoharan2024gated": {
    "title": "Improving Dictionary Learning with Gated Sparse Autoencoders",
    "authors": "Rajamanoharan, S., Conmy, A., Smith, L., et al.",
    "year": 2024,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2404.16014"
  },
  "gao2024scaling": {
    "title": "Scaling and Evaluating Sparse Autoencoders",
    "authors": "Gao, L., Dupre la Tour, T., Tillman, H., et al.",
    "year": 2024,
    "venue": "ICLR 2025",
    "url": "https://arxiv.org/abs/2406.04093"
  },
  "rajamanoharan2024jumprelu": {
    "title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
    "authors": "Rajamanoharan, S., Lieberum, T., Sonnerat, N., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2407.14435"
  },
  "karvonen2025saebench": {
    "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
    "authors": "Karvonen, A., Rager, C., Lin, J., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2503.09532"
  },
  "chanin2024absorption": {
    "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
    "authors": "Chanin, D., Wilken-Smith, J., Dulka, T., et al.",
    "year": 2024,
    "venue": "NeurIPS Workshop",
    "url": "https://arxiv.org/abs/2409.14507"
  },
  "turner2024steering": {
    "title": "Steering Language Models With Activation Engineering",
    "authors": "Turner, A. M., Thiergart, L., Leech, G., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2308.10248"
  },
  "panickssery2024caa": {
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "authors": "Panickssery, N., Gabrieli, N., Schulz, J., et al.",
    "year": 2024,
    "venue": "ACL",
    "url": "https://arxiv.org/abs/2312.06681"
  },
  "zou2023repe": {
    "title": "Representation Engineering: A Top-Down Approach to AI Transparency",
    "authors": "Zou, A., Phan, L., Chen, S., et al.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2310.01405"
  },
  "arditi2024refusal": {
    "title": "Refusal in Language Models Is Mediated by a Single Direction",
    "authors": "Arditi, A., Obeso, O., Syed, A., et al.",
    "year": 2024,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2406.11717"
  },
  "todd2024function": {
    "title": "Function Vectors in Large Language Models",
    "authors": "Todd, E., Li, M. L., Sharma, A. S., et al.",
    "year": 2024,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2310.15213"
  },
  "belrose2023leace": {
    "title": "LEACE: Perfect Linear Concept Erasure in Closed Form",
    "authors": "Belrose, N., Schneider-Joseph, D., Ravfogel, S., et al.",
    "year": 2023,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2306.03819"
  },
  "lindsey2025circuittracing": {
    "title": "Circuit Tracing: Revealing Computational Graphs in Language Models",
    "authors": "Lindsey, J., Batson, J., Denison, C., et al.",
    "year": 2025,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/methods.html"
  },
  "anthropic2025biology": {
    "title": "On the Biology of a Large Language Model",
    "authors": "Anthropic",
    "year": 2025,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html"
  },
  "dunefsky2024transcoders": {
    "title": "Transcoders Find Interpretable LLM Feature Circuits",
    "authors": "Dunefsky, J., Chlenski, P., Nanda, N.",
    "year": 2024,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2406.11944"
  },
  "marks2024sparse": {
    "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
    "authors": "Marks, S., Rager, C., Michaud, E. J., et al.",
    "year": 2024,
    "venue": "ICLR 2025",
    "url": "https://arxiv.org/abs/2403.19647"
  },
  "gurnee2024universal": {
    "title": "Universal Neurons in GPT2 Language Models",
    "authors": "Gurnee, W., Horsley, T., Guo, Z. C., et al.",
    "year": 2024,
    "venue": "TMLR",
    "url": "https://arxiv.org/abs/2401.12181"
  },
  "lin2025multimodal": {
    "title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models",
    "authors": "Lin, Z., Basu, S., Beigi, M., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2502.17516"
  },
  "hubinger2024sleeper": {
    "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
    "authors": "Hubinger, E., Denison, C., Mu, J., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2401.05566"
  },
  "greenblatt2024alignment": {
    "title": "Alignment Faking in Large Language Models",
    "authors": "Greenblatt, R., Denison, C., Wright, B., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2412.14093"
  },
  "anthropic2024probes": {
    "title": "Simple Probes Can Catch Sleeper Agents",
    "authors": "Anthropic",
    "year": 2024,
    "venue": "Anthropic Research Blog",
    "url": "https://www.anthropic.com/research/probes-catch-sleeper-agents"
  },
  "nanda2022openproblems": {
    "title": "200 Concrete Open Problems in Mechanistic Interpretability",
    "authors": "Nanda, N.",
    "year": 2022,
    "venue": "Alignment Forum",
    "url": "https://www.alignmentforum.org/s/yivyHaCAmMJ3CqSyj"
  },
  "sharkey2025openproblems": {
    "title": "Open Problems in Mechanistic Interpretability",
    "authors": "Sharkey, L., Chughtai, B., Batson, J., et al.",
    "year": 2025,
    "venue": "TMLR",
    "url": "https://arxiv.org/abs/2501.16496"
  },
  "rai2024practical": {
    "title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models",
    "authors": "Rai, D., Zhou, Y., Feng, S., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2407.02646"
  },
  "park2023lrh": {
    "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
    "authors": "Park, K., Choe, Y. J., Veitch, V.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2311.03658"
  },
  "ghandeharioun2024patchscopes": {
    "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
    "authors": "Ghandeharioun, A., Caciularu, A., Pearce, A., Dixon, L., Geva, M.",
    "year": 2024,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2401.06102"
  },
  "chen2024selfie": {
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "authors": "Chen, H., Vondrick, C., Mao, C.",
    "year": 2024,
    "venue": "ACL",
    "url": "https://arxiv.org/abs/2403.10949"
  },
  "li2025training": {
    "title": "Training Language Models to Explain Their Own Computations",
    "authors": "Li, B. Z., Guo, Z. C., Huang, V., Steinhardt, J., Andreas, J.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2511.08579"
  },
  "pan2024latentqa": {
    "title": "LatentQA: Teaching LLMs to Decode Activations Into Natural Language",
    "authors": "Pan, A., Chen, L., Steinhardt, J.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2412.08686"
  },
  "karvonen2025activationoracles": {
    "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
    "authors": "Karvonen, A., Chua, J., Dumas, C., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2512.15674"
  },
  "fiotto2024nnsight": {
    "title": "NNsight and NDIF: Democratizing Access to Foundation Model Internals",
    "authors": "Fiotto-Kaufman, J., Loftus, A., Todd, E., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2407.14561"
  },
  "dumas2024nnterp": {
    "title": "nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers",
    "authors": "Dumas, C.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2511.14465"
  },
  "park2024geometry": {
    "title": "The Geometry of Categorical and Hierarchical Concepts in Large Language Models",
    "authors": "Park, K., Choe, Y. J., Jiang, Y., Veitch, V.",
    "year": 2024,
    "venue": "ICLR 2025",
    "url": "https://arxiv.org/abs/2406.01506"
  },
  "engels2024multidimensional": {
    "title": "Not All Language Model Features Are One-Dimensionally Linear",
    "authors": "Engels, J., Michaud, E. J., Liao, I., Gurnee, W., Tegmark, M.",
    "year": 2024,
    "venue": "ICLR 2025",
    "url": "https://arxiv.org/abs/2405.14860"
  },
  "shabalin2025attention": {
    "title": "Attention Probes",
    "authors": "Shabalin, S., Belrose, N.",
    "year": 2025,
    "venue": "EleutherAI Blog",
    "url": "https://blog.eleuther.ai/attention-probes/"
  },
  "mckenzie2025probes": {
    "title": "Detecting High-Stakes Interactions with Activation Probes",
    "authors": "McKenzie, A., Pawar, U., Blandfort, P., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2506.10805"
  },
  "marshall2024ace": {
    "title": "Refusal in LLMs is an Affine Function",
    "authors": "Marshall, T., Scherlis, A., Belrose, N.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2411.09003"
  },
  "kramar2024atp": {
    "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
    "authors": "Kramár, J., Lieberum, T., Shah, R., Nanda, N.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2403.00745"
  },
  "syed2023eap": {
    "title": "Attribution Patching Outperforms Automated Circuit Discovery",
    "authors": "Syed, A., Rager, C., Conmy, A.",
    "year": 2023,
    "venue": "BlackboxNLP 2024",
    "url": "https://arxiv.org/abs/2310.10348"
  },
  "hanna2024faithfulness": {
    "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms",
    "authors": "Hanna, M., Pezzelle, S., Belinkov, Y.",
    "year": 2024,
    "venue": "COLM 2024",
    "url": "https://arxiv.org/abs/2403.17806"
  },
  "zhang2025eapgp": {
    "title": "EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification",
    "authors": "Zhang, L., Dong, W., Zhang, Z., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2502.06852"
  },
  "aranguri2025lda": {
    "title": "Discovering Undesired Rare Behaviors via Model Diff Amplification",
    "authors": "Aranguri, S., McGrath, T.",
    "year": 2025,
    "venue": "Goodfire",
    "url": "https://www.goodfire.ai/research/model-diff-amplification"
  },
  "betley2025emergent": {
    "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "authors": "Betley, J., Tan, D., Chrabąszcz, A., et al.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2502.17424"
  },
  "mack2024melbo": {
    "title": "Mechanistically Eliciting Latent Behaviors in Language Models",
    "authors": "Mack, A., Turner, A.",
    "year": 2024,
    "venue": "Alignment Forum",
    "url": "https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1"
  },
  "bloom2024saelens": {
    "title": "SAELens: Training Sparse Autoencoders on Language Models",
    "authors": "Bloom, J., Chanin, D., Tigges, C., Duong, A.",
    "year": 2024,
    "venue": "GitHub",
    "url": "https://github.com/jbloomAus/SAELens"
  },
  "lieberum2024gemma": {
    "title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2",
    "authors": "Lieberum, T., Rajamanoharan, S., Conmy, A., et al.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2408.05147"
  },
  "mcgrath2023hydra": {
    "title": "The Hydra Effect: Emergent Self-repair in Language Model Computations",
    "authors": "McGrath, T., Rahtz, M., Kramar, J., et al.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2307.15771"
  },
  "rushing2024selfrepair": {
    "title": "Explorations of Self-Repair in Language Models",
    "authors": "Rushing, C., Nanda, N.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2402.15390"
  },
  "mcdougall2023copy": {
    "title": "Copy Suppression: Comprehensively Understanding an Attention Head",
    "authors": "McDougall, C., Conmy, A., Rushing, C., et al.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2310.04625"
  },
  "meng2022rome": {
    "title": "Locating and Editing Factual Associations in GPT",
    "authors": "Meng, K., Bau, D., Mitchell, A., Finn, C.",
    "year": 2022,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2202.05262"
  },
  "meng2023memit": {
    "title": "Mass-Editing Memory in a Transformer",
    "authors": "Meng, K., Sharma, A., Andonian, A., et al.",
    "year": 2023,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2210.07229"
  },
  "hase2024localization": {
    "title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models",
    "authors": "Hase, P., Bansal, M., Kim, B., Ghandeharioun, A.",
    "year": 2024,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2301.04213"
  },
  "nanda2023othello": {
    "title": "Actually, Othello-GPT Has A Linear Emergent World Representation",
    "authors": "Nanda, N., Lee, A., Berber, M.",
    "year": 2023,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2309.00941"
  },
  "li2023othello": {
    "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
    "authors": "Li, K., Hopkins, A. K., Bau, D., et al.",
    "year": 2023,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2210.13382"
  },
  "geva2021kvmemories": {
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "authors": "Geva, M., Schuster, R., Berant, J., Levy, O.",
    "year": 2021,
    "venue": "EMNLP",
    "url": "https://arxiv.org/abs/2012.14913"
  },
  "geva2022concepts": {
    "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
    "authors": "Geva, M., Caciularu, A., Wang, K., Goldberg, Y.",
    "year": 2022,
    "venue": "EMNLP",
    "url": "https://arxiv.org/abs/2203.14680"
  },
  "geva2023factual": {
    "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    "authors": "Geva, M., Bastings, J., Filippova, K., Globerson, A.",
    "year": 2023,
    "venue": "EMNLP",
    "url": "https://arxiv.org/abs/2304.14767"
  },
  "dai2022knowledge": {
    "title": "Knowledge Neurons in Pretrained Transformers",
    "authors": "Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., Wei, F.",
    "year": 2022,
    "venue": "ACL",
    "url": "https://arxiv.org/abs/2104.08696"
  },
  "zhang2022moefication": {
    "title": "MoEfication: Transformer Feed-forward Layers Are Mixtures of Experts",
    "authors": "Zhang, Z., Lin, Y., Liu, Z., et al.",
    "year": 2022,
    "venue": "ACL Findings",
    "url": "https://arxiv.org/abs/2110.01786"
  },
  "bogdan2025thoughtanchors": {
    "title": "Thought Anchors: Which LLM Reasoning Steps Matter?",
    "authors": "Bogdan, P. C., Macar, U., Nanda, N., Conmy, A.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2506.19143"
  },
  "macar2025thoughtbranches": {
    "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
    "authors": "Macar, U., Bogdan, P. C., Rajamanoharan, S., Nanda, N.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2510.27484"
  },
  "geiger2021causal": {
    "title": "Causal Abstractions of Neural Networks",
    "authors": "Geiger, A., Lu, H., Icard, T., Potts, C.",
    "year": 2021,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2106.02997"
  },
  "geiger2023causal": {
    "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
    "authors": "Geiger, A., Ibeling, D., Zur, A., Chaudhary, M., Chauhan, S., Huang, J., Arora, A., Wu, Z., Goodman, N., Potts, C., Icard, T.",
    "year": 2025,
    "venue": "JMLR",
    "url": "https://arxiv.org/abs/2301.04709"
  },
  "wu2023boundlessdas": {
    "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
    "authors": "Wu, Z., Geiger, A., Icard, T., Potts, C., Goodman, N.",
    "year": 2023,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2305.08809"
  },
  "wu2024reft": {
    "title": "ReFT: Representation Finetuning for Language Models",
    "authors": "Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., Potts, C.",
    "year": 2024,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2404.03592"
  },
  "burns2022ccs": {
    "title": "Discovering Latent Knowledge in Language Models Without Supervision",
    "authors": "Burns, C., Ye, H., Klein, D., Steinhardt, J.",
    "year": 2023,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2212.03827"
  },
  "marks2023geometry": {
    "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
    "authors": "Marks, S., Tegmark, M.",
    "year": 2024,
    "venue": "COLM",
    "url": "https://arxiv.org/abs/2310.06824"
  },
  "li2023iti": {
    "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
    "authors": "Li, K., Patel, O., Viégas, F., Pfister, H., Wattenberg, M.",
    "year": 2023,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2306.03341"
  },
  "kossen2024entropy": {
    "title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs",
    "authors": "Kossen, J., Gal, Y., Rainforth, T.",
    "year": 2024,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2406.15927"
  },
  "levinstein2023liedetector": {
    "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
    "authors": "Levinstein, B. A., Herrmann, D. A.",
    "year": 2024,
    "venue": "Philosophical Studies",
    "url": "https://arxiv.org/abs/2307.00175"
  },
  "zhang2024bestpractices": {
    "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
    "authors": "Zhang, F., Nanda, N.",
    "year": 2024,
    "venue": "ICLR",
    "url": "https://arxiv.org/abs/2309.16042"
  },
  "gurnee2023neurons": {
    "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
    "authors": "Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., Bertsimas, D.",
    "year": 2023,
    "venue": "TMLR",
    "url": "https://arxiv.org/abs/2305.01610"
  },
  "wu2025axbench": {
    "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "authors": "Wu, Z., Arora, A., Geiger, A., et al.",
    "year": 2025,
    "venue": "ICML",
    "url": "https://arxiv.org/abs/2501.17148"
  },
  "kantamneni2025saesuseful": {
    "title": "Are Sparse Autoencoders Useful? A Case Study in Sparse Probing",
    "authors": "Kantamneni, S., Engels, J., Rajamanoharan, S., Tegmark, M., Nanda, N.",
    "year": 2025,
    "venue": "ICML",
    "url": "https://arxiv.org/abs/2502.16681"
  },
  "li2023dola": {
    "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
    "authors": "Li, Y., Lin, Z., Zhang, S., et al.",
    "year": 2023,
    "venue": "ICLR 2024",
    "url": "https://arxiv.org/abs/2309.03883"
  },
  "wendler2024latent": {
    "title": "Do Llamas Work in English? On the Latent Language of Multilingual Transformers",
    "authors": "Wendler, C., Veselovsky, V., Monea, G., West, R.",
    "year": 2024,
    "venue": "ACL",
    "url": "https://arxiv.org/abs/2402.10588"
  },
  "hernandez2023lre": {
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "authors": "Hernandez, E., Li, B. Z., Andreas, J.",
    "year": 2023,
    "venue": "ICLR 2024",
    "url": "https://arxiv.org/abs/2308.09124"
  },
  "liu2023incontext": {
    "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
    "authors": "Liu, Z., Kitouni, A., Nolte, N., et al.",
    "year": 2023,
    "venue": "ICML 2024",
    "url": "https://arxiv.org/abs/2311.06668"
  },
  "hendel2023icl": {
    "title": "In-Context Learning Creates Task Vectors",
    "authors": "Hendel, R., Geva, M., Globerson, A.",
    "year": 2023,
    "venue": "EMNLP Findings",
    "url": "https://arxiv.org/abs/2310.15916"
  },
  "allenzhu2023physics": {
    "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
    "authors": "Allen-Zhu, Z., Li, Y.",
    "year": 2023,
    "venue": "ICML 2024",
    "url": "https://arxiv.org/abs/2309.14316"
  },
  "weiss2021rasp": {
    "title": "Thinking Like Transformers",
    "authors": "Weiss, G., Goldberg, Y., Yahav, E.",
    "year": 2021,
    "venue": "ICML",
    "url": "https://arxiv.org/abs/2106.06981"
  },
  "yang2024multihop": {
    "title": "Large Language Models Internally Perform Multi-hop Factual Reasoning",
    "authors": "Yang, J., Ding, N., Li, Y., et al.",
    "year": 2024,
    "venue": "ACL",
    "url": "https://arxiv.org/abs/2402.16837"
  },
  "minder2025crosscoders": {
    "title": "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning",
    "authors": "Minder, J., Dumas, C., Juang, C., Chughtai, B., Nanda, N.",
    "year": 2025,
    "venue": "NeurIPS",
    "url": "https://arxiv.org/abs/2504.02922"
  },
  "minder2025finetuning": {
    "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences",
    "authors": "Minder, J., Dumas, C., Slocum, S., Casademunt, H., Holmes, C., West, R., Nanda, N.",
    "year": 2025,
    "venue": "arXiv",
    "url": "https://arxiv.org/abs/2510.13900"
  }
}
