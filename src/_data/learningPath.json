{
  "blocks": [
    {
      "slug": "transformer-foundations",
      "title": "Transformer Foundations",
      "topics": [
        { "slug": "mi-prerequisites", "title": "Prerequisites" },
        { "slug": "transformer-architecture", "title": "Transformer Architecture Intro" },
        { "slug": "attention-mechanism", "title": "The Attention Mechanism" },
        { "slug": "qk-ov-circuits", "title": "QK and OV Circuits" },
        { "slug": "composition-and-virtual-heads", "title": "Composition and Virtual Attention Heads" }
      ]
    },
    {
      "slug": "foundations-of-mi",
      "title": "Interpretability Fundamentals",
      "topics": [
        { "slug": "what-is-mech-interp", "title": "What is Interpretability?" },
        { "slug": "linear-representation-hypothesis", "title": "The Linear Representation Hypothesis" },
        { "slug": "superposition", "title": "The Superposition Hypothesis" }
      ]
    },
    {
      "slug": "basic-interp-techniques",
      "title": "Basic Interpretability Techniques",
      "topics": [
        { "slug": "logit-lens-and-tuned-lens", "title": "The Logit Lens and Tuned Lens" },
        { "slug": "direct-logit-attribution", "title": "Direct Logit Attribution" },
        { "slug": "reading-attention-patterns", "title": "Reading the Attention Patterns" }
      ]
    },
    {
      "slug": "observation-to-causation",
      "title": "Causal Interventions",
      "topics": [
        { "slug": "activation-patching", "title": "Activation Patching" },
        { "slug": "attribution-patching", "title": "Attribution Patching and Path Patching" }
      ]
    },
    {
      "slug": "probing",
      "title": "Probing",
      "topics": [
        { "slug": "probing-classifiers", "title": "Probing Classifiers" },
        { "slug": "caa-method", "title": "Contrastive Activation Addition (CAA)" },
        { "slug": "lat-probing", "title": "Linear Artificial Tomography (LAT)" }
      ]
    },
    {
      "slug": "steering",
      "title": "Steering",
      "topics": [
        { "slug": "addition-steering", "title": "Addition Steering" },
        { "slug": "ablation-steering", "title": "Ablation Steering" },
        { "slug": "representation-control", "title": "Representation Control" },
        { "slug": "function-vectors", "title": "Function Vectors" }
      ]
    },
    {
      "slug": "model-editing",
      "title": "Model Editing",
      "topics": [
        { "slug": "concept-erasure", "title": "Concept Erasure with LEACE" }
      ]
    },
    {
      "slug": "superposition-and-feature-extraction",
      "title": "Superposition & Feature Extraction",
      "topics": [
        { "slug": "sparse-autoencoders", "title": "Sparse Autoencoders: Decomposing Superposition" },
        { "slug": "sae-interpretability", "title": "Feature Dashboards and Automated Interpretability" },
        { "slug": "scaling-monosemanticity", "title": "Scaling Monosemanticity and Feature Steering" },
        { "slug": "sae-variants-and-evaluation", "title": "SAE Variants, Evaluation, and Honest Limitations" }
      ]
    },
    {
      "slug": "hidden-state-decoding",
      "title": "Hidden State Decoding",
      "topics": [
        { "slug": "hidden-state-decoding-intro", "title": "Hidden State Decoding: From Vectors to Language" },
        { "slug": "patchscopes", "title": "Patchscopes" },
        { "slug": "selfie-interpretation", "title": "SelfIE: Self-Interpretation of Embeddings" },
        { "slug": "training-self-explanation", "title": "Training Models to Explain Their Computations" },
        { "slug": "latentqa", "title": "LatentQA and Latent Interpretation Tuning" },
        { "slug": "activation-oracles", "title": "Activation Oracles" }
      ]
    },
    {
      "slug": "circuit-tracing-and-comparative-mi",
      "title": "Circuit Finding",
      "topics": [
        { "slug": "induction-heads", "title": "Induction Heads and In-Context Learning" },
        { "slug": "ioi-circuit", "title": "The IOI Circuit: Discovery and Mechanism" },
        { "slug": "circuit-evaluation", "title": "Circuit Evaluation: Faithfulness, Completeness, and Minimality" },
        { "slug": "transcoders", "title": "Transcoders: Interpretable MLP Replacements" },
        { "slug": "circuit-tracing", "title": "Circuit Tracing and Attribution Graphs" },
        { "slug": "crosscoders-and-model-diffing", "title": "Crosscoders and Model Diffing" },
        { "slug": "universality", "title": "Universality Across Models" },
        { "slug": "multimodal-mi", "title": "Multimodal Mechanistic Interpretability" }
      ]
    },
    {
      "slug": "mi-for-safety",
      "title": "MI for AI Safety",
      "topics": [
        { "slug": "refusal-direction", "title": "The Refusal Direction" },
        { "slug": "sleeper-agent-detection", "title": "Detecting Sleeper Agents" },
        { "slug": "deception-detection", "title": "Deception Detection and Alignment Faking" },
        { "slug": "safety-mechanisms-and-monitoring", "title": "Safety Mechanisms and MI-Based Monitoring" },
        { "slug": "mi-safety-limitations", "title": "Honest Limitations of MI for Safety" }
      ]
    },
    {
      "slug": "open-problems-and-frontiers",
      "title": "Open Problems & Field Assessment",
      "topics": [
        { "slug": "open-problems-methods", "title": "Open Problems: Methods and Research Questions" },
        { "slug": "field-assessment", "title": "What MI Can and Cannot Do" },
        { "slug": "future-directions", "title": "The Future of Mechanistic Interpretability" },
        { "slug": "course-synthesis", "title": "Course Synthesis" }
      ]
    }
  ]
}
