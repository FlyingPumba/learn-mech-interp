---
phase: 04-content-authoring-pilot-articles
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - eleventy.config.js
  - src/_includes/layouts/article.njk
  - src/_data/references.json
  - src/topics/attention-mechanism/index.md
autonomous: true

must_haves:
  truths:
    - "The Attention Mechanism article renders as readable long-form prose (not bullet fragments) with proper headings, math, definitions, and pause-and-think prompts"
    - "Front matter includes title, description, prerequisites, difficulty, block, and category fields"
    - "All math expressions (attention equation, softmax, QKV projections, residual stream) render correctly via KaTeX with no red error boxes"
    - "Paper references appear as numbered inline citations with working tooltips and links to source papers"
    - "Images placed in article directories display correctly when referenced by articles"
    - "Prerequisites render in the article header as links"
  artifacts:
    - path: "src/topics/attention-mechanism/index.md"
      provides: "Complete pilot article: The Attention Mechanism"
      min_lines: 150
    - path: "src/_data/references.json"
      provides: "Expanded citation data for all 3 pilot articles"
      contains: "vaswani2017attention"
    - path: "eleventy.config.js"
      provides: "PassthroughCopy for topic images"
      contains: "topics/*/images"
    - path: "src/_includes/layouts/article.njk"
      provides: "Prerequisites rendering in article header"
      contains: "prerequisites"
  key_links:
    - from: "src/topics/attention-mechanism/index.md"
      to: "src/_data/references.json"
      via: "cite shortcode keys"
      pattern: "cite.*vaswani2017"
---

<objective>
Set up the content authoring infrastructure and write the first pilot article: "The Attention Mechanism."

Purpose: Establish the image passthrough, prerequisite rendering, and expanded citation database needed by all pilot articles, then validate the Typst-to-prose conversion workflow on a foundational, math-heavy article with no images.

Output:
- Updated eleventy.config.js with image passthrough
- Updated article.njk with prerequisite rendering
- Expanded references.json with citations for all 3 pilot articles (~10-15 papers)
- Complete "Attention Mechanism" article as readable long-form Markdown
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-content-authoring-pilot-articles/04-RESEARCH.md
@eleventy.config.js
@src/_includes/layouts/article.njk
@src/_data/references.json
@src/topics/topics.11tydata.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Infrastructure prep and references.json expansion</name>
  <files>
    eleventy.config.js
    src/_includes/layouts/article.njk
    src/_data/references.json
  </files>
  <action>
    **1. Add image passthrough in eleventy.config.js:**
    After the existing `eleventyConfig.addPassthroughCopy("src/css");` line, add:
    ```javascript
    eleventyConfig.addPassthroughCopy("src/topics/*/images");
    ```

    **2. Add prerequisite rendering in article.njk:**
    Update the article header to render prerequisites from front matter. After the description paragraph, add a prerequisites block that renders when `prerequisites` array exists in front matter. Each prerequisite has `title` and `url` fields. Render as a simple line like:
    ```
    Prerequisites: <a href="url1">Title 1</a>, <a href="url2">Title 2</a>
    ```
    Use a `<p class="article-prerequisites">` wrapper. Style with a smaller font size and muted color using existing CSS variables.

    **3. Expand references.json with all citations needed for 3 pilot articles:**
    Keep the existing 5 entries. Add these new entries (use the key format `authorYYYYkeyword`):

    From SOURCES.md, add entries for:
    - `vaswani2017attention`: "Attention Is All You Need", Vaswani et al., 2017, NeurIPS, https://arxiv.org/abs/1706.03762
    - `alammar2018illustrated`: "The Illustrated Transformer", Alammar, J., 2018, Blog post, https://jalammar.github.io/illustrated-transformer/
    - `heimersheim2024patching`: "How to Use and Interpret Activation Patching", Heimersheim & Nanda, 2024, arXiv, https://arxiv.org/abs/2404.15255
    - `nanda2023attribution`: "Attribution Patching: Activation Patching At Industrial Scale Using Differential Calculus", Nanda, N., 2023, Blog post, https://www.neelnanda.io/mechanistic-interpretability/attribution-patching
    - `wang2022ioi`: "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small", Wang et al., 2022, arXiv, https://arxiv.org/abs/2211.00593
    - `olah2020zoom`: "Zoom In: An Introduction to Circuits", Olah et al., 2020, Distill, https://distill.pub/2020/circuits/zoom-in/
    - `conmy2023ioi`: "Towards Automated Circuit Discovery for Mechanistic Interpretability", Conmy et al., 2023, NeurIPS, https://arxiv.org/abs/2304.14997

    That gives us citations for all 3 pilot articles. The existing `elhage2022toy` entry covers superposition. Total: 12 entries.
  </action>
  <verify>
    Run `npx @11ty/eleventy --serve` and confirm the site builds with no errors. Check that references.json is valid JSON with 12 entries.
  </verify>
  <done>
    eleventy.config.js has image passthrough, article.njk renders prerequisites, references.json has 12 entries covering all 3 pilot articles.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write "The Attention Mechanism" pilot article</name>
  <files>
    src/topics/attention-mechanism/index.md
  </files>
  <action>
    Convert the Week 1 Typst source (`/Users/ivan/latex/mech-interp-course/week-01/week-01.typ`) into a readable long-form article. This is EDITORIAL conversion, not mechanical translation.

    **Front matter:**
    ```yaml
    ---
    title: "The Attention Mechanism"
    description: "How transformers enable tokens to communicate through queries, keys, and values, and why the residual stream's additive structure is the foundation of mechanistic interpretability."
    prerequisites: []
    difficulty: "foundational"
    block: "transformer-foundations"
    category: "core-concepts"
    ---
    ```

    **Content structure (h2 sections, since h1 is rendered by layout):**

    1. `## Why Attention?` -- Motivate with the pronoun resolution example ("The cat sat on the mat because it was tired"). Explain why feed-forward networks cannot solve this. 2-3 paragraphs of prose.

    2. `## Queries, Keys, and Values` -- The three roles. Use a blockquote definition. Include the projection equations: $\mathbf{q}_i = \mathbf{x}_i W_Q$, etc. Convert Typst math carefully using the conversion table from research. Explain in prose, not bullets.

    3. `## The Attention Equation` -- Build up from dot-product scores through scaling through softmax to the full equation. Use display math for the full attention formula: $\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$. Explain each step in prose.

    4. `## Self-Attention and Causal Masking` -- Explain self-attention (Q, K, V from same input). Explain causal masking for decoder-only models. Convert the two-column slide into flowing prose.

    5. `## Multi-Head Attention` -- Why multiple heads. The concatenation formula. The independent-heads view. Mention head specialization (previous token heads, induction heads, name movers) with cross-article forward references in plain text (not links, since those articles do not exist yet).

    6. `## The Residual Stream` -- The central communication channel. Definition blockquote. The additive update equation. The whiteboard analogy. Explain why additive structure enables mechanistic interpretability.

    7. `## Other Components` -- Layer normalization, MLPs (the feature processors), positional encodings. Brief treatment.

    8. `## The Full Transformer` -- Putting it all together. The forward pass as additive updates. The key summary equation. Why decoder-only for MI.

    9. Include a notation reference table (Markdown table) converting the Typst table from the source.

    **Content guidelines:**
    - Convert all `#pause` + bullet sequences into flowing prose paragraphs
    - Convert `#definition(title: "X")[...]` to `> **X:** ...` blockquote format
    - Convert `#pause-and-think[...]` to `<details class="pause-and-think"><summary>Pause and think: [topic]</summary>\n\n[content]\n\n</details>`
    - Convert `#focus-slide[...]` to an emphasized paragraph (bold or set off with whitespace)
    - Strip all recap sections (this is article 1, no recaps needed)
    - Drop the "Readings" section (not needed in web article format)
    - Add `{% cite "vaswani2017attention" %}` and `{% cite "alammar2018illustrated" %}` where the source references these papers
    - Use `{% sidenote "..." %}` for supplementary context (e.g., the note about layer norm coupling dimensions, the key-value memory interpretation of MLPs)
    - Convert ALL Typst math to KaTeX LaTeX using the conversion table:
      - `vx` -> `\mathbf{x}`, `vW` -> `\mathbf{W}`, `vq` -> `\mathbf{q}`, `vk` -> `\mathbf{k}`, `vv` -> `\mathbf{v}`
      - `residual` -> `\mathbf{r}`, `RR` -> `\mathbb{R}`, `d_"model"` -> `d_{\text{model}}`
      - `attn(Q, K, V)` -> `\text{Attn}(Q, K, V)`, `softmax` -> `\text{softmax}`
      - `arrow.r` -> `\rightarrow`, `approx` -> `\approx`, `eq.not` -> `\neq`
      - `cal(L)` -> `\mathcal{L}`, `bold("b")` -> `\mathbf{b}`, `partial` -> `\partial`
      - `dot.o` -> `\odot`, `dots.v` -> `\vdots`
      - `embed` -> `\text{Embed}`, `unembed` -> `\text{Unembed}`, `layernorm` -> `\text{LN}`
    - Target ~2000 words of readable prose
    - Do NOT include the Prerequisites Recap section from the Typst source (that is a lecture-only recap)
    - Add forward references as plain text mentions: "We will explore induction heads in a later article" rather than dead links
  </action>
  <verify>
    Run `npx @11ty/eleventy --serve` and open the article at `/topics/attention-mechanism/`. Verify:
    1. The page renders without build errors
    2. All math expressions display correctly (no red KaTeX error boxes)
    3. Citations appear as numbered links with tooltips
    4. Pause-and-think sections are collapsible
    5. The article reads as flowing prose, not bullet-point fragments
    6. Front matter fields are present and the title renders in the header
  </verify>
  <done>
    A complete, readable "Attention Mechanism" article exists at /topics/attention-mechanism/ with correct math, citations, definitions, and pause-and-think prompts. It reads as editorial prose reorganized by theme, not as converted slide bullets.
  </done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds with zero errors
2. `/topics/attention-mechanism/` renders as a complete, readable article
3. All KaTeX math renders without errors (check: attention equation, softmax, QKV projections, residual stream formula)
4. Citation tooltips work on hover for both `vaswani2017attention` and `alammar2018illustrated`
5. references.json has 11 valid entries
6. `eleventyConfig.addPassthroughCopy("src/topics/*/images")` is present in eleventy.config.js
7. article.njk renders prerequisites when present in front matter
</verification>

<success_criteria>
- The Attention Mechanism article is readable long-form prose (~2000 words), not bullet fragments
- All 6 content types from Phase 3 are exercised: math (inline + display), definitions (blockquotes), pause-and-think (details/summary), citations (cite shortcode), at least one sidenote
- The build system correctly uses front matter metadata (title, description rendered by layout)
- Image passthrough is configured for future articles with images
</success_criteria>

<output>
After completion, create `.planning/phases/04-content-authoring-pilot-articles/04-01-SUMMARY.md`
</output>
