---
phase: 04-content-authoring-pilot-articles
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/topics/activation-patching/index.md
  - src/topics/activation-patching/images/act_patch_setup.png
  - src/topics/activation-patching/images/act_patch_layers.png
  - src/topics/activation-patching/images/act_patch_heads.png
autonomous: true

must_haves:
  truths:
    - "The Activation Patching article renders as readable long-form prose covering the core idea, noising vs denoising, attribution patching, path patching, and a worked IOI example"
    - "All 3 course diagrams appear with descriptive alt text and figure captions"
    - "Multiple paper citations render correctly with tooltips (Heimersheim & Nanda, Nanda, Wang et al.)"
    - "Cross-article concept links to the attention-mechanism and superposition articles work"
    - "Front matter includes prerequisites linking to the attention-mechanism article"
  artifacts:
    - path: "src/topics/activation-patching/index.md"
      provides: "Complete pilot article: Activation Patching and Causal Interventions"
      min_lines: 180
    - path: "src/topics/activation-patching/images/act_patch_setup.png"
      provides: "Activation patching setup diagram from course materials"
  key_links:
    - from: "src/topics/activation-patching/index.md"
      to: "src/_data/references.json"
      via: "cite shortcode"
      pattern: "cite.*heimersheim2024"
    - from: "src/topics/activation-patching/index.md"
      to: "/topics/attention-mechanism/"
      via: "cross-article link"
      pattern: "/topics/attention-mechanism/"
    - from: "src/topics/activation-patching/index.md"
      to: "/topics/superposition/"
      via: "cross-article link"
      pattern: "/topics/superposition/"
---

<objective>
Write the third pilot article: "Activation Patching and Causal Interventions." This article validates cross-article linking, multi-citation usage, and the conversion of a methodology-focused advanced article.

Purpose: Complete the pilot article set by converting an advanced, citations-heavy article that naturally references concepts from the other two pilots, testing cross-article concept linking end-to-end.

Output:
- Complete "Activation Patching" article (~2500 words)
- 3 PNG diagrams copied to article-local images directory
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-content-authoring-pilot-articles/04-RESEARCH.md
@src/_data/references.json
@eleventy.config.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Copy diagrams and write the Activation Patching article</name>
  <files>
    src/topics/activation-patching/images/act_patch_setup.png
    src/topics/activation-patching/images/act_patch_layers.png
    src/topics/activation-patching/images/act_patch_heads.png
    src/topics/activation-patching/index.md
  </files>
  <action>
    **Step 1: Copy 3 PNG diagrams from course source:**
    ```bash
    mkdir -p src/topics/activation-patching/images
    cp /Users/ivan/latex/mech-interp-course/week-06/assets/act_patch_setup.png src/topics/activation-patching/images/
    cp /Users/ivan/latex/mech-interp-course/week-06/assets/act_patch_layers.png src/topics/activation-patching/images/
    cp /Users/ivan/latex/mech-interp-course/week-06/assets/act_patch_heads.png src/topics/activation-patching/images/
    ```

    **Step 2: Write the article** by converting Week 6 Typst source (`/Users/ivan/latex/mech-interp-course/week-06/week-06.typ`, 800 lines).

    **Front matter:**
    ```yaml
    ---
    title: "Activation Patching and Causal Interventions"
    description: "The primary technique for establishing causal claims about model internals: replace an activation and measure what changes. Covers noising vs denoising, attribution patching, and path patching."
    prerequisites:
      - title: "The Attention Mechanism"
        url: "/topics/attention-mechanism/"
    difficulty: "advanced"
    block: "observation-to-causation"
    category: "methods"
    ---
    ```

    **Content structure (h2 sections):**

    1. `## From Observation to Causation` -- The shift from observational tools (logit lens, probes, attention patterns) to causal interventions. The key problem: a probe detects information but the model may not rely on it. Reference the correlation vs causation gap. Do NOT duplicate the full recap from Week 5 -- instead, briefly state the problem and link forward. This section motivates the entire article: "Observation reveals what exists. Intervention reveals what matters." Use `{% cite "heimersheim2024patching" %}` as the primary methodological reference.

    2. `## The Clean/Corrupted Framework` -- The basic setup. Concrete IOI example:
       - Clean: "When Mary and John went to the store, John gave a drink to ___" -> Mary
       - Corrupted: swap subjects -> John
       Define activation patching in a blockquote definition. What makes a good clean/corrupted pair. Include the setup diagram:
       ```markdown
       ![Diagram showing the activation patching setup with clean and corrupted model runs side by side, with an arrow indicating activation replacement at a specific layer.](/topics/activation-patching/images/act_patch_setup.png "Figure 1: The activation patching setup. Run the model on both clean and corrupted inputs, then replace specific activations from one run into the other to measure causal effects.")
       ```
       Choosing a metric: logit difference (preferred) vs probability vs accuracy. Math for logit difference: $\Delta L = \text{logit}(\text{Mary}) - \text{logit}(\text{John})$.

    3. `## Noising vs. Denoising` -- The central conceptual distinction. Denoising (clean into corrupted): "Can this component restore correct behavior?" tests sufficiency. Noising (corrupted into clean): "Does damaging this component break behavior?" tests necessity. The AND/OR gate analogy from Heimersheim & Nanda. Why this matters in practice (backup Name Movers example). Include a pause-and-think about redundant components.

       Use `{% sidenote "The distinction between sufficiency and necessity maps directly onto Pearl's causal framework. Denoising identifies sufficient causes while noising identifies necessary causes." %}` or similar.

    4. `## A Worked Example: IOI in GPT-2 Small` -- Walk through the in-class example. Step 1: establish baseline. Step 2: patch layer by layer. Include the layers figure:
       ```markdown
       ![Bar chart showing patching recovery by layer. Layers 0-4 show small effect, layers 5-6 moderate, layers 7-8 large (S-Inhibition heads), layers 9-10 the largest (Name Mover heads).](/topics/activation-patching/images/act_patch_layers.png "Figure 2: Layer-by-layer activation patching results on the IOI task. Layers 7-10 carry the most causally important information.")
       ```
       Step 3: patch individual heads. Include the head heatmap:
       ```markdown
       ![Heatmap showing patching effect for each of 144 attention heads (12 layers x 12 heads). Most cells are near zero. Blue cells at heads 9.9, 10.0, 9.6 show positive effect (Name Movers). Red cells at 10.7, 11.10 show negative effect (Negative Name Movers).](/topics/activation-patching/images/act_patch_heads.png "Figure 3: Head-level activation patching results. The IOI circuit involves roughly 10-15 heads out of 144, revealing a sparse structure.")
       ```
       Interpretation: sparse structure, Name Movers, S-Inhibition heads, Negative Name Movers. Reference `{% cite "wang2022ioi" %}` for the full IOI circuit analysis.

    5. `## Attribution Patching` -- The scalability problem (144 heads in GPT-2, millions of neurons in GPT-3). The gradient approximation formula: $\text{Patch effect of } a_i \approx \nabla_{a_i}\mathcal{L} \cdot (a_i^{\text{clean}} - a_i^{\text{corrupt}})$. The efficiency gain (3 passes vs millions). When it works (small activations, linear regime) and breaks (large activations, nonlinearities). Best practice: screening then verification. Use `{% cite "nanda2023attribution" %}`.

    6. `## Path Patching` -- Beyond component outputs to connections. The key idea: patch only the part of H's output flowing to a specific downstream component K. From nodes to edges. Mention applications (Wang et al. IOI, ACDC algorithm). Use `{% cite "conmy2023ioi" %}` for ACDC. Brief treatment -- this is a teaser for the full IOI circuit article.

    7. `## The Causal Toolkit` -- Brief summary of the three levels (activation patching = nodes, attribution patching = fast screening, path patching = edges). One paragraph.

    **Cross-article linking:**
    - Link to [The Attention Mechanism](/topics/attention-mechanism/) when mentioning the residual stream, attention heads, or the additive structure
    - Link to [The Superposition Hypothesis](/topics/superposition/) when mentioning polysemanticity, features != neurons, or why head-level analysis fails
    - These are the two other pilot articles, so the links will resolve

    **Content guidelines:**
    - Same editorial conversion rules as Plans 01-02
    - This article has 4 citations: `heimersheim2024patching`, `nanda2023attribution`, `wang2022ioi`, `conmy2023ioi`
    - Convert all Typst math:
      - `hat(y)` -> `\hat{y}`, `cal(L)` -> `\mathcal{L}`, `nabla` -> `\nabla`
      - `partial` -> `\partial`, `ell` -> `\ell`
    - Use `{% sidenote "..." %}` for at least 2 supplementary notes (e.g., Pearl's causal framework, historical development note about Vig/Geiger/Meng)
    - Convert `#focus-slide` content to emphasized standalone paragraphs
    - Drop all recap of Week 5 content -- replace with brief motivation
    - Drop "Readings" and "Looking Ahead" sections
    - Target ~2500 words
    - The article has 3 figures -- all must have descriptive alt text and captions
  </action>
  <verify>
    Run `npx @11ty/eleventy --serve` and open `/topics/activation-patching/`. Verify:
    1. Page renders without build errors
    2. All 3 images display correctly with captions
    3. All math renders (logit difference, gradient approximation, patching formula)
    4. All 4 citation tooltips work (heimersheim2024, nanda2023, wang2022, conmy2023)
    5. Cross-article links to /topics/attention-mechanism/ and /topics/superposition/ resolve (no 404)
    6. Sidenotes appear in the margin
    7. Pause-and-think sections are collapsible
    8. Prerequisites link appears in article header
    9. The article reads as coherent, flowing prose
  </verify>
  <done>
    A complete, readable "Activation Patching" article exists at /topics/activation-patching/ with 3 diagrams, 4 citations, cross-article links to both other pilot articles, and correct math. It reads as editorial prose covering the core idea, noising vs denoising, a worked example, attribution patching, and path patching.
  </done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds with zero errors
2. `/topics/activation-patching/` renders as a complete, readable article (~2500 words)
3. All 3 PNG images display with alt text and figure captions
4. KaTeX renders all math without errors
5. 4 citations work with tooltips
6. Cross-article links to `/topics/attention-mechanism/` and `/topics/superposition/` resolve
7. At least 2 sidenotes render correctly
8. Prerequisites link appears in article header
</verification>

<success_criteria>
- The Activation Patching article is ~2500 words of readable long-form prose
- All 6 content types exercised: math, figures (3), citations (4), sidenotes (2+), pause-and-think, definitions
- Cross-article concept links work to both other pilot articles
- Multiple citations from different papers render with correct tooltips
- The article demonstrates the full conversion workflow for an advanced, methodology-focused topic
</success_criteria>

<output>
After completion, create `.planning/phases/04-content-authoring-pilot-articles/04-03-SUMMARY.md`
</output>
