---
phase: 04-content-authoring-pilot-articles
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/topics/superposition/index.md
  - src/topics/superposition/images/phase_diagram.png
  - src/topics/superposition/images/superposition_1d_antipodal.png
  - src/topics/superposition/images/superposition_2d_orthogonal.png
  - src/topics/superposition/images/superposition_2d_triangle.png
  - src/topics/superposition/images/superposition_2d_pentagon.png
  - src/topics/superposition/images/superposition_3d_packing.png
autonomous: true

must_haves:
  truths:
    - "The Superposition Hypothesis article renders as readable long-form prose covering the fundamental tension, toy model, phase diagrams, geometry, interference, and implications for MI"
    - "All 6 course diagrams appear with descriptive alt text and figure captions"
    - "Display math renders correctly for the loss function, interference equations, and probability expressions"
    - "The article includes sidenotes for geometric intuitions and supplementary context"
    - "Front matter includes prerequisites linking to the attention-mechanism article"
  artifacts:
    - path: "src/topics/superposition/index.md"
      provides: "Complete pilot article: The Superposition Hypothesis"
      min_lines: 200
    - path: "src/topics/superposition/images/phase_diagram.png"
      provides: "Phase diagram figure from course materials"
  key_links:
    - from: "src/topics/superposition/index.md"
      to: "src/_data/references.json"
      via: "cite shortcode"
      pattern: "cite.*elhage2022toy"
    - from: "src/topics/superposition/index.md"
      to: "src/topics/superposition/images/"
      via: "markdown figure syntax"
      pattern: "superposition/images/"
---

<objective>
Write the second pilot article: "The Superposition Hypothesis." This is the most complex pilot, with 6 diagrams, substantial math, and rich conceptual content.

Purpose: Validate the image pipeline, figure/caption rendering, and the conversion of a diagram-heavy intermediate-level article. This is the highest-fidelity test of all 6 content types working together.

Output:
- Complete "Superposition Hypothesis" article (~3000 words)
- 6 PNG diagrams copied to article-local images directory
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-content-authoring-pilot-articles/04-RESEARCH.md
@src/_data/references.json
@eleventy.config.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Copy diagrams and write The Superposition Hypothesis article</name>
  <files>
    src/topics/superposition/images/phase_diagram.png
    src/topics/superposition/images/superposition_1d_antipodal.png
    src/topics/superposition/images/superposition_2d_orthogonal.png
    src/topics/superposition/images/superposition_2d_triangle.png
    src/topics/superposition/images/superposition_2d_pentagon.png
    src/topics/superposition/images/superposition_3d_packing.png
    src/topics/superposition/index.md
  </files>
  <action>
    **Step 1: Copy 6 PNG diagrams from course source to article images directory:**
    ```bash
    mkdir -p src/topics/superposition/images
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/phase_diagram.png src/topics/superposition/images/
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/superposition_1d_antipodal.png src/topics/superposition/images/
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/superposition_2d_orthogonal.png src/topics/superposition/images/
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/superposition_2d_triangle.png src/topics/superposition/images/
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/superposition_2d_pentagon.png src/topics/superposition/images/
    cp /Users/ivan/latex/mech-interp-course/week-09/assets/superposition_3d_packing.png src/topics/superposition/images/
    ```

    **Step 2: Write the article** by converting Week 9 Typst source (`/Users/ivan/latex/mech-interp-course/week-09/week-09.typ`, 1077 lines).

    **Front matter:**
    ```yaml
    ---
    title: "The Superposition Hypothesis"
    description: "How neural networks represent more features than dimensions by encoding them as nearly-orthogonal directions, why this makes interpretability hard, and what the toy model reveals about when superposition occurs."
    prerequisites:
      - title: "The Attention Mechanism"
        url: "/topics/attention-mechanism/"
    difficulty: "intermediate"
    block: "superposition-and-feature-extraction"
    category: "core-concepts"
    ---
    ```

    **Content structure (h2 sections):**

    1. `## The Fundamental Tension` -- The counting problem: models need more features than dimensions. The two strategies (select top-d vs. pack more features). Define superposition in a blockquote definition. Explain the benefit/cost tradeoff (importance and sparsity). Convert from Sections 1-2 of the Typst source. Drop the recap of IOI circuit -- replace with a brief forward reference: "In earlier articles on circuit analysis, features aligned neatly with heads. But what happens when they do not?" Use `{% cite "elhage2022toy" %}` when referencing the paper.

    2. `## The Toy Model` -- Elhage et al.'s experimental setup. The architecture (linear encoder, ReLU, linear decoder). Why a toy model. The experimental knobs (importance, sparsity). The loss function. Display math for:
       - $\hat{\mathbf{x}} = \text{ReLU}(\mathbf{W}_e \mathbf{x}) \cdot \mathbf{W}_d$
       - $\mathcal{L} = \sum_{i=1}^{m} I_i \cdot \mathbb{E}[(x_i - \hat{x}_i)^2]$

    3. `## Phase Diagrams` -- The central result. Include the phase diagram figure:
       ```markdown
       ![Phase diagram showing superposition regions as a function of feature importance and sparsity. Blue region indicates no superposition with orthogonal features. Red region indicates strong superposition with packed features.](/topics/superposition/images/phase_diagram.png "Figure 1: Phase diagram for superposition. The transition from orthogonal representation to superposed representation is sharp, like a phase transition in physics.")
       ```
       Explain the two regions, why importance matters, why sparsity matters. The key insight: $P(\text{interference}) \approx P(\text{feature A active}) \times P(\text{feature B active})$. The phase transition sharpness.

    4. `## The Geometry of Superposition` -- Features as directions. The orthogonal case. Non-orthogonal packing. Include all 5 geometry figures with descriptive alt text:
       - `superposition_2d_orthogonal.png` -- baseline with 2 features in 2D
       - `superposition_1d_antipodal.png` -- 2 features in 1D (antipodal)
       - `superposition_2d_triangle.png` -- 3 features in 2D (triangle)
       - `superposition_2d_pentagon.png` -- 5 features in 2D (pentagon)
       - `superposition_3d_packing.png` -- 6 features in 3D (octahedron)

       Explain the progression from no superposition to dense packing. The regular polytope pattern. Use `{% sidenote "..." %}` for the geometric insight about sphere packing theory.

       Include a pause-and-think about what structure you would expect in a real transformer with d=768.

    5. `## Interference and Its Cost` -- What interference looks like (ghost activations). The cost formula. Why sparsity makes interference cheap. The superposition bargain.

    6. `## Why Superposition Makes Interpretability Hard` -- Polysemanticity (neurons respond to multiple features). Features != neurons. What this means for circuit analysis. The interpretability bottleneck. How bad it is in practice. Include a forward reference to sparse autoencoders in plain text (not a link).

    **Content guidelines:**
    - Same editorial conversion rules as Plan 01 (prose not bullets, convert #pause sequences, etc.)
    - Convert all Typst math using the conversion table:
      - `feat` -> `\mathbf{f}`, `vx` -> `\mathbf{x}`, `vW` -> `\mathbf{W}`
      - `cal(L)` -> `\mathcal{L}`, `EE` -> `\mathbb{E}`, `RR` -> `\mathbb{R}`
      - `arrow.r` -> `\rightarrow`, `approx` -> `\approx`, `eq.not` -> `\neq`
      - `prop` -> `\propto`, `hat(vx)` -> `\hat{\mathbf{x}}`
      - `sae` -> `\text{SAE}`, `encoder` -> `\text{Enc}`, `decoder` -> `\text{Dec}`
    - Each figure must have descriptive alt text (what the diagram shows) and a caption (what to notice)
    - Use `{% cite "elhage2022toy" %}` throughout (this article references primarily one paper)
    - Use `{% sidenote "..." %}` for supplementary geometric intuitions (at least 2-3 sidenotes)
    - Convert `#focus-slide` content to emphasized standalone paragraphs
    - Drop all recap sections (replace with brief cross-references)
    - Drop "Readings" and "Looking Ahead" sections
    - Target ~3000 words
    - All two-column slides become flowing prose (no column layout in articles)
  </action>
  <verify>
    Run `npx @11ty/eleventy --serve` and open `/topics/superposition/`. Verify:
    1. Page renders without build errors
    2. All 6 images display correctly with captions
    3. All math renders (loss function, interference formula, probability expressions)
    4. Citation tooltip works for `elhage2022toy`
    5. Sidenotes appear in the margin on wide screens
    6. Pause-and-think sections are collapsible
    7. Prerequisites link to `/topics/attention-mechanism/` appears in the article header
    8. The article reads as coherent, flowing prose
  </verify>
  <done>
    A complete, readable "Superposition Hypothesis" article exists at /topics/superposition/ with 6 diagrams, correct math, citations, sidenotes, and pause-and-think prompts. It reads as editorial prose covering the fundamental tension, toy model, phase diagrams, geometry, interference, and implications for MI.
  </done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds with zero errors
2. `/topics/superposition/` renders as a complete, readable article (~3000 words)
3. All 6 PNG images display with alt text and figure captions
4. KaTeX renders all math without errors
5. Citation `elhage2022toy` works with tooltip
6. At least 2 sidenotes appear correctly
7. Prerequisites show "The Attention Mechanism" as a linked prerequisite
8. Pause-and-think blocks are collapsible
</verification>

<success_criteria>
- The Superposition Hypothesis article is ~3000 words of readable long-form prose
- All 6 content types exercised: math (display + inline), figures (6 with captions), citations, sidenotes (2+), pause-and-think prompts, definitions (blockquotes)
- All 6 diagrams display correctly with descriptive alt text
- The image passthrough pipeline works end-to-end (images in source -> images in _site)
- Prerequisites metadata renders in the article header
</success_criteria>

<output>
After completion, create `.planning/phases/04-content-authoring-pilot-articles/04-02-SUMMARY.md`
</output>
