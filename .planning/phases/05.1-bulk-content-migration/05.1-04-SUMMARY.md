---
phase: 05.1-bulk-content-migration
plan: 04
subsystem: content
tags: [DLA, logit-lens, tuned-lens, probing, attention-patterns, KaTeX, citations]

# Dependency graph
requires:
  - phase: 05.1-01
    provides: references.json (44 entries) and learningPath.json (8 blocks, 35 topics)
provides:
  - direct-logit-attribution article (Block 2, Foundations of MI)
  - logit-lens-and-probing article with 3 diagrams (Block 2, Foundations of MI)
affects: [05.1-05, 05.1-06, future Block 3 articles that link back to observational tools]

# Tech tracking
tech-stack:
  added: []
  patterns: []

key-files:
  created:
    - src/topics/direct-logit-attribution/index.md
    - src/topics/logit-lens-and-probing/index.md
    - src/topics/logit-lens-and-probing/images/attn_induction.png
    - src/topics/logit-lens-and-probing/images/attn_prev_token.png
    - src/topics/logit-lens-and-probing/images/logit_lens_eiffel.png
  modified: []

key-decisions:
  - "DLA article structured as: key insight -> decomposition -> per-token attribution -> attention patterns -> limitation"
  - "Logit lens/probing article uses narrative arc from observation tools -> their limits -> need for causation"
  - "Attention pattern visualization placed in logit-lens article (not DLA) because the real GPT-2 data figures naturally pair with observational tools discussion"

patterns-established:
  - "Observational-vs-causal framing: both articles end with explicit limitation pointing to activation-patching"

# Metrics
duration: 4min 30s
completed: 2026-02-04
---

# Phase 5.1 Plan 04: Block 2 Part 2 (DLA and Probing) Summary

**Direct logit attribution article covering DLA decomposition and attention patterns, plus logit lens/tuned lens/probing article with 3 GPT-2 diagrams and 6 paper citations completing the observation-vs-causation narrative**

## Performance

- **Duration:** 4min 30s
- **Started:** 2026-02-04T15:14:31Z
- **Completed:** 2026-02-04T15:19:01Z
- **Tasks:** 2/2
- **Files created:** 5

## Accomplishments
- Created direct-logit-attribution article (~1,800 words) covering the DLA decomposition, per-token attribution as screening tool, reading attention patterns, and the observational limitation
- Created logit-lens-and-probing article (~2,300 words) covering logit lens, tuned lens, probing classifiers (structural, MDL, amnesic), and the correlation-vs-causation critique
- Placed 3 diagrams (attn_induction.png, attn_prev_token.png, logit_lens_eiffel.png) with descriptive alt text and Figure N: captions
- Both articles have block: "foundations-of-mi" completing all 5 Block 2 articles

## Task Commits

Each task was committed atomically:

1. **Task 1: Create direct-logit-attribution article** - `7874a7b` (feat)
2. **Task 2: Create logit-lens-and-probing article with 3 diagrams** - `17dc6e8` (feat)

## Files Created/Modified
- `src/topics/direct-logit-attribution/index.md` - DLA decomposition, per-token attribution, attention patterns, observational limitation
- `src/topics/logit-lens-and-probing/index.md` - Logit lens, tuned lens, probing classifiers, correlation-vs-causation critique
- `src/topics/logit-lens-and-probing/images/attn_induction.png` - Induction head attention pattern from GPT-2 Small
- `src/topics/logit-lens-and-probing/images/attn_prev_token.png` - Previous token head attention pattern from GPT-2 Small
- `src/topics/logit-lens-and-probing/images/logit_lens_eiffel.png` - Logit lens layer-by-layer predictions for Eiffel Tower prompt

## Decisions Made
- DLA article references elhage2021mathematical and wang2022ioi (IOI as concrete DLA example)
- Logit lens/probing article references 6 papers: nostalgebraist2020logitlens, belrose2023tunedlens, hewitt2019structural, voita2020mdl, elazar2021amnesic, belinkov2022probing
- Attention pattern figures placed in logit-lens article (where the observational tools discussion naturally includes real GPT-2 data) rather than DLA article
- Both articles end with explicit forward-links to activation-patching, establishing the observation-to-causation transition that motivates Block 3

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered

None.

## User Setup Required

None - no external service configuration required.

## Next Phase Readiness
- Block 2 (Foundations of MI) is now complete with all 5 articles: what-is-mech-interp, linear-representation-hypothesis, induction-heads, direct-logit-attribution, logit-lens-and-probing
- Both new articles have correct forward-links to Block 3 content (activation-patching, attribution-patching)
- Ready for Block 3 plans (05.1-05 onward) covering causal methods

---
*Phase: 05.1-bulk-content-migration*
*Completed: 2026-02-04*
