---
phase: 05.1-bulk-content-migration
plan: 02
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/attention-mechanism/index.md
  - src/topics/transformer-circuits/index.md
autonomous: true

must_haves:
  truths:
    - "attention-mechanism article covers all Week 1 content with no significant gaps"
    - "transformer-circuits article explains QK/OV circuits, composition, virtual attention heads, and TransformerLens"
    - "Both articles render correctly with all math, citations, and cross-links"
  artifacts:
    - path: "src/topics/attention-mechanism/index.md"
      provides: "Complete Week 1 coverage"
      contains: "block: \"transformer-foundations\""
    - path: "src/topics/transformer-circuits/index.md"
      provides: "Week 2 content as narrative article"
      contains: "block: \"transformer-foundations\""
  key_links:
    - from: "src/topics/transformer-circuits/index.md"
      to: "src/topics/attention-mechanism/index.md"
      via: "prerequisite and cross-article links"
      pattern: "/topics/attention-mechanism/"
---

<objective>
Complete Block 1 (Transformer Foundations): review/expand the attention-mechanism pilot and create the transformer-circuits article from Week 2.

Purpose: Block 1 establishes the transformer architecture foundation that all later articles build on. The attention-mechanism pilot needs verification against Week 1 Typst source, and Week 2 content on QK/OV circuits and composition is entirely new.
Output: 2 complete articles covering Weeks 1-2
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md

Source content:
@/Users/ivan/latex/mech-interp-course/week-01/week-01.typ
@/Users/ivan/latex/mech-interp-course/week-02/week-02.typ
@/Users/ivan/latex/mech-interp-course/globals.typ

Existing article to review:
@src/topics/attention-mechanism/index.md

Data files (already updated by Plan 01):
@src/_data/references.json
@src/_data/learningPath.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review and expand attention-mechanism pilot article</name>
  <files>src/topics/attention-mechanism/index.md</files>
  <action>
Compare the existing attention-mechanism pilot article against week-01.typ section by section. The pilot is ~207 lines; the Typst source is 658 lines.

Check that these Week 1 topics are covered adequately:
- Token embeddings and positional encodings
- The residual stream concept
- Attention mechanism: Q, K, V matrices, scaled dot-product, softmax
- Multi-head attention
- Layer normalization
- MLP layers (GELU activation, up/down projection)
- Full transformer architecture (stacking layers)

The research assessment says the pilot is "likely complete." If it is, leave it as-is. If there are gaps, expand the relevant sections with narrative prose converted from the Typst.

Ensure front matter has all required fields:
- title, description, prerequisites (should be `[]` for foundational), difficulty: "foundational", block: "transformer-foundations", category

Add cross-links to the new transformer-circuits article where QK/OV circuits or composition are mentioned.

Apply the Typst-to-KaTeX conversion rules from the research for any new math. Use {% cite "key" %} for any new paper references. Add sidenotes for supplementary context where appropriate.
  </action>
  <verify>Run `npx @11ty/eleventy` and check build succeeds. Open the rendered article and verify all math renders (no red KaTeX errors), all citations show tooltips, and cross-links point to valid paths.</verify>
  <done>attention-mechanism article covers all major Week 1 topics. Front matter is complete. Any new math uses valid KaTeX. Cross-link to transformer-circuits is present.</done>
</task>

<task type="auto">
  <name>Task 2: Create transformer-circuits article from Week 2</name>
  <files>src/topics/transformer-circuits/index.md</files>
  <action>
Create a new article directory and index.md:
```bash
mkdir -p src/topics/transformer-circuits
```

Convert week-02.typ (934 lines) into a narrative long-form article. Week 2 covers:

1. **QK and OV Circuits** - How attention heads decompose into QK (what to attend to) and OV (what to output) circuits. The mathematical framework from Elhage et al. (2021): $W_{QK}^h = W_Q^{hT} W_K^h$ and $W_{OV}^h = W_V^h W_O^h$.

2. **Circuit Composition** - How attention heads compose across layers: Q-composition, K-composition, V-composition. Virtual attention heads as emergent multi-layer circuits.

3. **The Mathematical Framework** - The "A Mathematical Framework for Transformer Circuits" paper's key contributions. Writing the transformer as a sum over paths through the residual stream.

4. **TransformerLens** - Brief introduction to the TransformerLens library for inspecting transformer internals (Neel Nanda's tool). How it enables MI research.

Front matter:
```yaml
---
title: "Transformer Circuits: QK, OV, and Composition"
description: "How attention heads decompose into interpretable QK and OV circuits, and how these circuits compose across layers to form complex computations."
prerequisites:
  - title: "The Attention Mechanism"
    url: "/topics/attention-mechanism/"
difficulty: "foundational"
block: "transformer-foundations"
category: "core-concepts"
---
```

Content guidelines:
- Convert Typst slide content to flowing narrative paragraphs (not bullet lists)
- Drop all slide-specific elements (#pause, #focus-slide, recap sections, readings slides)
- Translate all math from Typst to KaTeX using the conversion table in the research
- Use {% cite "elhage2021mathematical" %} for the framework paper (already in references.json)
- Add 2 pause-and-think prompts (adapt from source exercises or create conceptual questions)
- Add 2-4 sidenotes for supplementary context
- Cross-link back to attention-mechanism for foundational concepts (residual stream, attention heads)
- Cross-link forward to what-is-mech-interp and induction-heads where relevant
- No diagrams for this article (Week 2 has no PNGs)
- Target 1,500-2,500 words of narrative prose
  </action>
  <verify>Run `npx @11ty/eleventy` and check build succeeds. Verify the article appears in the sidebar under "Transformer Foundations." Check that all math renders, citations have tooltips, and cross-links work.</verify>
  <done>transformer-circuits article exists with complete front matter, narrative prose covering QK/OV circuits, composition, and TransformerLens. All math is valid KaTeX. At least 2 pause-and-think prompts and 2 sidenotes. Cross-links to attention-mechanism work.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds without errors
2. Both articles appear in sidebar under "Transformer Foundations" block
3. transformer-circuits shows as second article in the block (after attention-mechanism)
4. Prev/next navigation links work between the two Block 1 articles
5. No KaTeX red error boxes in either article
6. All {% cite %} shortcodes resolve (no [??] markers)
</verification>

<success_criteria>
- attention-mechanism covers all Week 1 topics (verified against Typst source)
- transformer-circuits is a complete, readable article covering all Week 2 topics
- Both articles have complete front matter and render correctly
- Cross-links between articles work
- Both appear correctly in sidebar navigation
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-02-SUMMARY.md`
</output>
