---
phase: 05.1-bulk-content-migration
plan: 02
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/attention-mechanism/index.md
  - src/topics/qk-ov-circuits/index.md
  - src/topics/composition-and-virtual-heads/index.md
autonomous: true

must_haves:
  truths:
    - "attention-mechanism pilot article is reviewed and confirmed complete against Week 1 Typst source"
    - "qk-ov-circuits article covers QK and OV circuit decomposition with worked example from Week 2"
    - "composition-and-virtual-heads article covers V/K/Q-composition, virtual heads, and TransformerLens vocabulary from Week 2"
    - "All three articles have complete front matter with block: transformer-foundations"
    - "Cross-links exist between the three articles and to downstream articles where concepts are referenced"
  artifacts:
    - path: "src/topics/qk-ov-circuits/index.md"
      provides: "QK and OV Circuits article"
      contains: "block: \"transformer-foundations\""
    - path: "src/topics/composition-and-virtual-heads/index.md"
      provides: "Composition and Virtual Attention Heads article"
      contains: "block: \"transformer-foundations\""
  key_links:
    - from: "src/topics/qk-ov-circuits/index.md"
      to: "src/topics/attention-mechanism/index.md"
      via: "prerequisite link and inline cross-reference"
      pattern: "/topics/attention-mechanism/"
    - from: "src/topics/composition-and-virtual-heads/index.md"
      to: "src/topics/qk-ov-circuits/index.md"
      via: "prerequisite link"
      pattern: "/topics/qk-ov-circuits/"
---

<objective>
Create Block 1 (Transformer Foundations): review the existing attention-mechanism pilot for completeness, then create 2 new articles covering Week 2 content (QK/OV circuits and composition/virtual heads).

Purpose: Block 1 establishes the transformer architecture foundation that all subsequent MI articles build upon.
Output: 3 articles (1 reviewed pilot + 2 new) covering Weeks 1-2.
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md
@src/topics/attention-mechanism/index.md (existing pilot -- review target)
@src/_data/references.json (updated by Plan 01)
@src/_data/learningPath.json (updated by Plan 01)

Typst source files to convert:
- /Users/ivan/latex/mech-interp-course/week-01/week-01.typ (for pilot review)
- /Users/ivan/latex/mech-interp-course/week-02/week-02.typ (for new articles)
- /Users/ivan/latex/mech-interp-course/globals.typ (math notation reference)

Course research notes (READ THESE for deeper content, additional references, and pedagogical context beyond the Typst slides):
- /Users/ivan/latex/mech-interp-course/.planning/phases/02-foundations/02-RESEARCH.md (Weeks 1-2 domain research)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review attention-mechanism pilot and create qk-ov-circuits article</name>
  <files>
    src/topics/attention-mechanism/index.md
    src/topics/qk-ov-circuits/index.md
  </files>
  <action>
**Part A: Review attention-mechanism pilot**

Read the existing pilot at `src/topics/attention-mechanism/index.md` and compare against the Typst source at `/Users/ivan/latex/mech-interp-course/week-01/week-01.typ`. The pilot should cover: the attention mechanism, QK/V matrices, multi-head attention, the residual stream, layer norms, MLPs, positional encodings, and the full transformer. The Week 1 prerequisites recap (ML/DL basics, linear algebra) can be omitted since these are prerequisites for the site, not article content.

If the pilot is missing any major Week 1 topics, add them. If it already covers all major topics (likely per the research assessment), confirm completeness and make only minor improvements if needed (fix any KaTeX errors, add missing cross-links to the new qk-ov-circuits article, ensure 2 pause-and-think prompts exist).

Update the front matter to ensure it matches the schema: title, description, prerequisites (should be `[]` since it is the first article), difficulty: "foundational", block: "transformer-foundations", category: "core-concepts".

**Part B: Create qk-ov-circuits article**

Create `src/topics/qk-ov-circuits/index.md` converting Week 2 sections 2-3 from the Typst source. This covers:

- The Residual Stream as a Vector Space (the communication channel perspective, how heads read/write)
- QK and OV Circuits (the factored weight matrices, low-rank structure, what QK circuit computes vs what OV circuit computes)
- The worked example of QK/OV decomposition

Content guidelines:
- Target 1,500-2,500 words of narrative prose
- Convert all Typst math to KaTeX using the conversion table in the research (e.g., `vW` -> `\mathbf{W}`, `RR^n` -> `\mathbb{R}^n`, `d_"model"` -> `d_{\text{model}}`)
- Use blockquote format for definitions: `> **Term:** Definition text`
- Include 2 pause-and-think prompts adapted from course exercises
- Include 2-4 sidenotes for supplementary context
- Use {% cite "key" %} for paper references (elhage2021mathematical is the key reference)
- Cross-link to attention-mechanism as prerequisite and forward-link to composition-and-virtual-heads

Front matter:
```yaml
title: "QK and OV Circuits"
description: "How attention heads decompose into independent QK (matching) and OV (copying) circuits through the low-rank factorization of weight matrices."
prerequisites:
  - title: "The Attention Mechanism"
    url: "/topics/attention-mechanism/"
difficulty: "foundational"
block: "transformer-foundations"
category: "core-concepts"
```
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` from the project root -- no build errors. Check that `src/topics/qk-ov-circuits/index.md` exists and has valid front matter by running `head -15 src/topics/qk-ov-circuits/index.md`.
  </verify>
  <done>attention-mechanism pilot reviewed and confirmed complete (or expanded if needed). qk-ov-circuits article exists with complete front matter, narrative prose covering QK/OV decomposition, KaTeX math, citations, sidenotes, pause-and-think prompts, and cross-links.</done>
</task>

<task type="auto">
  <name>Task 2: Create composition-and-virtual-heads article</name>
  <files>src/topics/composition-and-virtual-heads/index.md</files>
  <action>
Create `src/topics/composition-and-virtual-heads/index.md` converting Week 2 sections 4-6 from the Typst source at `/Users/ivan/latex/mech-interp-course/week-02/week-02.typ`. This covers:

- Virtual Attention Heads and Composition (what composition means in the circuit framework)
- V-Composition, K-Composition, Q-Composition (the three types, what each enables)
- Two-layer expansion and the combinatorial richness of composition
- TransformerLens vocabulary (the naming conventions researchers use: `blocks.0.attn.hook_q`, etc.)

Content guidelines:
- Target 1,500-2,500 words of narrative prose
- Convert all Typst math to KaTeX using the conversion table
- Use blockquote format for definitions (e.g., V-Composition, K-Composition, Q-Composition)
- Include 2 pause-and-think prompts
- Include 2-4 sidenotes
- Use {% cite "key" %} for references (elhage2021mathematical, olsson2022context for induction head composition example)
- Cross-link to qk-ov-circuits (prerequisite) and forward-link to induction-heads (where composition is applied to discover induction circuits)

Front matter:
```yaml
title: "Composition and Virtual Attention Heads"
description: "How attention heads compose across layers through V-, K-, and Q-composition, creating virtual attention heads with capabilities no single head possesses."
prerequisites:
  - title: "QK and OV Circuits"
    url: "/topics/qk-ov-circuits/"
difficulty: "foundational"
block: "transformer-foundations"
category: "core-concepts"
```
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Check that `src/topics/composition-and-virtual-heads/index.md` exists with valid front matter.
  </verify>
  <done>composition-and-virtual-heads article exists with complete front matter, narrative prose covering V/K/Q-composition and TransformerLens vocabulary, KaTeX math, citations, sidenotes, pause-and-think prompts, and cross-links to qk-ov-circuits and induction-heads.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy --dryrun` succeeds with no errors
2. All 3 Block 1 articles exist: attention-mechanism, qk-ov-circuits, composition-and-virtual-heads
3. Each article has valid front matter with block: "transformer-foundations"
4. qk-ov-circuits and composition-and-virtual-heads each have 2 pause-and-think prompts, 2+ sidenotes, 1+ citations
5. Cross-links work between the three articles
</verification>

<success_criteria>
- 3 articles in Block 1 (Transformer Foundations) covering all of Weeks 1-2
- Pilot article reviewed and confirmed complete
- 2 new articles with narrative prose, KaTeX math, citations, cross-links
- All front matter valid and consistent
- Build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-02-SUMMARY.md`
</output>
