---
phase: 05.1-bulk-content-migration
plan: 07
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/scaling-monosemanticity/index.md
  - src/topics/sae-variants-and-evaluation/index.md
autonomous: true

must_haves:
  truths:
    - "scaling-monosemanticity article covers scaling results, Golden Gate Claude, safety-relevant features"
    - "sae-variants-and-evaluation article covers Gated/TopK/JumpReLU SAEs, SAEBench, and honest limitations (absorption, splitting, dead features)"
    - "Both articles have block: superposition-and-feature-extraction"
  artifacts:
    - path: "src/topics/scaling-monosemanticity/index.md"
      provides: "Scaling Monosemanticity and Feature Steering article"
      contains: "block: \"superposition-and-feature-extraction\""
    - path: "src/topics/sae-variants-and-evaluation/index.md"
      provides: "SAE Variants, Evaluation, and Honest Limitations article"
      contains: "block: \"superposition-and-feature-extraction\""
  key_links:
    - from: "src/topics/scaling-monosemanticity/index.md"
      to: "src/topics/sae-interpretability/index.md"
      via: "prerequisite link"
      pattern: "/topics/sae-interpretability/"
    - from: "src/topics/sae-variants-and-evaluation/index.md"
      to: "src/topics/scaling-monosemanticity/index.md"
      via: "prerequisite link"
      pattern: "/topics/scaling-monosemanticity/"
---

<objective>
Create Block 4 Part 2: scaling-monosemanticity and sae-variants-and-evaluation articles from Week 11, completing the 5 articles in Block 4.

Purpose: These articles cover the scaling of SAEs to production models and the critical assessment of SAE methods.
Output: 2 new articles completing Block 4.
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md
@src/_data/references.json (updated by Plan 01)
@src/_data/learningPath.json (updated by Plan 01)

Typst source files:
- /Users/ivan/latex/mech-interp-course/week-11/week-11.typ
- /Users/ivan/latex/mech-interp-course/globals.typ
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scaling-monosemanticity article</name>
  <files>src/topics/scaling-monosemanticity/index.md</files>
  <action>
Create `src/topics/scaling-monosemanticity/index.md` from Week 11 sections 2-4. Cover:

- Scaling Monosemanticity: from 4,096 features in a small model to 34 million features in Claude 3 Sonnet
- Multilingual, multimodal, abstract features: features that fire across languages, modalities, and levels of abstraction
- Feature hierarchies: coarse-to-fine feature organization
- Scaling laws for feature discovery
- Golden Gate Claude: the feature steering demonstration -- finding a "Golden Gate Bridge" feature and clamping it to create a model obsessed with the bridge
- The technical mechanism of feature steering (clamping SAE features during inference)
- Causal implications: steering proves features are causally relevant, not just correlated
- Safety-relevant features: features related to deception, power-seeking, sycophancy

Front matter:
```yaml
title: "Scaling Monosemanticity and Feature Steering"
description: "How scaling sparse autoencoders to millions of features revealed multilingual, multimodal, and abstract concepts -- and how clamping these features enables steering model behavior."
prerequisites:
  - title: "Feature Dashboards and Automated Interpretability"
    url: "/topics/sae-interpretability/"
difficulty: "advanced"
block: "superposition-and-feature-extraction"
category: "applications"
```

References: templeton2024scaling (primary), bricken2023monosemanticity. Cross-link to sae-interpretability (prereq), sparse-autoencoders (architecture), forward to sae-variants-and-evaluation and activation-engineering.

Content guidelines: 2,000-2,500 words, narrative prose, KaTeX math where needed, 2 pause-and-think, 2-4 sidenotes. The Golden Gate Claude story is engaging -- tell it well.
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify file exists with valid front matter.
  </verify>
  <done>scaling-monosemanticity article created covering scaling results, Golden Gate Claude, and safety-relevant features.</done>
</task>

<task type="auto">
  <name>Task 2: Create sae-variants-and-evaluation article</name>
  <files>src/topics/sae-variants-and-evaluation/index.md</files>
  <action>
Create `src/topics/sae-variants-and-evaluation/index.md` from Week 11 sections 5-8. Cover:

- The L1 Problem: why the L1 sparsity penalty introduces systematic biases (shrinkage)
- SAE Variants:
  - Gated SAEs (Rajamanoharan et al.): separate gating and magnitude pathways
  - TopK SAEs (Gao et al.): fixed sparsity via top-k selection
  - JumpReLU SAEs (Rajamanoharan et al.): learnable threshold replacing L1
- Evaluating SAEs: SAEBench (Karvonen et al.), the key metrics (reconstruction fidelity, sparsity, interpretability), the key finding that different SAE types have different tradeoff profiles
- Honest Limitations:
  - Feature absorption: important features getting absorbed into other features
  - Feature splitting: one concept spread across multiple features
  - Dead features: features that never activate after training
  - Non-uniqueness: different SAE runs produce different feature sets
  - Interpretability illusions: features that appear monosemantic but are not

Front matter:
```yaml
title: "SAE Variants, Evaluation, and Honest Limitations"
description: "The landscape of SAE architectures beyond vanilla L1 -- Gated, TopK, and JumpReLU SAEs -- how to evaluate them with SAEBench, and the honest limitations that remain unsolved."
prerequisites:
  - title: "Scaling Monosemanticity and Feature Steering"
    url: "/topics/scaling-monosemanticity/"
difficulty: "advanced"
block: "superposition-and-feature-extraction"
category: "methods"
```

References: rajamanoharan2024gated, gao2024scaling, rajamanoharan2024jumprelu, karvonen2025saebench, chanin2024absorption. Cross-link to sparse-autoencoders (base architecture), scaling-monosemanticity (what SAEs found), forward to transcoders (alternative to SAEs).

Content guidelines: 2,000-2,500 words, KaTeX math (each variant's loss function), blockquote definitions for each SAE variant, 2 pause-and-think, 2-4 sidenotes. The limitations section is critical -- be honest about what SAEs cannot do.
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify file exists with valid front matter.
  </verify>
  <done>sae-variants-and-evaluation article created covering all SAE variants, SAEBench evaluation, and honest limitations.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy --dryrun` succeeds
2. Both articles exist with block: "superposition-and-feature-extraction"
3. scaling-monosemanticity covers Golden Gate Claude and safety features
4. sae-variants-and-evaluation covers at least 3 SAE variants and 4+ limitations
5. Citations reference all relevant papers (templeton2024scaling, SAE variant papers, karvonen2025saebench)
</verification>

<success_criteria>
- 2 new articles completing Block 4 (5 total articles in Superposition & Feature Extraction)
- Critical perspective on SAE limitations maintained
- All front matter valid, build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-07-SUMMARY.md`
</output>
