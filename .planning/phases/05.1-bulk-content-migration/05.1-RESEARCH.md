# Phase 5.1: Bulk Content Migration - Research

**Researched:** 2026-02-04
**Domain:** Converting 16 weeks of Typst slide content into thematic long-form Markdown articles for an Eleventy static site
**Confidence:** HIGH

## Summary

Phase 5.1 converts all 16 weeks of Typst course content (~14,744 lines across 16 files) into thematic long-form Markdown articles, reorganized by topic rather than by lecture week. The research investigated six areas: (1) the established article authoring patterns from the 3 pilot articles, (2) the Typst-to-Markdown conversion patterns including math notation translation, (3) the proposed thematic article inventory and week-to-article mapping, (4) the complete reference catalog needing addition to references.json, (5) the diagram placement plan for all 27 PNGs, and (6) the learningPath.json and cross-linking structure needed for full site coverage.

Three pilot articles already exist (attention-mechanism from week 1, activation-patching from week 6, superposition from week 9), establishing patterns for front matter, citations, sidenotes, figures, definitions, pause-and-think prompts, and cross-article links. These pilots cover only a fraction of their respective weeks' content and need expansion. The remaining 13 weeks have no articles at all. The conversion is not 1:1 (slides to paragraphs) but a substantive rewrite: slide bullet points become narrative prose, definitions become blockquotes, slide-specific elements (#pause, #focus-slide, two-column layouts) are dropped, and the content is reorganized by theme rather than by weekly lecture sequence.

The site infrastructure (Eleventy, KaTeX, citations, sidenotes, sidebar, learning path, prev/next navigation) is complete and working. New articles automatically appear in navigation when added to learningPath.json with matching front matter. The main work is content authoring -- there is no infrastructure to build.

**Primary recommendation:** Organize work into 6 batches by course block. For each batch: (1) add all needed references to references.json, (2) create/expand article directories with images, (3) write the articles converting Typst to narrative Markdown, (4) update learningPath.json with new topics, and (5) verify rendering. Each batch is independently shippable. Pilot articles should be expanded first (they are partial) before moving to new content.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| @11ty/eleventy | 3.0.0 | Static site generator | Already installed, all infrastructure built |
| markdown-it | * | Markdown processing | Configured with KaTeX, figure, anchor plugins |
| @mdit/plugin-katex | * | LaTeX math rendering | Already configured with htmlAndMathml output |
| @mdit/plugin-figure | * | Image captions | Already configured for `![alt](src "caption")` syntax |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| @sindresorhus/slugify | * | Heading ID generation | Already configured for TOC/anchor links |
| eleventy-plugin-toc | * | Table of contents | Already configured, reads h2/h3 |
| @11ty/eleventy-navigation | * | Sidebar hierarchy | Already configured, reads eleventyNavigation from front matter |

### Alternatives Considered
None. The stack is fully established. No new libraries needed for content migration.

**Installation:**
No installation needed. All dependencies are already in place.

## Architecture Patterns

### Article File Structure
```
src/topics/
  {topic-slug}/
    index.md              # Article content with front matter
    images/               # Article-specific images (PNGs)
      {diagram-name}.png
```

Each article follows this template (derived from pilot articles):

```markdown
---
title: "Article Title"
description: "One-sentence description for meta tags and article header."
prerequisites:
  - title: "Prerequisite Article Title"
    url: "/topics/prerequisite-slug/"
difficulty: "foundational|intermediate|advanced"
block: "block-slug-from-learningPath"
category: "core-concepts|methods|applications"
---

## First Major Section

Narrative prose converting slide content...

> **Definition Name:** Blockquote format for formal definitions.

$$
\text{Display math in KaTeX}
$$

Inline math like $x \in \mathbb{R}^n$ uses dollar signs.

![Alt text for accessibility](/topics/slug/images/diagram.png "Figure N: Caption text")

{% cite "referenceKey" %} for numbered citations with hover tooltips.

{% sidenote "Supplementary context that appears in the margin on wide screens." %}

[Cross-article link text](/topics/other-article/#section-anchor)

<details class="pause-and-think">
<summary>Pause and think: Prompt title</summary>

Discussion question adapted from course exercises.

</details>
```

### Pattern 1: Front Matter Schema
**What:** Every article requires these front matter fields.
**When to use:** Every article.
**Fields:**
- `title`: Quoted string, rendered by layout (not duplicated in content)
- `description`: One-sentence summary for header and meta
- `prerequisites`: Array of `{title, url}` objects (empty `[]` for foundational articles)
- `difficulty`: One of `foundational`, `intermediate`, `advanced`
- `block`: Slug matching a block in learningPath.json
- `category`: One of `core-concepts`, `methods`, `applications` (used by pilot articles)
- `order`: Optional integer for ordering within a block (topics.11tydata.js reads it)

### Pattern 2: Typst Math to KaTeX Conversion
**What:** Systematic translation rules from Typst math syntax to KaTeX.
**When to use:** Every math expression in every article.

| Typst | KaTeX | Notes |
|-------|-------|-------|
| `$...$` (inline) | `$...$` | Same delimiter |
| `$ ... $` (display, on own line) | `$$\n...\n$$` | Must be on separate lines |
| `vx`, `vh`, `vW` (custom ops in globals.typ) | `\mathbf{x}`, `\mathbf{h}`, `\mathbf{W}` | Bold for vectors/matrices |
| `RR^n` | `\mathbb{R}^n` | Blackboard bold |
| `W_(Q K)^h` | `W_{QK}^h` | Subscript grouping |
| `sum_(l=0)^(L-1)` | `\sum_{l=0}^{L-1}` | Sum notation |
| `d_"model"` | `d_{\text{model}}` | Text in subscripts |
| `"Attn"`, `attn` (custom op) | `\text{Attn}` | Operator names |
| `embed(x)` (custom op) | `\text{Embed}(x)` | Custom operators become \text |
| `vec(1, 0, 1, -1)` | Not used in articles | Column vectors not needed in prose |
| `mat(...)` | Not used in articles | Matrices not needed in prose |
| `underbrace(expr, "label")` | `\underbrace{expr}_{\text{label}}` | Underbraces |
| `alpha_(i,j)` | `\alpha_{i,j}` | Greek with subscripts |
| `partial cal(L) \/ partial vW` | `\frac{\partial \mathcal{L}}{\partial \mathbf{W}}` | Partial derivatives |
| `bold("u")` | `\mathbf{u}` | Bold vectors |
| `bb(R)` | `\mathbb{R}` | Blackboard bold |

### Pattern 3: Typst Slide Elements to Markdown
**What:** How slide-specific Typst constructs map to article format.
**When to use:** During conversion of every week file.

| Typst Slide Element | Article Equivalent | Notes |
|---------------------|-------------------|-------|
| `= Section Title` | `## Section Title` | Section headings become h2 |
| `== Slide Title` | Content flows under current h2 | Slide titles are NOT separate headings; merge content |
| `#pause` | Drop entirely | Incremental reveals have no article equivalent |
| `#focus-slide[...]` | Bold paragraph or pull quote | Key takeaway becomes emphasized prose |
| `#definition(title: "Name")[...]` | `> **Name:** content` | Blockquote definition format |
| `#pause-and-think[...]` | `<details class="pause-and-think">` | Two per article (established pattern) |
| `#slide(composer: (1fr, 1fr))[...][...]` | Merge into single-column prose | Two-column layouts linearize |
| `#image("assets/file.png")` | `![alt](/topics/slug/images/file.png "Figure N: caption")` | Figure with caption |
| `#table(...)` | Markdown table | `\| Header \| ...\|` format |
| `#align(center)[...]` | Drop alignment; content flows naturally | |
| Slide recap sections | Drop or briefly reference | "As we saw in [Article Name](/topics/slug/)" |
| Readings slide | Integrate as citations throughout | {% cite "key" %} inline |
| `#slide(repeat: N, self => {...})` | Merge revealed steps into prose | Callback-style animations become narrative |

### Pattern 4: Definition Blockquotes
**What:** Formal definitions use blockquote format with bold title.
**When to use:** For every `#definition(title: "...")` in the Typst source.
**Example:**
```markdown
> **Residual Stream:** The residual stream is the $d_{\text{model}}$-dimensional vector that flows through the transformer, starting as the token embedding and accumulating additive updates from each attention head and MLP layer.
```

### Pattern 5: Cross-Article Links
**What:** Links between articles for related concepts.
**When to use:** When one article references a concept covered in another article.
**Example:**
```markdown
[the residual stream](/topics/attention-mechanism/#the-residual-stream)
[superposition](/topics/superposition/)
```
**Convention:** Use the section anchor (slugified h2) when linking to a specific section.

### Anti-Patterns to Avoid
- **Preserving slide structure in prose:** Do NOT write "Slide 1: ..., Slide 2: ..." or keep the fragmented bullet-point style of slides. Convert to flowing narrative paragraphs.
- **Keeping recap sections:** Each week's Typst starts with a recap of the previous week. Articles are standalone -- replace recaps with cross-article links.
- **Keeping "readings" sections verbatim:** Integrate paper references as inline citations throughout the article using {% cite "key" %}.
- **Duplicating content across articles:** If weeks 7 and 8 both discuss the IOI circuit, the article should cover it once, not repeat.
- **Skipping pilot article content gaps:** The pilot articles cover only a fraction of their respective weeks. Weeks 1, 6, and 9 need expansion, not just "it's already done."
- **Creating one article per week:** The goal is thematic organization. Some weeks combine into one article (e.g., weeks 7-8 -> IOI circuit). Some weeks split into multiple articles (potentially).

## Proposed Article Inventory

### Thematic Mapping: Weeks to Articles

Based on the syllabus, source content analysis, and the goal of thematic organization, the proposed article set is:

**Block 1: Transformer Foundations (2 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `attention-mechanism` | The Attention Mechanism | Week 1 | None | foundational | EXISTS (pilot) - needs review for completeness |
| `transformer-circuits` | Transformer Circuits: QK, OV, and Composition | Week 2 | None | foundational | NEW |

Week 1 pilot article covers attention, residual stream, and transformer components comprehensively. Week 2 content (QK/OV circuits, composition, virtual attention heads, TransformerLens) is a distinct topic that deserves its own article.

**Block 2: Foundations of MI (3 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `what-is-mech-interp` | What is Mechanistic Interpretability? | Week 3 | None | foundational | NEW |
| `induction-heads` | Induction Heads and In-Context Learning | Week 4 | None | intermediate | NEW |
| `observational-tools` | Observational Tools: Looking Inside the Model | Week 5 | 3 PNGs | intermediate | NEW |

Week 3 is conceptual framing (MI landscape, features, circuits, universality, LRH). Week 4 is the canonical circuit discovery (induction heads, mathematical framework applied, DLA). Week 5 is methods (logit lens, tuned lens, probing). Each is a distinct theme.

**Block 3: From Observation to Causation (2-3 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `activation-patching` | Activation Patching and Causal Interventions | Week 6 | 3 PNGs | advanced | EXISTS (pilot) - needs review for completeness |
| `ioi-circuit` | The IOI Circuit: A Complete Case Study | Weeks 7-8 | 1 PNG | advanced | NEW |

The activation-patching pilot article already covers the week 6 content well (noising/denoising, attribution patching, path patching, worked IOI example). Review needed to confirm completeness vs. the full week 6 Typst. Weeks 7 and 8 are a two-part deep dive into a single case study (IOI) and should be combined into one article.

**Block 4: Superposition and Feature Extraction (3 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `superposition` | The Superposition Hypothesis | Week 9 | 6 PNGs | intermediate | EXISTS (pilot) - needs review for completeness |
| `sparse-autoencoders` | Sparse Autoencoders: Decomposing Superposition | Week 10 | 1 PNG | intermediate | NEW |
| `scaling-saes` | Scaling SAEs and Feature Geometry | Week 11 | None | advanced | NEW |

The superposition pilot article covers the core week 9 content well. Review needed for completeness. Week 10 (SAE architecture, training, monosemanticity) and Week 11 (scaling, Golden Gate, SAE limitations, variants) are distinct enough for separate articles.

**Block 5: Advanced Topics (3 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `steering` | Steering and Representation Engineering | Week 12 | 3 PNGs | advanced | NEW |
| `circuit-tracing` | Circuit Tracing at Scale | Week 13 | 3 PNGs | advanced | NEW |
| `model-diffing-universality` | Model Diffing, Universality, and Multimodal MI | Week 14 | 2 PNGs | advanced | NEW |

Each week covers a distinct advanced topic.

**Block 6: Synthesis and Open Frontiers (2 articles)**

| Article Slug | Title | Source Weeks | Diagrams | Difficulty | Status |
|-------------|-------|-------------|----------|------------|--------|
| `mi-safety` | Mechanistic Interpretability for AI Safety | Week 15 | 3 PNGs | advanced | NEW |
| `open-problems` | Open Problems and the Future of MI | Week 16 | 2 PNGs | advanced | NEW |

**Total: 16 articles (3 existing + 13 new)**

### Pilot Article Completeness Assessment

The 3 pilot articles were written as simplified introductions, not as complete week conversions. Assessment:

**attention-mechanism (Week 1):**
- Covers: attention mechanism, QK/V, multi-head attention, residual stream, layer norm, MLPs, positional encodings, full transformer
- Missing from Week 1: Prerequisites recap (ML/DL basics, linear algebra review) -- these are prerequisites, so omitting is correct for a web article
- Assessment: **Likely complete.** The pilot covers all major Week 1 topics. Minor review needed.

**activation-patching (Week 6):**
- Covers: clean/corrupted framework, noising/denoising, worked IOI example, attribution patching, path patching
- Missing from Week 6: Causal scrubbing mention (this is actually in Week 8 content)
- Assessment: **Likely complete.** The pilot article covers all Week 6 topics. Minor review needed.

**superposition (Week 9):**
- Covers: fundamental tension, toy model, phase diagrams, geometry (all polytopes), interference, why it makes MI hard
- Missing from Week 9: No obvious gaps
- Assessment: **Likely complete.** The pilot article is comprehensive. Minor review needed.

## Reference Inventory

### Current references.json (11 entries)
Already in references.json:
1. `alammar2018illustrated`
2. `bricken2023monosemanticity`
3. `conmy2023ioi`
4. `elhage2021mathematical`
5. `elhage2022toy`
6. `heimersheim2024patching`
7. `nanda2023attribution`
8. `olah2020zoom`
9. `olsson2022context`
10. `vaswani2017attention`
11. `wang2022ioi`

### References to add (~42 new entries)

The following references from SOURCES.md need to be added to references.json. Each needs: key, title, authors, year, venue, url.

**For Block 2 articles (weeks 3-5):**
- `bereska2024review` - Bereska & Gavves (2024), MI for AI Safety survey
- `nostalgebraist2020logitlens` - nostalgebraist (2020), Logit Lens blog post
- `belrose2023tunedlens` - Belrose et al. (2023), Tuned Lens
- `hewitt2019structural` - Hewitt & Manning (2019), Structural Probes
- `voita2020mdl` - Voita & Titov (2020), MDL Probing
- `elazar2021amnesic` - Elazar et al. (2021), Amnesic Probing
- `belinkov2022probing` - Belinkov (2022), Probing Classifiers survey

**For Block 3 articles (weeks 7-8):**
- `chan2022causalscrubbing` - Chan et al. (2022), Causal Scrubbing

**For Block 4 articles (weeks 10-11):**
- `templeton2024scaling` - Templeton et al. (2024), Scaling Monosemanticity
- `rajamanoharan2024gated` - Rajamanoharan et al. (2024), Gated SAEs
- `gao2024scaling` - Gao et al. (2024), Scaling and Evaluating SAEs
- `rajamanoharan2024jumprelu` - Rajamanoharan et al. (2024), JumpReLU SAEs
- `karvonen2025saebench` - Karvonen et al. (2025), SAEBench
- `chanin2024absorption` - Chanin et al. (2024), Feature Absorption

**For Block 5 articles (weeks 12-14):**
- `turner2024steering` - Turner et al. (2024), Activation Engineering / ActAdd
- `panickssery2024caa` - Panickssery et al. (2024), Contrastive Activation Addition
- `zou2023repe` - Zou et al. (2023), Representation Engineering
- `arditi2024refusal` - Arditi et al. (2024), Refusal Direction
- `todd2024function` - Todd et al. (2024), Function Vectors
- `belrose2023leace` - Belrose et al. (2023), LEACE
- `lindsey2025circuittracing` - Lindsey et al. (2025), Circuit Tracing
- `anthropic2025biology` - Anthropic (2025), Biology of an LLM
- `dunefsky2024transcoders` - Dunefsky et al. (2024), Transcoders
- `marks2024sparse` - Marks et al. (2024), Sparse Feature Circuits
- `gurnee2024universal` - Gurnee et al. (2024), Universal Neurons
- `lin2025multimodal` - Lin et al. (2025), Multimodal MI Survey

**For Block 6 articles (weeks 15-16):**
- `hubinger2024sleeper` - Hubinger et al. (2024), Sleeper Agents
- `greenblatt2024alignment` - Greenblatt et al. (2024), Alignment Faking
- `anthropic2024probes` - Anthropic (2024), Probes Catch Sleeper Agents
- `nanda2022openproblems` - Nanda (2022), 200 Concrete Open Problems
- `sharkey2025openproblems` - Sharkey et al. (2025), Open Problems in MI

**Supplementary (may be referenced across articles):**
- `rai2024practical` - Rai et al. (2024), Practical Review of MI
- `park2023lrh` - Park et al. (2023), Linear Representation Hypothesis (if cited)

**Total new references: ~32 (exact count depends on which supplementary references are actually cited)**

Each reference entry has this JSON format (from existing references.json):
```json
{
  "key": {
    "title": "Full Paper Title",
    "authors": "LastName, F., LastName, F., et al.",
    "year": 2024,
    "venue": "Conference/Journal/Blog post",
    "url": "https://..."
  }
}
```

## Diagram Placement Plan

### All 27 PNGs by Article

| Diagram File | Source Week | Target Article | Alt Text Needed | Already Placed |
|-------------|-----------|----------------|-----------------|----------------|
| `attn_induction.png` | week-05 | `observational-tools` | Induction head attention pattern | No |
| `attn_prev_token.png` | week-05 | `observational-tools` | Previous token head attention pattern | No |
| `logit_lens_eiffel.png` | week-05 | `observational-tools` | Logit lens output example | No |
| `act_patch_setup.png` | week-06 | `activation-patching` | Activation patching setup | Yes (pilot) |
| `act_patch_layers.png` | week-06 | `activation-patching` | Layer-by-layer patching results | Yes (pilot) |
| `act_patch_heads.png` | week-06 | `activation-patching` | Head-level patching heatmap | Yes (pilot) |
| `ioi_circuit_diagram.png` | week-08 | `ioi-circuit` | Full IOI circuit diagram | No |
| `phase_diagram.png` | week-09 | `superposition` | Phase diagram for superposition | Yes (pilot) |
| `superposition_1d_antipodal.png` | week-09 | `superposition` | 1D antipodal encoding | Yes (pilot) |
| `superposition_2d_orthogonal.png` | week-09 | `superposition` | 2D orthogonal features | Yes (pilot) |
| `superposition_2d_pentagon.png` | week-09 | `superposition` | 2D pentagon packing | Yes (pilot) |
| `superposition_2d_triangle.png` | week-09 | `superposition` | 2D triangle packing | Yes (pilot) |
| `superposition_3d_packing.png` | week-09 | `superposition` | 3D octahedron packing | Yes (pilot) |
| `sae_architecture.png` | week-10 | `sparse-autoencoders` | SAE encoder-decoder architecture | No |
| `refusal_ablation_results.png` | week-12 | `steering` | Refusal ablation results | No |
| `refusal_direction_schematic.png` | week-12 | `steering` | Refusal direction schematic | No |
| `steering_method_comparison.png` | week-12 | `steering` | Comparison of steering methods | No |
| `attribution_graph_schematic.png` | week-13 | `circuit-tracing` | Attribution graph structure | No |
| `circuit_evolution.png` | week-13 | `circuit-tracing` | Circuit analysis evolution | No |
| `transcoder_vs_sae.png` | week-13 | `circuit-tracing` | Transcoder vs SAE comparison | No |
| `crosscoder_model_diffing.png` | week-14 | `model-diffing-universality` | Crosscoder model diffing | No |
| `universality_evidence.png` | week-14 | `model-diffing-universality` | Evidence for universality | No |
| `mi_safety_assessment.png` | week-15 | `mi-safety` | MI safety assessment | No |
| `safety_applications_overview.png` | week-15 | `mi-safety` | Safety applications overview | No |
| `sleeper_agent_detection.png` | week-15 | `mi-safety` | Sleeper agent detection | No |
| `field_assessment_timeline.png` | week-16 | `open-problems` | Field assessment timeline | No |
| `open_problems_taxonomy.png` | week-16 | `open-problems` | Open problems taxonomy | No |

**Image handling:** Copy PNGs from `/Users/ivan/latex/mech-interp-course/week-XX/assets/` to `/Users/ivan/src/learn-mech-interp/src/topics/{article-slug}/images/`. The Eleventy passthrough copy rule `src/topics/*/images` already handles this.

## learningPath.json Update Plan

The final learningPath.json should contain all 6 blocks with all topics in reading order:

```json
{
  "blocks": [
    {
      "slug": "transformer-foundations",
      "title": "Transformer Foundations",
      "topics": [
        { "slug": "attention-mechanism", "title": "The Attention Mechanism" },
        { "slug": "transformer-circuits", "title": "Transformer Circuits: QK, OV, and Composition" }
      ]
    },
    {
      "slug": "foundations-of-mi",
      "title": "Foundations of MI",
      "topics": [
        { "slug": "what-is-mech-interp", "title": "What is Mechanistic Interpretability?" },
        { "slug": "induction-heads", "title": "Induction Heads and In-Context Learning" },
        { "slug": "observational-tools", "title": "Observational Tools: Looking Inside the Model" }
      ]
    },
    {
      "slug": "observation-to-causation",
      "title": "Observation to Causation",
      "topics": [
        { "slug": "activation-patching", "title": "Activation Patching and Causal Interventions" },
        { "slug": "ioi-circuit", "title": "The IOI Circuit: A Complete Case Study" }
      ]
    },
    {
      "slug": "superposition-and-feature-extraction",
      "title": "Superposition & Feature Extraction",
      "topics": [
        { "slug": "superposition", "title": "The Superposition Hypothesis" },
        { "slug": "sparse-autoencoders", "title": "Sparse Autoencoders: Decomposing Superposition" },
        { "slug": "scaling-saes", "title": "Scaling SAEs and Feature Geometry" }
      ]
    },
    {
      "slug": "advanced-topics",
      "title": "Advanced Topics",
      "topics": [
        { "slug": "steering", "title": "Steering and Representation Engineering" },
        { "slug": "circuit-tracing", "title": "Circuit Tracing at Scale" },
        { "slug": "model-diffing-universality", "title": "Model Diffing, Universality, and Multimodal MI" }
      ]
    },
    {
      "slug": "synthesis-and-frontiers",
      "title": "Synthesis & Open Frontiers",
      "topics": [
        { "slug": "mi-safety", "title": "Mechanistic Interpretability for AI Safety" },
        { "slug": "open-problems", "title": "Open Problems and the Future of MI" }
      ]
    }
  ]
}
```

Note: The existing block slugs `transformer-foundations`, `superposition-and-feature-extraction`, and `observation-to-causation` are already used by the pilot articles' front matter and must be preserved. New block slugs (`foundations-of-mi`, `advanced-topics`, `synthesis-and-frontiers`) follow the same kebab-case pattern.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Citation tooltips | Manual HTML citation markup | `{% cite "key" %}` shortcode | Auto-numbering, consistent tooltip format, references.json lookup |
| Margin notes | Manual aside elements | `{% sidenote "content" %}` shortcode | Numbered, responsive, togglable on mobile |
| Figure captions | Raw `<figure>` HTML | `![alt](src "Figure N: caption")` markdown-it-figure syntax | Consistent styling, semantic HTML |
| Table of contents | Manual anchor lists | Automatic from h2/h3 via eleventy-plugin-toc | Updates when headings change |
| Navigation ordering | Manual prev/next links | learningPath.json ordering + learningPath collection | Single source of truth for article order |
| Sidebar categories | Manual sidebar HTML | eleventyNavigation + `block` front matter | Automatic hierarchy from data |
| Heading anchor IDs | Manual id attributes | IdAttributePlugin + @sindresorhus/slugify | Consistent slug format |

**Key insight:** All content-type rendering is already built and working. The author just writes Markdown with shortcodes. No infrastructure work is needed -- only content authoring.

## Common Pitfalls

### Pitfall 1: Preserving Slide Structure in Articles
**What goes wrong:** Articles read like "Slide 1: here is a bullet. Slide 2: here is another bullet." The fragmented, incremental-reveal structure of slides produces choppy, hard-to-read articles.
**Why it happens:** Direct transcription of slide content without narrative restructuring.
**How to avoid:** Treat the Typst source as source material, not as a template. Each article section should flow as connected paragraphs. Merge related slide content into coherent narrative. A slide's 3 bullets become one paragraph with connecting sentences.
**Warning signs:** More than 2 consecutive one-sentence paragraphs. Bulleted lists longer than 5 items. Sections with no prose paragraphs.

### Pitfall 2: KaTeX vs Typst Math Syntax Mismatches
**What goes wrong:** Typst math like `W_(Q K)^h` is pasted directly into Markdown, where KaTeX does not understand Typst syntax. The math renders as errors or garbled output.
**Why it happens:** Typst and LaTeX/KaTeX have different math syntax. Typst uses `_()` for subscript groups; KaTeX uses `_{}`. Typst uses `RR` for blackboard bold; KaTeX uses `\mathbb{R}`. Custom operators from globals.typ (attn, mlp, embed) do not exist in KaTeX.
**How to avoid:** Use the conversion table in the Architecture Patterns section. Every math expression must be manually translated. Common traps: `d_"model"` -> `d_{\text{model}}`, `sum_(l=0)^(L-1)` -> `\sum_{l=0}^{L-1}`, `vx` -> `\mathbf{x}`.
**Warning signs:** Red error boxes in rendered math. `\undefined` errors in browser console. Math that looks like source code rather than formatted equations.

### Pitfall 3: Missing or Incorrect Cross-Article Links
**What goes wrong:** An article references a concept covered in another article but uses a broken link, wrong anchor, or no link at all. The reader cannot follow the cross-reference.
**Why it happens:** When articles are written independently, cross-references are easy to forget. Anchor IDs depend on the exact heading text, which may change.
**How to avoid:** When referencing a concept from another article, immediately add the cross-link. Use the heading text to derive the anchor: "The Residual Stream" -> `#the-residual-stream`. Test links after all articles are written.
**Warning signs:** Mentions of "as we saw earlier" or "in a previous article" without an actual link. 404 errors when clicking cross-references.

### Pitfall 4: Inconsistent Front Matter
**What goes wrong:** Articles have inconsistent or missing front matter fields. Block slugs do not match learningPath.json. Prerequisites reference non-existent articles. Difficulty levels are inconsistent.
**Why it happens:** 16 articles written over multiple sessions without checking consistency.
**How to avoid:** Define all front matter before writing content. Validate block slugs against learningPath.json. Only list prerequisites that actually exist as articles. Use the difficulty scale consistently: foundational (no MI prereqs), intermediate (builds on foundations), advanced (requires multiple prior articles).
**Warning signs:** Article not appearing in sidebar. Wrong block in breadcrumbs. "Read first" links pointing to 404s.

### Pitfall 5: Treating Pilot Articles as Complete
**What goes wrong:** The existing pilot articles (attention-mechanism, activation-patching, superposition) are assumed to fully cover their respective weeks, so week 1, 6, and 9 content is skipped during migration.
**Why it happens:** The pilots exist and render correctly, creating an illusion of completeness.
**How to avoid:** Compare each pilot article against the full Typst source for its week. Check that every major section, equation, and concept from the Typst is represented. For weeks 1, 6, and 9, the task is "review and expand if needed," not "skip."
**Warning signs:** A week's Typst has 800+ lines but the corresponding article is 200 lines. Key concepts from the syllabus learning objectives do not appear in the article.

### Pitfall 6: Reference Key Conflicts
**What goes wrong:** A new reference key conflicts with an existing one, or two references use inconsistent key formats.
**Why it happens:** No systematic key naming convention was documented; pilot articles used ad-hoc keys.
**How to avoid:** Follow the established pattern: `authorYYYYkeyword` (e.g., `elhage2022toy`, `heimersheim2024patching`). For multi-author papers, use the first author's last name. For blog posts with handles, use the handle (e.g., `nostalgebraist2020logitlens`). Check for existing keys before adding.
**Warning signs:** `[??]` rendering in articles (citation shortcode returns error for unknown key). Duplicate keys in references.json.

### Pitfall 7: Article Length Imbalance
**What goes wrong:** Some articles are 3,000 words while others are 300 words, creating an inconsistent reading experience. Or one article tries to cover too much and becomes unwieldy.
**Why it happens:** Source weeks vary from 658 to 1,102 lines of Typst. Some topics are inherently more complex. Combining two weeks (e.g., 7-8 for IOI) creates extra-long articles.
**How to avoid:** Target 1,500-3,000 words per article (the pilot articles are ~1,500-2,500 words). If a combined-week article exceeds 4,000 words, consider splitting. If a single-week article is under 1,000 words, consider whether it should be merged with a related topic.
**Warning signs:** TOC has more than 8 h2 sections. Article takes more than 15 minutes to read. Article has fewer than 3 h2 sections.

## Code Examples

### Creating a New Article Directory
```bash
# Create article directory with images
mkdir -p src/topics/sparse-autoencoders/images

# Copy diagrams from source
cp /Users/ivan/latex/mech-interp-course/week-10/assets/sae_architecture.png \
   src/topics/sparse-autoencoders/images/
```

### Article Front Matter Example (New Article)
```markdown
---
title: "Sparse Autoencoders: Decomposing Superposition"
description: "How sparse autoencoders learn an overcomplete dictionary of monosemantic features, decomposing the polysemantic activations that superposition creates."
prerequisites:
  - title: "The Superposition Hypothesis"
    url: "/topics/superposition/"
difficulty: "intermediate"
block: "superposition-and-feature-extraction"
category: "methods"
---
```

### Adding a Reference to references.json
```json
{
  "templeton2024scaling": {
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "authors": "Templeton, A., Conerly, T., Marcus, J., et al.",
    "year": 2024,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
  }
}
```

### Typst-to-Markdown Conversion Example

**Typst source (from week-10):**
```typst
#definition(title: "Sparse Autoencoder")[
  A *sparse autoencoder* takes an activation vector $bold(x) in bb(R)^n$,
  projects it to a wider latent space $bold(f) in bb(R)^m$ ($m >> n$),
  applies ReLU for sparsity, and reconstructs:
  $ hat(bold(x)) = bold(W)_"dec" dot bold(f) + bold(b)_"dec" $
]
```

**Markdown output:**
```markdown
> **Sparse Autoencoder:** A sparse autoencoder takes an activation vector $\mathbf{x} \in \mathbb{R}^n$, projects it to a wider latent space $\mathbf{f} \in \mathbb{R}^m$ ($m \gg n$), applies ReLU for sparsity, and reconstructs:

$$
\hat{\mathbf{x}} = \mathbf{W}_{\text{dec}} \cdot \mathbf{f} + \mathbf{b}_{\text{dec}}
$$
```

### Image Placement Example
```markdown
![Diagram showing the SAE architecture with encoder projecting from model dimension to a wider latent space, ReLU activation, and decoder projecting back.](/topics/sparse-autoencoders/images/sae_architecture.png "Figure 1: The sparse autoencoder architecture. The encoder projects activations to a wider latent space with ReLU sparsity, and the decoder reconstructs the original activations from the sparse features.")
```

## Batch Execution Strategy

The 16 articles should be produced in 6 batches corresponding to the 6 course blocks. Each batch is independently shippable:

### Batch 1: Block 1 - Transformer Foundations
- Review `attention-mechanism` pilot for completeness vs Week 1 Typst
- Create `transformer-circuits` from Week 2 (934 lines Typst)
- Add any missing references
- Update learningPath.json Block 1

### Batch 2: Block 2 - Foundations of MI
- Create `what-is-mech-interp` from Week 3 (747 lines)
- Create `induction-heads` from Week 4 (790 lines)
- Create `observational-tools` from Week 5 (801 lines, 3 PNGs)
- Add ~7 references (probing papers, logit lens, tuned lens, Bereska survey)
- Update learningPath.json Block 2

### Batch 3: Block 3 - Observation to Causation
- Review `activation-patching` pilot for completeness vs Week 6 Typst
- Create `ioi-circuit` from Weeks 7-8 (945+1102 = 2047 lines, 1 PNG)
- Add ~1 reference (causal scrubbing)
- Update learningPath.json Block 3

### Batch 4: Block 4 - Superposition and Feature Extraction
- Review `superposition` pilot for completeness vs Week 9 Typst
- Create `sparse-autoencoders` from Week 10 (817 lines, 1 PNG)
- Create `scaling-saes` from Week 11 (1086 lines)
- Add ~6 references (Templeton, SAE variants papers)
- Update learningPath.json Block 4

### Batch 5: Block 5 - Advanced Topics
- Create `steering` from Week 12 (940 lines, 3 PNGs)
- Create `circuit-tracing` from Week 13 (987 lines, 3 PNGs)
- Create `model-diffing-universality` from Week 14 (932 lines, 2 PNGs)
- Add ~12 references (steering papers, circuit tracing, multimodal)
- Update learningPath.json Block 5

### Batch 6: Block 6 - Synthesis and Open Frontiers
- Create `mi-safety` from Week 15 (1051 lines, 3 PNGs)
- Create `open-problems` from Week 16 (1077 lines, 2 PNGs)
- Add ~5 references (sleeper agents, alignment faking, open problems)
- Update learningPath.json Block 6
- Final cross-link audit across all articles

### Estimated Volume Per Batch
| Batch | Articles | Source Lines | Diagrams | New Refs | Estimated Words |
|-------|----------|-------------|----------|----------|-----------------|
| 1 | 1 new + 1 review | 934 | 0 | 0 | ~2,000 |
| 2 | 3 new | 2,338 | 3 | ~7 | ~6,000 |
| 3 | 1 new + 1 review | 2,047 | 1 | ~1 | ~3,000 |
| 4 | 2 new + 1 review | 1,903 | 1 | ~6 | ~4,000 |
| 5 | 3 new | 2,859 | 8 | ~12 | ~6,000 |
| 6 | 2 new | 2,128 | 5 | ~5 | ~4,000 |
| **Total** | **13 new + 3 review** | **14,744** | **27** | **~32** | **~25,000** |

## Content Conversion Checklist (Per Article)

For each article, verify:

- [ ] Front matter complete: title, description, prerequisites, difficulty, block, category
- [ ] Block slug matches learningPath.json
- [ ] Prerequisites reference only articles that exist
- [ ] All major topics from syllabus learning objectives are covered
- [ ] Prose is narrative (not bullet-point fragments)
- [ ] All math is valid KaTeX (no Typst syntax remnants)
- [ ] All diagrams have descriptive alt text and "Figure N:" captions
- [ ] All paper references use {% cite "key" %} with valid keys in references.json
- [ ] 2 pause-and-think prompts per article (adapted from source exercises)
- [ ] 2-4 sidenotes per article for supplementary context
- [ ] Cross-article links to related concepts
- [ ] No slide-specific artifacts (#pause, recap sections, readings lists)
- [ ] Article slug is in learningPath.json
- [ ] Renders correctly with `npx @11ty/eleventy --serve`

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Slide-per-page format | Long-form articles | Phase 4 (pilot articles) | Content must be rewritten as prose, not transcribed |
| Week-based organization | Thematic organization | Phase 4 decision | Articles grouped by topic, not by lecture week |
| 1:1 week-to-article | Many-to-one / one-to-many | Phase 5.1 design | Weeks 7-8 merge into one article; most weeks stay 1:1 |

**Deprecated/outdated:**
- The test article at `src/topics/test/index.md` exists for rendering verification only. It is not part of the learningPath and should be left as-is (not deleted, not converted).

## Open Questions

1. **Whether weeks 7-8 should be one or two articles**
   - What we know: Both weeks cover the IOI circuit. Week 7 introduces the task and core mechanism (Name Movers, Duplicate Token Heads, S-Inhibition Heads). Week 8 covers Negative Name Movers, Backup Name Movers, circuit evaluation, and causal scrubbing.
   - What's unclear: Whether 2,047 lines of combined source material produces an article that is too long (>4,000 words).
   - Recommendation: Start as one article. If it exceeds 4,000 words, split into "The IOI Circuit" and "Circuit Evaluation and Validation." The planner should plan for one article but note the split option.

2. **Whether the attention-mechanism pilot article needs expansion**
   - What we know: The pilot article is ~207 lines of Markdown and covers all major Week 1 topics from the syllabus.
   - What's unclear: Whether it covers them with sufficient depth compared to the 658-line Typst source.
   - Recommendation: Compare section by section. The pilot likely covers the substance (it reads well and is comprehensive). A review task should confirm, not assume, completeness. If gaps are found, expand the pilot.

3. **Whether Week 14's three sub-topics (model diffing, universality, multimodal) should be one article or three**
   - What we know: Week 14 covers three loosely related topics: crosscoders/model diffing, universality across models, and multimodal MI. The Typst source is 932 lines.
   - What's unclear: Whether the three sub-topics are connected enough for one coherent article.
   - Recommendation: One article titled "Model Diffing, Universality, and Multimodal MI." The connecting theme is "comparing and extending MI across models and modalities." If the article feels disjointed, the planner can split.

4. **Exact difficulty assignments for new articles**
   - What we know: The pilot articles use foundational (attention), intermediate (superposition), and advanced (activation-patching). Block 1 should be foundational, blocks 5-6 should be advanced.
   - What's unclear: Where exactly the intermediate/advanced boundary falls for block 2-4 articles.
   - Recommendation: Use the mapping in the article inventory table above. The planner can adjust based on actual content depth.

## Sources

### Primary (HIGH confidence)
- Existing pilot articles at `/Users/ivan/src/learn-mech-interp/src/topics/*/index.md` -- established all patterns
- `eleventy.config.js` at `/Users/ivan/src/learn-mech-interp/eleventy.config.js` -- all rendering infrastructure
- `learningPath.json` at `/Users/ivan/src/learn-mech-interp/src/_data/learningPath.json` -- navigation data structure
- `references.json` at `/Users/ivan/src/learn-mech-interp/src/_data/references.json` -- citation data structure
- `topics.11tydata.js` at `/Users/ivan/src/learn-mech-interp/src/topics/topics.11tydata.js` -- front matter schema
- `SYLLABUS.md` at `/Users/ivan/latex/mech-interp-course/SYLLABUS.md` -- week-by-week topic coverage
- `SOURCES.md` at `/Users/ivan/latex/mech-interp-course/SOURCES.md` -- complete reference catalog
- `STATE.md` at `/Users/ivan/src/learn-mech-interp/.planning/STATE.md` -- all prior decisions
- All 16 week Typst files at `/Users/ivan/latex/mech-interp-course/week-*/week-*.typ` -- source content
- `globals.typ` at `/Users/ivan/latex/mech-interp-course/globals.typ` -- math notation reference

### Secondary (MEDIUM confidence)
- Course research files at `/Users/ivan/latex/mech-interp-course/.planning/phases/*/` -- domain content organization
- Previous phase research at `/Users/ivan/src/learn-mech-interp/.planning/phases/01-foundation-deployment/01-RESEARCH.md` -- established patterns

### Tertiary (LOW confidence)
- None. This phase is purely content authoring using established infrastructure. No external tools or libraries to validate.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- no new tools or libraries needed; all infrastructure is built and verified
- Architecture patterns: HIGH -- all patterns derived from existing pilot articles and verified infrastructure
- Article inventory: HIGH -- derived systematically from syllabus, source content, and thematic analysis
- Conversion patterns: HIGH -- derived from comparing Typst source to pilot article output
- Reference inventory: HIGH -- derived from complete SOURCES.md catalog
- Diagram placement: HIGH -- derived from complete PNG inventory and thematic article mapping
- Batch strategy: HIGH -- follows course block structure with clear dependencies
- Pitfalls: MEDIUM -- based on analysis of conversion challenges, not yet validated by execution

**Research date:** 2026-02-04
**Valid until:** 2026-06-04 (90 days -- content authoring, no library dependencies to go stale)
