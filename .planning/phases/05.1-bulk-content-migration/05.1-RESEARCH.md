# Phase 5.1: Bulk Content Migration - Research

**Researched:** 2026-02-04
**Updated:** 2026-02-04 (granular article inventory replacing 1-per-week structure)
**Domain:** Converting 16 weeks of Typst slide content into thematic long-form Markdown articles for an Eleventy static site
**Confidence:** HIGH

## Summary

Phase 5.1 converts all 16 weeks of Typst course content (~14,744 lines across 16 files) into **granular, topic-focused** long-form Markdown articles, reorganized by topic rather than by lecture week. Each major concept, method, or technique gets its own article (or at most a tightly-coupled pair shares one). The result is **35 articles** (3 existing pilots + 32 new), not 16.

The research investigated six areas: (1) the established article authoring patterns from the 3 pilot articles, (2) the Typst-to-Markdown conversion patterns including math notation translation, (3) the granular thematic article inventory with week-to-article mapping, (4) the complete reference catalog needing addition to references.json, (5) the diagram placement plan for all 27 PNGs, and (6) the learningPath.json and cross-linking structure needed for full site coverage.

Three pilot articles already exist (attention-mechanism from week 1, activation-patching from week 6, superposition from week 9), establishing patterns for front matter, citations, sidenotes, figures, definitions, pause-and-think prompts, and cross-article links. These pilots cover only a fraction of their respective weeks' content and need expansion. The remaining 13 weeks have no articles at all. The conversion is not 1:1 (slides to paragraphs) but a substantive rewrite: slide bullet points become narrative prose, definitions become blockquotes, slide-specific elements (#pause, #focus-slide, two-column layouts) are dropped, and the content is reorganized by theme rather than by weekly lecture sequence.

The site infrastructure (Eleventy, KaTeX, citations, sidenotes, sidebar, learning path, prev/next navigation) is complete and working. New articles automatically appear in navigation when added to learningPath.json with matching front matter. The main work is content authoring -- there is no infrastructure to build.

**Primary recommendation:** Organize work into 8 batches by course block. For each batch: (1) add all needed references to references.json, (2) create/expand article directories with images, (3) write the articles converting Typst to narrative Markdown, (4) update learningPath.json with new topics, and (5) verify rendering. Each batch is independently shippable. Pilot articles should be expanded first (they are partial) before moving to new content.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| @11ty/eleventy | 3.0.0 | Static site generator | Already installed, all infrastructure built |
| markdown-it | * | Markdown processing | Configured with KaTeX, figure, anchor plugins |
| @mdit/plugin-katex | * | LaTeX math rendering | Already configured with htmlAndMathml output |
| @mdit/plugin-figure | * | Image captions | Already configured for `![alt](src "caption")` syntax |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| @sindresorhus/slugify | * | Heading ID generation | Already configured for TOC/anchor links |
| eleventy-plugin-toc | * | Table of contents | Already configured, reads h2/h3 |
| @11ty/eleventy-navigation | * | Sidebar hierarchy | Already configured, reads eleventyNavigation from front matter |

### Alternatives Considered
None. The stack is fully established. No new libraries needed for content migration.

**Installation:**
No installation needed. All dependencies are already in place.

## Architecture Patterns

### Article File Structure
```
src/topics/
  {topic-slug}/
    index.md              # Article content with front matter
    images/               # Article-specific images (PNGs)
      {diagram-name}.png
```

Each article follows this template (derived from pilot articles):

```markdown
---
title: "Article Title"
description: "One-sentence description for meta tags and article header."
prerequisites:
  - title: "Prerequisite Article Title"
    url: "/topics/prerequisite-slug/"
difficulty: "foundational|intermediate|advanced"
block: "block-slug-from-learningPath"
category: "core-concepts|methods|applications"
---

## First Major Section

Narrative prose converting slide content...

> **Definition Name:** Blockquote format for formal definitions.

$$
\text{Display math in KaTeX}
$$

Inline math like $x \in \mathbb{R}^n$ uses dollar signs.

![Alt text for accessibility](/topics/slug/images/diagram.png "Figure N: Caption text")

{% cite "referenceKey" %} for numbered citations with hover tooltips.

{% sidenote "Supplementary context that appears in the margin on wide screens." %}

[Cross-article link text](/topics/other-article/#section-anchor)

<details class="pause-and-think">
<summary>Pause and think: Prompt title</summary>

Discussion question adapted from course exercises.

</details>
```

### Pattern 1: Front Matter Schema
**What:** Every article requires these front matter fields.
**When to use:** Every article.
**Fields:**
- `title`: Quoted string, rendered by layout (not duplicated in content)
- `description`: One-sentence summary for header and meta
- `prerequisites`: Array of `{title, url}` objects (empty `[]` for foundational articles)
- `difficulty`: One of `foundational`, `intermediate`, `advanced`
- `block`: Slug matching a block in learningPath.json
- `category`: One of `core-concepts`, `methods`, `applications` (used by pilot articles)
- `order`: Optional integer for ordering within a block (topics.11tydata.js reads it)

### Pattern 2: Typst Math to KaTeX Conversion
**What:** Systematic translation rules from Typst math syntax to KaTeX.
**When to use:** Every math expression in every article.

| Typst | KaTeX | Notes |
|-------|-------|-------|
| `$...$` (inline) | `$...$` | Same delimiter |
| `$ ... $` (display, on own line) | `$$\n...\n$$` | Must be on separate lines |
| `vx`, `vh`, `vW` (custom ops in globals.typ) | `\mathbf{x}`, `\mathbf{h}`, `\mathbf{W}` | Bold for vectors/matrices |
| `RR^n` | `\mathbb{R}^n` | Blackboard bold |
| `W_(Q K)^h` | `W_{QK}^h` | Subscript grouping |
| `sum_(l=0)^(L-1)` | `\sum_{l=0}^{L-1}` | Sum notation |
| `d_"model"` | `d_{\text{model}}` | Text in subscripts |
| `"Attn"`, `attn` (custom op) | `\text{Attn}` | Operator names |
| `embed(x)` (custom op) | `\text{Embed}(x)` | Custom operators become \text |
| `vec(1, 0, 1, -1)` | Not used in articles | Column vectors not needed in prose |
| `mat(...)` | Not used in articles | Matrices not needed in prose |
| `underbrace(expr, "label")` | `\underbrace{expr}_{\text{label}}` | Underbraces |
| `alpha_(i,j)` | `\alpha_{i,j}` | Greek with subscripts |
| `partial cal(L) \/ partial vW` | `\frac{\partial \mathcal{L}}{\partial \mathbf{W}}` | Partial derivatives |
| `bold("u")` | `\mathbf{u}` | Bold vectors |
| `bb(R)` | `\mathbb{R}` | Blackboard bold |

### Pattern 3: Typst Slide Elements to Markdown
**What:** How slide-specific Typst constructs map to article format.
**When to use:** During conversion of every week file.

| Typst Slide Element | Article Equivalent | Notes |
|---------------------|-------------------|-------|
| `= Section Title` | `## Section Title` | Section headings become h2 |
| `== Slide Title` | Content flows under current h2 | Slide titles are NOT separate headings; merge content |
| `#pause` | Drop entirely | Incremental reveals have no article equivalent |
| `#focus-slide[...]` | Bold paragraph or pull quote | Key takeaway becomes emphasized prose |
| `#definition(title: "Name")[...]` | `> **Name:** content` | Blockquote definition format |
| `#pause-and-think[...]` | `<details class="pause-and-think">` | Two per article (established pattern) |
| `#slide(composer: (1fr, 1fr))[...][...]` | Merge into single-column prose | Two-column layouts linearize |
| `#image("assets/file.png")` | `![alt](/topics/slug/images/file.png "Figure N: caption")` | Figure with caption |
| `#table(...)` | Markdown table | `\| Header \| ...\|` format |
| `#align(center)[...]` | Drop alignment; content flows naturally | |
| Slide recap sections | Drop or briefly reference | "As we saw in [Article Name](/topics/slug/)" |
| Readings slide | Integrate as citations throughout | {% cite "key" %} inline |
| `#slide(repeat: N, self => {...})` | Merge revealed steps into prose | Callback-style animations become narrative |

### Pattern 4: Definition Blockquotes
**What:** Formal definitions use blockquote format with bold title.
**When to use:** For every `#definition(title: "...")` in the Typst source.
**Example:**
```markdown
> **Residual Stream:** The residual stream is the $d_{\text{model}}$-dimensional vector that flows through the transformer, starting as the token embedding and accumulating additive updates from each attention head and MLP layer.
```

### Pattern 5: Cross-Article Links
**What:** Links between articles for related concepts.
**When to use:** When one article references a concept covered in another article.
**Example:**
```markdown
[the residual stream](/topics/attention-mechanism/#the-residual-stream)
[superposition](/topics/superposition/)
```
**Convention:** Use the section anchor (slugified h2) when linking to a specific section.

### Anti-Patterns to Avoid
- **Preserving slide structure in prose:** Do NOT write "Slide 1: ..., Slide 2: ..." or keep the fragmented bullet-point style of slides. Convert to flowing narrative paragraphs.
- **Keeping recap sections:** Each week's Typst starts with a recap of the previous week. Articles are standalone -- replace recaps with cross-article links.
- **Keeping "readings" sections verbatim:** Integrate paper references as inline citations throughout the article using {% cite "key" %}.
- **Duplicating content across articles:** If two source weeks both discuss the same concept, the article should cover it once, not repeat.
- **Skipping pilot article content gaps:** The pilot articles cover only a fraction of their respective weeks. Weeks 1, 6, and 9 need expansion, not just "it's already done."
- **Creating one article per week:** The goal is granular thematic organization. Each major concept or method gets its own article. Weeks routinely split into 2-4 articles.

## Granular Article Inventory

### Design Principles

The article inventory follows these principles:
1. **One article per major concept/method/technique.** A "major" topic is one that has its own section heading in the Typst source, its own definition, or its own paper.
2. **Tightly coupled topics may share an article.** The logit lens and tuned lens share one article because the tuned lens is a direct improvement on the logit lens. ActAdd and CAA share one because CAA is a direct extension of ActAdd.
3. **Loosely coupled topics in the same week get separate articles.** Week 12 covers ActAdd/CAA, RepE, the refusal direction, LEACE, and function vectors -- these are distinct techniques that deserve separate treatment.
4. **Target 1,500-2,500 words per article.** This is the sweet spot established by the pilot articles.
5. **Every article must be self-contained** with cross-links to prerequisites, not dependent on reading order within a week.

### Thematic Mapping: Weeks to Articles

**Block 1: Transformer Foundations (3 articles, from Weeks 1-2)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 1 | `attention-mechanism` | The Attention Mechanism | W1 S1-S6 | Prerequisites Recap, The Attention Mechanism, Multi-Head Attention, The Residual Stream, Layer Norms/MLPs/Positional Encodings, The Full Transformer | None | foundational | EXISTS (pilot) -- review for completeness |
| 2 | `qk-ov-circuits` | QK and OV Circuits | W2 S2-S3 | The Residual Stream as Vector Space, QK and OV Circuits (full worked example) | None | foundational | NEW |
| 3 | `composition-and-virtual-heads` | Composition and Virtual Attention Heads | W2 S4-S6 | Virtual Attention Heads and Composition (V/K/Q-composition, two-layer expansion, combinatorial richness), TransformerLens vocabulary | None | foundational | NEW |

**Rationale:** Week 1 is the existing pilot (covers attention, residual stream, MLPs, positional encodings). Week 2 has two distinct conceptual units: the QK/OV circuit decomposition (with its worked example) and the composition framework (virtual heads, V/K/Q-composition). These are distinct enough that separating them keeps each article focused. The TransformerLens vocabulary naturally pairs with composition since it introduces the naming conventions researchers use.

**Block 2: Foundations of MI (5 articles, from Weeks 3-5)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 4 | `what-is-mech-interp` | What is Mechanistic Interpretability? | W3 S2-S3 | The Interpretability Landscape (black-box vs white-box, post-hoc vs intrinsic, correlation vs causation), The Three Claims (features, circuits, universality) | None | foundational | NEW |
| 5 | `linear-representation-hypothesis` | The Linear Representation Hypothesis | W3 S4-S5 | The LRH (features as directions, why linear, empirical evidence, connection to superposition), Polysemanticity: Why Neurons Fail | None | foundational | NEW |
| 6 | `induction-heads` | Induction Heads and In-Context Learning | W4 S2-S5 | One-layer models (end-to-end QK/OV), Two-layer models (composition in action), Induction Heads (the discovery, the two-step mechanism, the phase change) | None | intermediate | NEW |
| 7 | `direct-logit-attribution` | Direct Logit Attribution | W4 S6-S7 | DLA (the key insight, decomposition, per-token attribution, as screening tool), Reading Attention Patterns (common patterns, limitations) | None | intermediate | NEW |
| 8 | `logit-lens-and-probing` | The Logit Lens, Tuned Lens, and Probing Classifiers | W5 S2-S7 | The Logit Lens, The Tuned Lens, Probing (the promise, structural probes, MDL probing, amnesic probing, the critique), The Key Limitation (observation vs causation) | 3 PNGs | intermediate | NEW |

**Rationale:** Week 3 splits naturally into two articles: the MI landscape + three claims (conceptual framing) and the LRH + polysemanticity (the mathematical basis for features). Week 4 splits into induction heads (the circuit discovery story) and DLA + attention patterns (the observational tools introduced in Week 4). Week 5 stays as one article because the logit lens, tuned lens, and probing are all observational tools with a shared narrative arc that builds to the "correlation vs causation" punchline. Splitting probing into its own article would leave both articles too thin.

**Block 3: From Observation to Causation (4 articles, from Weeks 6-8)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 9 | `activation-patching` | Activation Patching and Causal Interventions | W6 S2-S4 | The Core Idea (clean/corrupted framework, the patching operation, choosing a metric), Noising vs Denoising (the central distinction, AND/OR gate analogy), Interpreting Patching Results | 3 PNGs | intermediate | EXISTS (pilot) -- review for completeness |
| 10 | `attribution-patching` | Attribution Patching and Path Patching | W6 S5-S6 | Attribution Patching (gradient approximation, efficiency gain, when it works/breaks), Path Patching (beyond component outputs, from components to edges, applications) | None | intermediate | NEW |
| 11 | `ioi-circuit` | The IOI Circuit: Discovery and Mechanism | W7 S2-S7 | The IOI Task, The Human-Readable Algorithm, The Discovery Methodology (backward tracing), Name Mover Heads, Duplicate Token Heads and Induction Heads, S-Inhibition Heads | 1 PNG | advanced | NEW |
| 12 | `circuit-evaluation` | Circuit Evaluation: Faithfulness, Completeness, and Minimality | W8 S2-S8 | Negative Name Movers (loss hedging), Backup Name Movers (redundancy), The Full Circuit Diagram, Evaluating Circuits (three criteria, tensions), Causal Scrubbing (strengths and limitations), Lessons from IOI, Limitations of Circuit Analysis | None | advanced | NEW |

**Rationale:** The existing activation-patching pilot covers the core idea and noising/denoising well. Attribution patching and path patching are distinct enough methods (gradient approximation vs edge-level analysis) to warrant their own article -- this also keeps the pilot article at a reasonable length. Weeks 7-8 split into the IOI circuit discovery (the algorithm and its components) and circuit evaluation (the meta-question of how we know a circuit is correct). This split is natural: Week 7 is "here is the circuit" and Week 8 is "here is how we evaluate circuits plus the surprising components." The evaluation article includes Negative Name Movers and Backup Name Movers because they are motivated by the evaluation criteria, not the discovery.

**Block 4: Superposition and Feature Extraction (5 articles, from Weeks 9-11)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 13 | `superposition` | The Superposition Hypothesis | W9 S2-S8 | The Fundamental Tension, The Toy Model Setup, Phase Diagrams, The Geometry of Superposition, Interference and Its Cost, Why Superposition Makes MI Hard | 6 PNGs | intermediate | EXISTS (pilot) -- review for completeness |
| 14 | `sparse-autoencoders` | Sparse Autoencoders: Decomposing Superposition | W10 S2-S5 | From Superposition to Dictionary Learning, The SAE Architecture, Training SAEs, Towards Monosemanticity (results) | 1 PNG | intermediate | NEW |
| 15 | `sae-interpretability` | Feature Dashboards and Automated Interpretability | W10 S6-S7 | Feature Dashboards (what monosemantic features look like, interpretation), Automated Interpretability (using LLMs to describe features) | None | intermediate | NEW |
| 16 | `scaling-monosemanticity` | Scaling Monosemanticity and Feature Steering | W11 S2-S4 | Scaling Monosemanticity (from 4,096 to 34M features, multilingual, multimodal, abstract features, feature hierarchies, scaling laws), Golden Gate Claude (feature steering, the technical mechanism, causal implications), Safety-Relevant Features | None | advanced | NEW |
| 17 | `sae-variants-and-evaluation` | SAE Variants, Evaluation, and Honest Limitations | W11 S5-S8 | The L1 Problem and SAE Variants (Gated SAEs, TopK SAEs, JumpReLU SAEs), Evaluating SAEs (SAEBench, the key finding), Honest Limitations (feature absorption, feature splitting, dead features, non-uniqueness, interpretability illusions) | None | advanced | NEW |

**Rationale:** Week 9 stays as one article (the pilot). Week 10 splits into the SAE architecture/training article and a feature dashboards/automated interpretability article -- these are conceptually different (how SAEs work vs. how we interpret what SAEs find). Week 11 is the richest week with two clear halves: the scaling results + Golden Gate Claude (the exciting discoveries) and the SAE variants + evaluation + limitations (the critical assessment). Each half is substantial enough for its own article.

**Block 5: Representation Engineering and Steering (5 articles, from Week 12)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 18 | `activation-engineering` | Activation Engineering: ActAdd and Contrastive Activation Addition | W12 S2-S3 | Activation Engineering and ActAdd (the paradigm, the method, the math, key properties), Contrastive Activation Addition (CAA: the improvement, layer-specific effects, sycophancy steering, additivity) | 1 PNG | advanced | NEW |
| 19 | `representation-engineering` | Representation Engineering | W12 S4 | RepE (the paradigm shift, two components: representation reading via LAT and representation control, safety applications, how ActAdd/CAA fit within RepE) | None | advanced | NEW |
| 20 | `refusal-direction` | The Refusal Direction | W12 S5-S6 | The Refusal Direction (hypothesis, computing the direction, connection to CAA), The Refusal Direction Experiment (ablation, addition, capability preservation, dual-use, implications) | 2 PNGs | advanced | NEW |
| 21 | `concept-erasure` | Concept Erasure with LEACE | W12 S7 | LEACE (the complement to steering, the guarantee, concept scrubbing, the complete representation toolkit) | None | advanced | NEW |
| 22 | `function-vectors` | Function Vectors | W12 S8 | Function Vectors (from engineered to natural directions, what they are, robustness and composability) | None | advanced | NEW |

**Rationale:** Week 12 is the most content-dense week with 6 distinct techniques. ActAdd and CAA are tightly coupled (CAA is the direct improvement of ActAdd) so they share an article. RepE is the broader theoretical framework -- it gets its own article because it is conceptually distinct from the specific methods. The refusal direction is a self-contained case study with its own experiment and diagrams, clearly its own article. LEACE is a distinct method (erasure vs. addition). Function vectors are a distinct concept. This gives 5 articles from one week, which is appropriate given the content density.

**Block 6: Circuit Tracing and Comparative MI (5 articles, from Weeks 13-14)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 23 | `transcoders` | Transcoders: Interpretable MLP Replacements | W13 S2 | Transcoders (replacing MLPs with sparse interpretable layers, transcoders vs SAEs for interpretability) | 1 PNG | advanced | NEW |
| 24 | `circuit-tracing` | Circuit Tracing and Attribution Graphs | W13 S3-S5 | Sparse Feature Circuits (SAE features as circuit nodes), Attribution Graphs (feature-level circuit tracing with cross-layer transcoders, the evolution from head-level to feature-level), Case Studies from the Biology Paper | 2 PNGs | advanced | NEW |
| 25 | `crosscoders-and-model-diffing` | Crosscoders and Model Diffing | W14 S2-S3 | Crosscoders (shared dictionaries across models, the architecture), Model Diffing (what fine-tuning changes, shared vs exclusive features, connection to refusal direction, limitations) | 1 PNG | advanced | NEW |
| 26 | `universality` | Universality Across Models | W14 S4-S5 | Representation Similarity Metrics (CKA, SVCCA), Universality Revisited (training universality, scale universality, architecture universality, weak vs strong universality) | 1 PNG | advanced | NEW |
| 27 | `multimodal-mi` | Multimodal Mechanistic Interpretability | W14 S6-S7 | CLIP Interpretability (SAEs for vision, steering in vision), Generative VLMs, Diffusion Model Interpretability (circuit analysis, concept evolution), The State of the Field | None | advanced | NEW |

**Rationale:** Week 13 splits into transcoders (a specific architecture) and circuit tracing/attribution graphs (the broader methodology + case studies). These are related but the transcoder is a self-contained concept while attribution graphs are the application. Week 14 splits into three articles: crosscoders/model diffing (comparing models), universality (a theoretical question with its own evidence base), and multimodal MI (extending beyond text). These three sub-topics of Week 14 are loosely connected and each has enough substance for its own article.

**Block 7: MI for AI Safety (4 articles, from Week 15)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 28 | `sleeper-agent-detection` | Detecting Sleeper Agents with Mechanistic Interpretability | W15 S3 | The Sleeper Agent Threat Model, Backdoors Survive Safety Training, The Detection Result, Connection to Course Themes, CRITICAL LIMITATION: Trained vs Natural Deception | 1 PNG | advanced | NEW |
| 29 | `deception-detection` | Deception Detection and Alignment Faking | W15 S4 | What is Alignment Faking?, The Empirical Evidence, Why Behavioral Evaluations Fail, Detection Efforts (internal probes work), SAEs Fail as Deception Detectors | None | advanced | NEW |
| 30 | `safety-mechanisms-and-monitoring` | Understanding Safety Mechanisms and MI-Based Monitoring | W15 S5-S6 | The Refusal Direction Recap, Model Organisms for Emergent Misalignment, Detection and Steering via the Misalignment Direction, Sabotage Evaluations, Attribution Graph Inspection, Limitations | 2 PNGs | advanced | NEW |
| 31 | `mi-safety-limitations` | Honest Limitations of MI for Safety | W15 S7-S8 | The Scorecard, Interpretability Illusions, Non-Identifiability, SAEs Discard Safety-Relevant Information, Scalability, Per-Input vs Global Understanding, The Dual-Use Problem, The Gap | None | advanced | NEW |

**Rationale:** Week 15 covers four distinct safety applications plus a limitations section. Sleeper agent detection is a self-contained case study with its own diagram. Deception detection / alignment faking is a different threat model with different methods. Safety mechanisms and monitoring are the "control" applications (refusal direction + monitoring). The honest limitations section is substantial enough and important enough to be its own article -- it synthesizes all the caveats and is the critical counterpoint to the applications.

**Block 8: Open Problems and Field Assessment (4 articles, from Week 16)**

| # | Article Slug | Title | Source | Typst Sections | Diagrams | Difficulty | Status |
|---|-------------|-------|--------|----------------|----------|------------|--------|
| 32 | `open-problems-methods` | Open Problems in Mechanistic Interpretability: Methods | W16 S2-S4 | Open Problems: Methods and Foundations (decomposition, description, validation), Concrete Research Questions (Nanda's 200 problems, circuits in the wild, interpreting algorithmic problems, exploring superposition, techniques and tooling, studying learned features) | 1 PNG | advanced | NEW |
| 33 | `field-assessment` | What MI Can and Cannot Do: A Field Assessment | W16 S5-S7 | What MI Can Do (finding features, demonstrating causal effects, tracing circuits), What MI Cannot Do (global circuit descriptions, scalable verification, completeness guarantees, detecting natural deception), The Criticism and the Response (Hendrycks, DeepMind pivot, Nanda's assessment, continued investment) | 1 PNG | advanced | NEW |
| 34 | `future-directions` | The Future of Mechanistic Interpretability | W16 S8-S9 | Future Directions: Research Frontiers (intrinsic interpretability, automated MI, beyond transformers, formal verification, cognitive interpretability), Career Paths and Getting Started | None | advanced | NEW |
| 35 | `course-synthesis` | Course Synthesis: From Transformers to Interpretability | W16 S10 | The Core Insight, The Toolkit We Built, What Remains, The Honest Assessment, What You Now Know | None | advanced | NEW |

**Rationale:** Week 16 is the synthesis week with four distinct modes: surveying open problems, assessing the field honestly, looking ahead to future directions, and synthesizing the full course. Each mode is a different article. The course synthesis article serves as a capstone that ties everything together and can function as a "start here if you want an overview" entry point.

### Full Inventory Summary

| Block | Block Title | Articles | New | Exists | Source Weeks |
|-------|-------------|----------|-----|--------|-------------|
| 1 | Transformer Foundations | 3 | 2 | 1 | W1-W2 |
| 2 | Foundations of MI | 5 | 5 | 0 | W3-W5 |
| 3 | From Observation to Causation | 4 | 3 | 1 | W6-W8 |
| 4 | Superposition & Feature Extraction | 5 | 4 | 1 | W9-W11 |
| 5 | Representation Engineering & Steering | 5 | 5 | 0 | W12 |
| 6 | Circuit Tracing & Comparative MI | 5 | 5 | 0 | W13-W14 |
| 7 | MI for AI Safety | 4 | 4 | 0 | W15 |
| 8 | Open Problems & Field Assessment | 4 | 4 | 0 | W16 |
| **Total** | | **35** | **32** | **3** | **W1-W16** |

### Pilot Article Completeness Assessment

The 3 pilot articles were written as simplified introductions, not as complete week conversions. Assessment:

**attention-mechanism (Week 1):**
- Covers: attention mechanism, QK/V, multi-head attention, residual stream, layer norm, MLPs, positional encodings, full transformer
- Missing from Week 1: Prerequisites recap (ML/DL basics, linear algebra review) -- these are prerequisites, so omitting is correct for a web article
- Assessment: **Likely complete.** The pilot covers all major Week 1 topics. Minor review needed.

**activation-patching (Week 6):**
- Covers: clean/corrupted framework, noising/denoising, worked IOI example, attribution patching, path patching
- Now: attribution patching and path patching will move to a separate article (`attribution-patching`). The pilot should be reviewed for completeness of the core activation patching content (clean/corrupted, noising/denoising, interpreting results).
- Assessment: **Needs adjustment.** The pilot may cover more than the new scope (activation patching only) or may need attribution/path patching content extracted into the new companion article.

**superposition (Week 9):**
- Covers: fundamental tension, toy model, phase diagrams, geometry (all polytopes), interference, why it makes MI hard
- Missing from Week 9: No obvious gaps
- Assessment: **Likely complete.** The pilot article is comprehensive. Minor review needed.

## Reference Inventory

### Current references.json (11 entries)
Already in references.json:
1. `alammar2018illustrated`
2. `bricken2023monosemanticity`
3. `conmy2023ioi`
4. `elhage2021mathematical`
5. `elhage2022toy`
6. `heimersheim2024patching`
7. `nanda2023attribution`
8. `olah2020zoom`
9. `olsson2022context`
10. `vaswani2017attention`
11. `wang2022ioi`

### References to add (~32 new entries)

The following references from SOURCES.md need to be added to references.json. Each needs: key, title, authors, year, venue, url.

**For Block 2 articles (weeks 3-5):**
- `bereska2024review` - Bereska & Gavves (2024), MI for AI Safety survey
- `nostalgebraist2020logitlens` - nostalgebraist (2020), Logit Lens blog post
- `belrose2023tunedlens` - Belrose et al. (2023), Tuned Lens
- `hewitt2019structural` - Hewitt & Manning (2019), Structural Probes
- `voita2020mdl` - Voita & Titov (2020), MDL Probing
- `elazar2021amnesic` - Elazar et al. (2021), Amnesic Probing
- `belinkov2022probing` - Belinkov (2022), Probing Classifiers survey

**For Block 3 articles (weeks 7-8):**
- `chan2022causalscrubbing` - Chan et al. (2022), Causal Scrubbing

**For Block 4 articles (weeks 10-11):**
- `templeton2024scaling` - Templeton et al. (2024), Scaling Monosemanticity
- `rajamanoharan2024gated` - Rajamanoharan et al. (2024), Gated SAEs
- `gao2024scaling` - Gao et al. (2024), Scaling and Evaluating SAEs
- `rajamanoharan2024jumprelu` - Rajamanoharan et al. (2024), JumpReLU SAEs
- `karvonen2025saebench` - Karvonen et al. (2025), SAEBench
- `chanin2024absorption` - Chanin et al. (2024), Feature Absorption

**For Block 5 articles (week 12):**
- `turner2024steering` - Turner et al. (2024), Activation Engineering / ActAdd
- `panickssery2024caa` - Panickssery et al. (2024), Contrastive Activation Addition
- `zou2023repe` - Zou et al. (2023), Representation Engineering
- `arditi2024refusal` - Arditi et al. (2024), Refusal Direction
- `todd2024function` - Todd et al. (2024), Function Vectors
- `belrose2023leace` - Belrose et al. (2023), LEACE

**For Block 6 articles (weeks 13-14):**
- `lindsey2025circuittracing` - Lindsey et al. (2025), Circuit Tracing
- `anthropic2025biology` - Anthropic (2025), Biology of an LLM
- `dunefsky2024transcoders` - Dunefsky et al. (2024), Transcoders
- `marks2024sparse` - Marks et al. (2024), Sparse Feature Circuits
- `gurnee2024universal` - Gurnee et al. (2024), Universal Neurons
- `lin2025multimodal` - Lin et al. (2025), Multimodal MI Survey

**For Block 7 articles (week 15):**
- `hubinger2024sleeper` - Hubinger et al. (2024), Sleeper Agents
- `greenblatt2024alignment` - Greenblatt et al. (2024), Alignment Faking
- `anthropic2024probes` - Anthropic (2024), Probes Catch Sleeper Agents

**For Block 8 articles (week 16):**
- `nanda2022openproblems` - Nanda (2022), 200 Concrete Open Problems
- `sharkey2025openproblems` - Sharkey et al. (2025), Open Problems in MI

**Supplementary (may be referenced across articles):**
- `rai2024practical` - Rai et al. (2024), Practical Review of MI
- `park2023lrh` - Park et al. (2023), Linear Representation Hypothesis (if cited)

**Total new references: ~32 (exact count depends on which supplementary references are actually cited)**

Each reference entry has this JSON format (from existing references.json):
```json
{
  "key": {
    "title": "Full Paper Title",
    "authors": "LastName, F., LastName, F., et al.",
    "year": 2024,
    "venue": "Conference/Journal/Blog post",
    "url": "https://..."
  }
}
```

## Diagram Placement Plan

### All 27 PNGs by Article

| Diagram File | Source Week | Target Article | Alt Text Needed | Already Placed |
|-------------|-----------|----------------|-----------------|----------------|
| `attn_induction.png` | week-05 | `logit-lens-and-probing` | Induction head attention pattern | No |
| `attn_prev_token.png` | week-05 | `logit-lens-and-probing` | Previous token head attention pattern | No |
| `logit_lens_eiffel.png` | week-05 | `logit-lens-and-probing` | Logit lens output example | No |
| `act_patch_setup.png` | week-06 | `activation-patching` | Activation patching setup | Yes (pilot) |
| `act_patch_layers.png` | week-06 | `activation-patching` | Layer-by-layer patching results | Yes (pilot) |
| `act_patch_heads.png` | week-06 | `activation-patching` | Head-level patching heatmap | Yes (pilot) |
| `ioi_circuit_diagram.png` | week-08 | `circuit-evaluation` | Full IOI circuit diagram | No |
| `phase_diagram.png` | week-09 | `superposition` | Phase diagram for superposition | Yes (pilot) |
| `superposition_1d_antipodal.png` | week-09 | `superposition` | 1D antipodal encoding | Yes (pilot) |
| `superposition_2d_orthogonal.png` | week-09 | `superposition` | 2D orthogonal features | Yes (pilot) |
| `superposition_2d_pentagon.png` | week-09 | `superposition` | 2D pentagon packing | Yes (pilot) |
| `superposition_2d_triangle.png` | week-09 | `superposition` | 2D triangle packing | Yes (pilot) |
| `superposition_3d_packing.png` | week-09 | `superposition` | 3D octahedron packing | Yes (pilot) |
| `sae_architecture.png` | week-10 | `sparse-autoencoders` | SAE encoder-decoder architecture | No |
| `refusal_ablation_results.png` | week-12 | `refusal-direction` | Refusal ablation results | No |
| `refusal_direction_schematic.png` | week-12 | `refusal-direction` | Refusal direction schematic | No |
| `steering_method_comparison.png` | week-12 | `activation-engineering` | Comparison of steering methods | No |
| `attribution_graph_schematic.png` | week-13 | `circuit-tracing` | Attribution graph structure | No |
| `circuit_evolution.png` | week-13 | `circuit-tracing` | Circuit analysis evolution | No |
| `transcoder_vs_sae.png` | week-13 | `transcoders` | Transcoder vs SAE comparison | No |
| `crosscoder_model_diffing.png` | week-14 | `crosscoders-and-model-diffing` | Crosscoder model diffing | No |
| `universality_evidence.png` | week-14 | `universality` | Evidence for universality | No |
| `mi_safety_assessment.png` | week-15 | `mi-safety-limitations` | MI safety assessment | No |
| `safety_applications_overview.png` | week-15 | `safety-mechanisms-and-monitoring` | Safety applications overview | No |
| `sleeper_agent_detection.png` | week-15 | `sleeper-agent-detection` | Sleeper agent detection | No |
| `field_assessment_timeline.png` | week-16 | `field-assessment` | Field assessment timeline | No |
| `open_problems_taxonomy.png` | week-16 | `open-problems-methods` | Open problems taxonomy | No |

**Image handling:** Copy PNGs from `/Users/ivan/latex/mech-interp-course/week-XX/assets/` to `/Users/ivan/src/learn-mech-interp/src/topics/{article-slug}/images/`. The Eleventy passthrough copy rule `src/topics/*/images` already handles this.

## learningPath.json Update Plan

The final learningPath.json should contain all 8 blocks with all topics in reading order:

```json
{
  "blocks": [
    {
      "slug": "transformer-foundations",
      "title": "Transformer Foundations",
      "topics": [
        { "slug": "attention-mechanism", "title": "The Attention Mechanism" },
        { "slug": "qk-ov-circuits", "title": "QK and OV Circuits" },
        { "slug": "composition-and-virtual-heads", "title": "Composition and Virtual Attention Heads" }
      ]
    },
    {
      "slug": "foundations-of-mi",
      "title": "Foundations of MI",
      "topics": [
        { "slug": "what-is-mech-interp", "title": "What is Mechanistic Interpretability?" },
        { "slug": "linear-representation-hypothesis", "title": "The Linear Representation Hypothesis" },
        { "slug": "induction-heads", "title": "Induction Heads and In-Context Learning" },
        { "slug": "direct-logit-attribution", "title": "Direct Logit Attribution" },
        { "slug": "logit-lens-and-probing", "title": "The Logit Lens, Tuned Lens, and Probing Classifiers" }
      ]
    },
    {
      "slug": "observation-to-causation",
      "title": "From Observation to Causation",
      "topics": [
        { "slug": "activation-patching", "title": "Activation Patching and Causal Interventions" },
        { "slug": "attribution-patching", "title": "Attribution Patching and Path Patching" },
        { "slug": "ioi-circuit", "title": "The IOI Circuit: Discovery and Mechanism" },
        { "slug": "circuit-evaluation", "title": "Circuit Evaluation: Faithfulness, Completeness, and Minimality" }
      ]
    },
    {
      "slug": "superposition-and-feature-extraction",
      "title": "Superposition & Feature Extraction",
      "topics": [
        { "slug": "superposition", "title": "The Superposition Hypothesis" },
        { "slug": "sparse-autoencoders", "title": "Sparse Autoencoders: Decomposing Superposition" },
        { "slug": "sae-interpretability", "title": "Feature Dashboards and Automated Interpretability" },
        { "slug": "scaling-monosemanticity", "title": "Scaling Monosemanticity and Feature Steering" },
        { "slug": "sae-variants-and-evaluation", "title": "SAE Variants, Evaluation, and Honest Limitations" }
      ]
    },
    {
      "slug": "representation-engineering",
      "title": "Representation Engineering & Steering",
      "topics": [
        { "slug": "activation-engineering", "title": "Activation Engineering: ActAdd and CAA" },
        { "slug": "representation-engineering", "title": "Representation Engineering" },
        { "slug": "refusal-direction", "title": "The Refusal Direction" },
        { "slug": "concept-erasure", "title": "Concept Erasure with LEACE" },
        { "slug": "function-vectors", "title": "Function Vectors" }
      ]
    },
    {
      "slug": "circuit-tracing-and-comparative-mi",
      "title": "Circuit Tracing & Comparative MI",
      "topics": [
        { "slug": "transcoders", "title": "Transcoders: Interpretable MLP Replacements" },
        { "slug": "circuit-tracing", "title": "Circuit Tracing and Attribution Graphs" },
        { "slug": "crosscoders-and-model-diffing", "title": "Crosscoders and Model Diffing" },
        { "slug": "universality", "title": "Universality Across Models" },
        { "slug": "multimodal-mi", "title": "Multimodal Mechanistic Interpretability" }
      ]
    },
    {
      "slug": "mi-for-safety",
      "title": "MI for AI Safety",
      "topics": [
        { "slug": "sleeper-agent-detection", "title": "Detecting Sleeper Agents" },
        { "slug": "deception-detection", "title": "Deception Detection and Alignment Faking" },
        { "slug": "safety-mechanisms-and-monitoring", "title": "Safety Mechanisms and MI-Based Monitoring" },
        { "slug": "mi-safety-limitations", "title": "Honest Limitations of MI for Safety" }
      ]
    },
    {
      "slug": "open-problems-and-frontiers",
      "title": "Open Problems & Field Assessment",
      "topics": [
        { "slug": "open-problems-methods", "title": "Open Problems: Methods and Research Questions" },
        { "slug": "field-assessment", "title": "What MI Can and Cannot Do" },
        { "slug": "future-directions", "title": "The Future of Mechanistic Interpretability" },
        { "slug": "course-synthesis", "title": "Course Synthesis" }
      ]
    }
  ]
}
```

Note: The existing block slugs `transformer-foundations`, `superposition-and-feature-extraction`, and `observation-to-causation` are already used by the pilot articles' front matter and must be preserved. New block slugs (`foundations-of-mi`, `representation-engineering`, `circuit-tracing-and-comparative-mi`, `mi-for-safety`, `open-problems-and-frontiers`) follow the same kebab-case pattern.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Citation tooltips | Manual HTML citation markup | `{% cite "key" %}` shortcode | Auto-numbering, consistent tooltip format, references.json lookup |
| Margin notes | Manual aside elements | `{% sidenote "content" %}` shortcode | Numbered, responsive, togglable on mobile |
| Figure captions | Raw `<figure>` HTML | `![alt](src "Figure N: caption")` markdown-it-figure syntax | Consistent styling, semantic HTML |
| Table of contents | Manual anchor lists | Automatic from h2/h3 via eleventy-plugin-toc | Updates when headings change |
| Navigation ordering | Manual prev/next links | learningPath.json ordering + learningPath collection | Single source of truth for article order |
| Sidebar categories | Manual sidebar HTML | eleventyNavigation + `block` front matter | Automatic hierarchy from data |
| Heading anchor IDs | Manual id attributes | IdAttributePlugin + @sindresorhus/slugify | Consistent slug format |

**Key insight:** All content-type rendering is already built and working. The author just writes Markdown with shortcodes. No infrastructure work is needed -- only content authoring.

## Common Pitfalls

### Pitfall 1: Preserving Slide Structure in Articles
**What goes wrong:** Articles read like "Slide 1: here is a bullet. Slide 2: here is another bullet." The fragmented, incremental-reveal structure of slides produces choppy, hard-to-read articles.
**Why it happens:** Direct transcription of slide content without narrative restructuring.
**How to avoid:** Treat the Typst source as source material, not as a template. Each article section should flow as connected paragraphs. Merge related slide content into coherent narrative. A slide's 3 bullets become one paragraph with connecting sentences.
**Warning signs:** More than 2 consecutive one-sentence paragraphs. Bulleted lists longer than 5 items. Sections with no prose paragraphs.

### Pitfall 2: KaTeX vs Typst Math Syntax Mismatches
**What goes wrong:** Typst math like `W_(Q K)^h` is pasted directly into Markdown, where KaTeX does not understand Typst syntax. The math renders as errors or garbled output.
**Why it happens:** Typst and LaTeX/KaTeX have different math syntax. Typst uses `_()` for subscript groups; KaTeX uses `_{}`. Typst uses `RR` for blackboard bold; KaTeX uses `\mathbb{R}`. Custom operators from globals.typ (attn, mlp, embed) do not exist in KaTeX.
**How to avoid:** Use the conversion table in the Architecture Patterns section. Every math expression must be manually translated. Common traps: `d_"model"` -> `d_{\text{model}}`, `sum_(l=0)^(L-1)` -> `\sum_{l=0}^{L-1}`, `vx` -> `\mathbf{x}`.
**Warning signs:** Red error boxes in rendered math. `\undefined` errors in browser console. Math that looks like source code rather than formatted equations.

### Pitfall 3: Missing or Incorrect Cross-Article Links
**What goes wrong:** An article references a concept covered in another article but uses a broken link, wrong anchor, or no link at all. The reader cannot follow the cross-reference.
**Why it happens:** With 35 articles, cross-references are more numerous and easier to get wrong than with 16. Anchor IDs depend on the exact heading text, which may change.
**How to avoid:** When referencing a concept from another article, immediately add the cross-link. Use the heading text to derive the anchor: "The Residual Stream" -> `#the-residual-stream`. Test links after all articles are written.
**Warning signs:** Mentions of "as we saw earlier" or "in a previous article" without an actual link. 404 errors when clicking cross-references.

### Pitfall 4: Inconsistent Front Matter
**What goes wrong:** Articles have inconsistent or missing front matter fields. Block slugs do not match learningPath.json. Prerequisites reference non-existent articles. Difficulty levels are inconsistent.
**Why it happens:** 35 articles written over multiple sessions without checking consistency.
**How to avoid:** Define all front matter before writing content. Validate block slugs against learningPath.json. Only list prerequisites that actually exist as articles. Use the difficulty scale consistently: foundational (no MI prereqs), intermediate (builds on foundations), advanced (requires multiple prior articles).
**Warning signs:** Article not appearing in sidebar. Wrong block in breadcrumbs. "Read first" links pointing to 404s.

### Pitfall 5: Treating Pilot Articles as Complete
**What goes wrong:** The existing pilot articles (attention-mechanism, activation-patching, superposition) are assumed to fully cover their respective weeks, so week 1, 6, and 9 content is skipped during migration.
**Why it happens:** The pilots exist and render correctly, creating an illusion of completeness.
**How to avoid:** Compare each pilot article against the full Typst source for its week. Check that every major section, equation, and concept from the Typst is represented. For weeks 1, 6, and 9, the task is "review and expand if needed," not "skip." Note: activation-patching pilot may need content extracted into the new `attribution-patching` companion article.
**Warning signs:** A week's Typst has 800+ lines but the corresponding article is 200 lines. Key concepts from the syllabus learning objectives do not appear in the article.

### Pitfall 6: Reference Key Conflicts
**What goes wrong:** A new reference key conflicts with an existing one, or two references use inconsistent key formats.
**Why it happens:** No systematic key naming convention was documented; pilot articles used ad-hoc keys.
**How to avoid:** Follow the established pattern: `authorYYYYkeyword` (e.g., `elhage2022toy`, `heimersheim2024patching`). For multi-author papers, use the first author's last name. For blog posts with handles, use the handle (e.g., `nostalgebraist2020logitlens`). Check for existing keys before adding.
**Warning signs:** `[??]` rendering in articles (citation shortcode returns error for unknown key). Duplicate keys in references.json.

### Pitfall 7: Article Length Imbalance
**What goes wrong:** Some articles are 3,000 words while others are 300 words, creating an inconsistent reading experience.
**Why it happens:** With granular splitting, some topics are inherently denser than others. An article like "Function Vectors" may be shorter than "The IOI Circuit."
**How to avoid:** Target 1,000-2,500 words per article (the pilot articles are ~1,500-2,500 words). If an article is under 800 words, consider merging with a tightly-related article. If over 3,000 words, consider splitting. Shorter articles (like concept-erasure or function-vectors) are acceptable if the topic is genuinely self-contained -- 1,000 words is fine for a focused concept.
**Warning signs:** TOC has more than 8 h2 sections. Article takes more than 15 minutes to read. Article has fewer than 2 h2 sections.

### Pitfall 8: Splitting Too Aggressively
**What goes wrong:** An article covers such a narrow topic that it lacks context, forcing the reader to constantly jump between articles to follow the argument.
**Why it happens:** Mechanical splitting by section headings without considering narrative coherence.
**How to avoid:** Each article must tell a self-contained story. It should have a clear "why does this matter?" opening, the core content, and a "what does this mean?" conclusion. If an article cannot stand alone without the reader having just read the preceding article, it should be merged back. Cross-links handle prerequisites; the article body must be independently coherent.
**Warning signs:** An article that is all technical detail with no framing. An article that starts with "Continuing from the previous article..."

## Code Examples

### Creating a New Article Directory
```bash
# Create article directory with images
mkdir -p src/topics/sparse-autoencoders/images

# Copy diagrams from source
cp /Users/ivan/latex/mech-interp-course/week-10/assets/sae_architecture.png \
   src/topics/sparse-autoencoders/images/
```

### Article Front Matter Example (New Article)
```markdown
---
title: "Sparse Autoencoders: Decomposing Superposition"
description: "How sparse autoencoders learn an overcomplete dictionary of monosemantic features, decomposing the polysemantic activations that superposition creates."
prerequisites:
  - title: "The Superposition Hypothesis"
    url: "/topics/superposition/"
difficulty: "intermediate"
block: "superposition-and-feature-extraction"
category: "methods"
---
```

### Adding a Reference to references.json
```json
{
  "templeton2024scaling": {
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "authors": "Templeton, A., Conerly, T., Marcus, J., et al.",
    "year": 2024,
    "venue": "Anthropic",
    "url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
  }
}
```

### Typst-to-Markdown Conversion Example

**Typst source (from week-10):**
```typst
#definition(title: "Sparse Autoencoder")[
  A *sparse autoencoder* takes an activation vector $bold(x) in bb(R)^n$,
  projects it to a wider latent space $bold(f) in bb(R)^m$ ($m >> n$),
  applies ReLU for sparsity, and reconstructs:
  $ hat(bold(x)) = bold(W)_"dec" dot bold(f) + bold(b)_"dec" $
]
```

**Markdown output:**
```markdown
> **Sparse Autoencoder:** A sparse autoencoder takes an activation vector $\mathbf{x} \in \mathbb{R}^n$, projects it to a wider latent space $\mathbf{f} \in \mathbb{R}^m$ ($m \gg n$), applies ReLU for sparsity, and reconstructs:

$$
\hat{\mathbf{x}} = \mathbf{W}_{\text{dec}} \cdot \mathbf{f} + \mathbf{b}_{\text{dec}}
$$
```

### Image Placement Example
```markdown
![Diagram showing the SAE architecture with encoder projecting from model dimension to a wider latent space, ReLU activation, and decoder projecting back.](/topics/sparse-autoencoders/images/sae_architecture.png "Figure 1: The sparse autoencoder architecture. The encoder projects activations to a wider latent space with ReLU sparsity, and the decoder reconstructs the original activations from the sparse features.")
```

## Batch Execution Strategy

The 35 articles should be produced in 8 batches corresponding to the 8 blocks. Each batch is independently shippable:

### Batch 1: Block 1 - Transformer Foundations (3 articles)
- Review `attention-mechanism` pilot for completeness vs Week 1 Typst
- Create `qk-ov-circuits` from Week 2 sections 2-3 (QK/OV circuits, worked example)
- Create `composition-and-virtual-heads` from Week 2 sections 4-6 (V/K/Q-composition, TransformerLens)
- Add any missing references
- Update learningPath.json Block 1

### Batch 2: Block 2 - Foundations of MI (5 articles)
- Create `what-is-mech-interp` from Week 3 sections 2-3 (MI landscape, three claims)
- Create `linear-representation-hypothesis` from Week 3 sections 4-5 (LRH, polysemanticity)
- Create `induction-heads` from Week 4 sections 2-5 (one-layer/two-layer models, induction heads, phase change)
- Create `direct-logit-attribution` from Week 4 sections 6-7 (DLA, reading attention patterns)
- Create `logit-lens-and-probing` from Week 5 sections 2-7 (logit lens, tuned lens, probing, 3 PNGs)
- Add ~7 references (probing papers, logit lens, tuned lens, Bereska survey)
- Update learningPath.json Block 2

### Batch 3: Block 3 - Observation to Causation (4 articles)
- Review `activation-patching` pilot for completeness; extract attribution/path patching content if needed
- Create `attribution-patching` from Week 6 sections 5-6 (attribution patching, path patching)
- Create `ioi-circuit` from Week 7 sections 2-7 (IOI task, algorithm, discovery methodology, head classes)
- Create `circuit-evaluation` from Week 8 sections 2-8 (Negative NMs, Backup NMs, evaluation, causal scrubbing, 1 PNG)
- Add ~1 reference (causal scrubbing)
- Update learningPath.json Block 3

### Batch 4: Block 4 - Superposition and Feature Extraction (5 articles)
- Review `superposition` pilot for completeness vs Week 9 Typst
- Create `sparse-autoencoders` from Week 10 sections 2-5 (architecture, training, results, 1 PNG)
- Create `sae-interpretability` from Week 10 sections 6-7 (dashboards, automated interpretability)
- Create `scaling-monosemanticity` from Week 11 sections 2-4 (scaling, Golden Gate, safety features)
- Create `sae-variants-and-evaluation` from Week 11 sections 5-8 (variants, SAEBench, limitations)
- Add ~6 references (Templeton, SAE variant papers)
- Update learningPath.json Block 4

### Batch 5: Block 5 - Representation Engineering & Steering (5 articles)
- Create `activation-engineering` from Week 12 sections 2-3 (ActAdd, CAA, 1 PNG)
- Create `representation-engineering` from Week 12 section 4 (RepE framework)
- Create `refusal-direction` from Week 12 sections 5-6 (refusal experiment, 2 PNGs)
- Create `concept-erasure` from Week 12 section 7 (LEACE)
- Create `function-vectors` from Week 12 section 8 (function vectors)
- Add ~6 references (steering papers)
- Update learningPath.json Block 5

### Batch 6: Block 6 - Circuit Tracing & Comparative MI (5 articles)
- Create `transcoders` from Week 13 section 2 (transcoders, 1 PNG)
- Create `circuit-tracing` from Week 13 sections 3-5 (sparse feature circuits, attribution graphs, Biology case studies, 2 PNGs)
- Create `crosscoders-and-model-diffing` from Week 14 sections 2-3 (crosscoders, model diffing, 1 PNG)
- Create `universality` from Week 14 sections 4-5 (similarity metrics, universality, 1 PNG)
- Create `multimodal-mi` from Week 14 sections 6-7 (CLIP, diffusion, multimodal)
- Add ~6 references (circuit tracing, multimodal, universality)
- Update learningPath.json Block 6

### Batch 7: Block 7 - MI for AI Safety (4 articles)
- Create `sleeper-agent-detection` from Week 15 section 3 (1 PNG)
- Create `deception-detection` from Week 15 section 4
- Create `safety-mechanisms-and-monitoring` from Week 15 sections 5-6 (2 PNGs)
- Create `mi-safety-limitations` from Week 15 sections 7-8 (1 PNG)
- Add ~3 references (sleeper agents, alignment faking)
- Update learningPath.json Block 7

### Batch 8: Block 8 - Open Problems & Field Assessment (4 articles)
- Create `open-problems-methods` from Week 16 sections 2-4 (1 PNG)
- Create `field-assessment` from Week 16 sections 5-7 (1 PNG)
- Create `future-directions` from Week 16 sections 8-9
- Create `course-synthesis` from Week 16 section 10
- Add ~2 references (open problems papers)
- Update learningPath.json Block 8
- Final cross-link audit across all 35 articles

### Estimated Volume Per Batch
| Batch | Articles | New | Review | Source Lines | Diagrams | New Refs | Est. Words |
|-------|----------|-----|--------|-------------|----------|----------|------------|
| 1 | 3 | 2 | 1 | 934 | 0 | 0 | ~4,000 |
| 2 | 5 | 5 | 0 | 2,338 | 3 | ~7 | ~9,000 |
| 3 | 4 | 3 | 1 | 2,847 | 1 | ~1 | ~7,000 |
| 4 | 5 | 4 | 1 | 2,903 | 1 | ~6 | ~9,000 |
| 5 | 5 | 5 | 0 | 940 | 3 | ~6 | ~8,000 |
| 6 | 5 | 5 | 0 | 1,919 | 5 | ~6 | ~8,000 |
| 7 | 4 | 4 | 0 | 1,051 | 4 | ~3 | ~6,000 |
| 8 | 4 | 4 | 0 | 1,077 | 2 | ~2 | ~6,000 |
| **Total** | **35** | **32** | **3** | **14,744** | **27** | **~32** | **~57,000** |

## Content Conversion Checklist (Per Article)

For each article, verify:

- [ ] Front matter complete: title, description, prerequisites, difficulty, block, category
- [ ] Block slug matches learningPath.json
- [ ] Prerequisites reference only articles that exist
- [ ] All major topics from the source Typst sections are covered
- [ ] Prose is narrative (not bullet-point fragments)
- [ ] All math is valid KaTeX (no Typst syntax remnants)
- [ ] All diagrams have descriptive alt text and "Figure N:" captions
- [ ] All paper references use {% cite "key" %} with valid keys in references.json
- [ ] 2 pause-and-think prompts per article (adapted from source exercises)
- [ ] 2-4 sidenotes per article for supplementary context
- [ ] Cross-article links to related concepts (especially to prerequisite and follow-up articles)
- [ ] No slide-specific artifacts (#pause, recap sections, readings lists)
- [ ] Article slug is in learningPath.json
- [ ] Article tells a self-contained story (has framing, not just technical content)
- [ ] Renders correctly with `npx @11ty/eleventy --serve`

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Slide-per-page format | Long-form articles | Phase 4 (pilot articles) | Content must be rewritten as prose, not transcribed |
| Week-based organization | Thematic organization | Phase 4 decision | Articles grouped by topic, not by lecture week |
| 1:1 week-to-article | Granular: one article per major topic | Phase 5.1 research update | 35 articles, not 16. Each concept/method gets its own focused article |

**Deprecated/outdated:**
- The test article at `src/topics/test/index.md` exists for rendering verification only. It is not part of the learningPath and should be left as-is (not deleted, not converted).
- The original 16-article inventory (1 per week) proposed in the initial research is superseded by the 35-article granular inventory.

## Open Questions

1. **Whether the activation-patching pilot needs splitting**
   - What we know: The pilot currently covers activation patching, attribution patching, and path patching. In the new inventory, attribution patching and path patching are a separate article.
   - What's unclear: Whether the pilot's content should be left as-is with the new article covering attribution/path patching in more depth, or whether content should be extracted from the pilot.
   - Recommendation: Review the pilot. If it covers attribution/path patching superficially, leave it and let the new article go deep. If it covers them in detail, extract that content into the new article and trim the pilot to focus on core activation patching only.

2. **Article length for narrow topics**
   - What we know: Some articles cover narrow topics (concept-erasure, function-vectors, transcoders). These may be 800-1,200 words rather than the 1,500-2,500 target.
   - What's unclear: Whether short articles feel incomplete or whether they are appropriately focused.
   - Recommendation: Accept 800-1,200 words for genuinely narrow topics. A focused 1,000-word article on LEACE is better than padding it to 2,000 words with tangential content. If during writing an article feels too thin (under 800 words), merge it with a closely related article.

3. **Block naming for the pilot articles' existing front matter**
   - What we know: The pilot articles use block slugs `transformer-foundations`, `observation-to-causation`, and `superposition-and-feature-extraction`. The new 8-block structure preserves these three slugs. Two new blocks replace `advanced-topics` and `synthesis-and-frontiers` with five more specific blocks.
   - What's unclear: Whether the old `advanced-topics` slug appears anywhere that needs updating.
   - Recommendation: Check if any existing data files reference `advanced-topics` or `synthesis-and-frontiers`. If so, update them. The pilot articles' front matter should not need changes since their block slugs are preserved.

4. **Reading order within blocks**
   - What we know: learningPath.json determines reading order via array position. Within each block, articles should flow logically.
   - What's unclear: Some blocks have no strict dependency order (e.g., within Block 5, does the reader need to read RepE before the refusal direction?).
   - Recommendation: Order articles within blocks by logical dependency where it exists. For loosely ordered articles, use the order that matches the course's pedagogical flow.

## Sources

### Primary (HIGH confidence)
- Existing pilot articles at `/Users/ivan/src/learn-mech-interp/src/topics/*/index.md` -- established all patterns
- `eleventy.config.js` at `/Users/ivan/src/learn-mech-interp/eleventy.config.js` -- all rendering infrastructure
- `learningPath.json` at `/Users/ivan/src/learn-mech-interp/src/_data/learningPath.json` -- navigation data structure
- `references.json` at `/Users/ivan/src/learn-mech-interp/src/_data/references.json` -- citation data structure
- `topics.11tydata.js` at `/Users/ivan/src/learn-mech-interp/src/topics/topics.11tydata.js` -- front matter schema
- `SYLLABUS.md` at `/Users/ivan/latex/mech-interp-course/SYLLABUS.md` -- week-by-week topic coverage
- `SOURCES.md` at `/Users/ivan/latex/mech-interp-course/SOURCES.md` -- complete reference catalog
- `STATE.md` at `/Users/ivan/src/learn-mech-interp/.planning/STATE.md` -- all prior decisions
- All 16 week Typst files at `/Users/ivan/latex/mech-interp-course/week-*/week-*.typ` -- source content (all 16 files read in full for section-level topic analysis)
- `globals.typ` at `/Users/ivan/latex/mech-interp-course/globals.typ` -- math notation reference

### Secondary (MEDIUM confidence)
- Course research files at `/Users/ivan/latex/mech-interp-course/.planning/phases/*/` -- domain content organization
- Previous phase research at `/Users/ivan/src/learn-mech-interp/.planning/phases/01-foundation-deployment/01-RESEARCH.md` -- established patterns

### Tertiary (LOW confidence)
- None. This phase is purely content authoring using established infrastructure. No external tools or libraries to validate.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- no new tools or libraries needed; all infrastructure is built and verified
- Architecture patterns: HIGH -- all patterns derived from existing pilot articles and verified infrastructure
- Article inventory: HIGH -- derived from reading all 16 Typst files at the section level, mapping each section to articles by thematic coherence, and validating against the syllabus learning objectives
- Conversion patterns: HIGH -- derived from comparing Typst source to pilot article output
- Reference inventory: HIGH -- derived from complete SOURCES.md catalog
- Diagram placement: HIGH -- derived from complete PNG inventory and granular article mapping
- Batch strategy: HIGH -- follows block structure with clear dependencies
- Pitfalls: MEDIUM -- based on analysis of conversion challenges, not yet validated by execution

**Research date:** 2026-02-04
**Valid until:** 2026-06-04 (90 days -- content authoring, no library dependencies to go stale)
