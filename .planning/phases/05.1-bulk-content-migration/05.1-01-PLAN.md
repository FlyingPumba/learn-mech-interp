---
phase: 05.1-bulk-content-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/_data/references.json
  - src/_data/learningPath.json
autonomous: true

must_haves:
  truths:
    - "references.json contains all ~42 references needed for all 16 articles"
    - "learningPath.json has 6 blocks with all 16 topics in correct reading order"
    - "Existing 3 block slugs preserved (transformer-foundations, superposition-and-feature-extraction, observation-to-causation)"
  artifacts:
    - path: "src/_data/references.json"
      provides: "Complete citation data for all articles"
      contains: "templeton2024scaling"
    - path: "src/_data/learningPath.json"
      provides: "Full 6-block, 16-topic learning path"
      contains: "synthesis-and-frontiers"
  key_links:
    - from: "src/_data/references.json"
      to: "{% cite %} shortcodes in all articles"
      via: "citation key lookup"
      pattern: "bereska2024review|templeton2024scaling|lindsey2025circuittracing"
    - from: "src/_data/learningPath.json"
      to: "src/_includes/partials/sidebar.njk"
      via: "block/topic rendering"
      pattern: "blocks.*topics.*slug"
---

<objective>
Add all paper references and update the learning path to cover the full 16-article curriculum.

Purpose: Wave 2 article-writing plans need references.json to contain all citation keys and learningPath.json to have the complete structure. This data-prep plan removes shared-file contention from all downstream plans.
Output: Complete references.json (~42+ entries) and learningPath.json (6 blocks, 16 topics)
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md

Source references catalog:
@/Users/ivan/latex/mech-interp-course/SOURCES.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add all new references to references.json</name>
  <files>src/_data/references.json</files>
  <action>
Read the current references.json (11 entries) and SOURCES.md from the course repo. Add all ~32 new reference entries needed for the full article set. For each reference, provide: key, title, authors, year, venue, url.

Follow the established key format: `authorYYYYkeyword` (e.g., `templeton2024scaling`). For multi-author papers, use first author's last name. For blog posts with handles, use the handle.

References to add (grouped by which articles need them):

**Block 2 (Weeks 3-5):**
- `bereska2024review` - Bereska & Gavves (2024), "Mechanistic Interpretability for AI Safety: A Review"
- `nostalgebraist2020logitlens` - nostalgebraist (2020), "interpreting GPT: the logit lens" blog post
- `belrose2023tunedlens` - Belrose et al. (2023), "Eliciting Latent Predictions from Transformers with the Tuned Lens"
- `hewitt2019structural` - Hewitt & Manning (2019), "A Structural Probe for Finding Syntax in Word Representations"
- `voita2020mdl` - Voita & Titov (2020), "Information-Theoretic Probing with Minimum Description Length"
- `elazar2021amnesic` - Elazar et al. (2021), "Amnesic Probing: Behavioral Explanation with Uninformative Representations"
- `belinkov2022probing` - Belinkov (2022), "Probing Classifiers: Promises, Shortcomings, and Advances"

**Block 3 (Weeks 7-8):**
- `chan2022causalscrubbing` - Chan et al. (2022), "Causal Scrubbing: A Method for Rigorously Testing Interpretability Hypotheses"

**Block 4 (Weeks 10-11):**
- `templeton2024scaling` - Templeton et al. (2024), "Scaling Monosemanticity"
- `rajamanoharan2024gated` - Rajamanoharan et al. (2024), "Improving Dictionary Learning with Gated Sparse Autoencoders"
- `gao2024scaling` - Gao et al. (2024), "Scaling and Evaluating Sparse Autoencoders"
- `rajamanoharan2024jumprelu` - Rajamanoharan et al. (2024), "JumpReLU Sparse Autoencoders"
- `karvonen2025saebench` - Karvonen et al. (2025), "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders"
- `chanin2024absorption` - Chanin et al. (2024), "Feature Absorption in Sparse Autoencoders"

**Block 5 (Weeks 12-14):**
- `turner2024steering` - Turner et al. (2024), "Activation Addition: Steering Language Models Without Optimization"
- `panickssery2024caa` - Panickssery et al. (2024), "Steering Llama 2 via Contrastive Activation Addition"
- `zou2023repe` - Zou et al. (2023), "Representation Engineering: A Top-Down Approach to AI Transparency"
- `arditi2024refusal` - Arditi et al. (2024), "Refusal in Language Models Is Mediated by a Single Direction"
- `todd2024function` - Todd et al. (2024), "Function Vectors in Large Language Models"
- `belrose2023leace` - Belrose et al. (2023), "LEACE: Perfect Linear Concept Erasure in Closed Form"
- `lindsey2025circuittracing` - Lindsey et al. (2025), "Circuit Tracing: Revealing Computational Graphs in Language Models"
- `anthropic2025biology` - Anthropic (2025), "On the Biology of a Large Language Model"
- `dunefsky2024transcoders` - Dunefsky et al. (2024), "Transcoders Find Interpretable LLM Feature Circuits"
- `marks2024sparse` - Marks et al. (2024), "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models"
- `gurnee2024universal` - Gurnee et al. (2024), "Universal Neurons in GPT2 Language Models"
- `lin2025multimodal` - Lin et al. (2025), "Multimodal Mechanistic Interpretability" (survey or key paper)

**Block 6 (Weeks 15-16):**
- `hubinger2024sleeper` - Hubinger et al. (2024), "Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training"
- `greenblatt2024alignment` - Greenblatt et al. (2024), "Alignment Faking in Large Language Models"
- `anthropic2024probes` - Anthropic (2024), "Probes Catch Sleeper Agents"
- `nanda2022openproblems` - Nanda (2022), "200 Concrete Open Problems in Mechanistic Interpretability"
- `sharkey2025openproblems` - Sharkey et al. (2025), "Open Problems in Mechanistic Interpretability"

**Supplementary:**
- `rai2024practical` - Rai et al. (2024), "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models"
- `park2023lrh` - Park et al. (2023), "The Linear Representation Hypothesis and the Geometry of Large Language Models"

For each reference, look up the correct URL. Use ArXiv URLs (https://arxiv.org/abs/XXXX.XXXXX) for ArXiv papers. Use transformer-circuits.pub URLs for Anthropic papers. Use direct blog URLs for blog posts. Fetch bibtex from ArXiv where needed: `curl -s "https://arxiv.org/bibtex/XXXX.XXXXX"`.

Preserve all 11 existing entries exactly as they are. Add new entries maintaining valid JSON.
  </action>
  <verify>Run `node -e "const r = require('./src/_data/references.json'); console.log(Object.keys(r).length)"` from the project root. Count should be ~43. Also verify JSON is valid: `node -e "JSON.parse(require('fs').readFileSync('src/_data/references.json','utf8'))"` should not throw.</verify>
  <done>references.json contains all existing 11 + ~32 new entries with valid key, title, authors, year, venue, url for each. JSON parses without error.</done>
</task>

<task type="auto">
  <name>Task 2: Update learningPath.json with full 6-block structure</name>
  <files>src/_data/learningPath.json</files>
  <action>
Replace the current learningPath.json (3 blocks, 3 topics) with the complete structure from the research. The final file must have 6 blocks with 16 topics total in reading order:

```json
{
  "blocks": [
    {
      "slug": "transformer-foundations",
      "title": "Transformer Foundations",
      "topics": [
        { "slug": "attention-mechanism", "title": "The Attention Mechanism" },
        { "slug": "transformer-circuits", "title": "Transformer Circuits: QK, OV, and Composition" }
      ]
    },
    {
      "slug": "foundations-of-mi",
      "title": "Foundations of MI",
      "topics": [
        { "slug": "what-is-mech-interp", "title": "What is Mechanistic Interpretability?" },
        { "slug": "induction-heads", "title": "Induction Heads and In-Context Learning" },
        { "slug": "observational-tools", "title": "Observational Tools: Looking Inside the Model" }
      ]
    },
    {
      "slug": "observation-to-causation",
      "title": "Observation to Causation",
      "topics": [
        { "slug": "activation-patching", "title": "Activation Patching and Causal Interventions" },
        { "slug": "ioi-circuit", "title": "The IOI Circuit: A Complete Case Study" }
      ]
    },
    {
      "slug": "superposition-and-feature-extraction",
      "title": "Superposition & Feature Extraction",
      "topics": [
        { "slug": "superposition", "title": "The Superposition Hypothesis" },
        { "slug": "sparse-autoencoders", "title": "Sparse Autoencoders: Decomposing Superposition" },
        { "slug": "scaling-saes", "title": "Scaling SAEs and Feature Geometry" }
      ]
    },
    {
      "slug": "advanced-topics",
      "title": "Advanced Topics",
      "topics": [
        { "slug": "steering", "title": "Steering and Representation Engineering" },
        { "slug": "circuit-tracing", "title": "Circuit Tracing at Scale" },
        { "slug": "model-diffing-universality", "title": "Model Diffing, Universality, and Multimodal MI" }
      ]
    },
    {
      "slug": "synthesis-and-frontiers",
      "title": "Synthesis & Open Frontiers",
      "topics": [
        { "slug": "mi-safety", "title": "Mechanistic Interpretability for AI Safety" },
        { "slug": "open-problems", "title": "Open Problems and the Future of MI" }
      ]
    }
  ]
}
```

Preserve the existing block slugs exactly: `transformer-foundations`, `superposition-and-feature-extraction`, `observation-to-causation`. These are referenced by pilot article front matter.

Note: The block order has changed from the current file. The current file has transformer-foundations first, then superposition-and-feature-extraction, then observation-to-causation. The new order follows the reading path: transformer-foundations, foundations-of-mi, observation-to-causation, superposition-and-feature-extraction, advanced-topics, synthesis-and-frontiers. The pilot articles' `block` front matter field references the block slug, not its position, so reordering is safe.
  </action>
  <verify>Run `node -e "const lp = require('./src/_data/learningPath.json'); console.log(lp.blocks.length, 'blocks'); console.log(lp.blocks.reduce((a,b) => a + b.topics.length, 0), 'topics')"`. Should output "6 blocks" and "16 topics". Also verify JSON validity.</verify>
  <done>learningPath.json has 6 blocks, 16 topics, in correct reading order. Existing block slugs preserved. JSON parses without error.</done>
</task>

</tasks>

<verification>
1. `node -e "const r = require('./src/_data/references.json'); console.log(Object.keys(r).length, 'references')"` outputs ~43 references
2. `node -e "const lp = require('./src/_data/learningPath.json'); console.log(lp.blocks.length, 'blocks,', lp.blocks.reduce((a,b) => a + b.topics.length, 0), 'topics')"` outputs "6 blocks, 16 topics"
3. `npx @11ty/eleventy` builds without errors (existing articles still render)
4. Existing pilot articles still appear correctly in sidebar (block slugs preserved)
</verification>

<success_criteria>
- references.json has 42+ entries, all valid JSON, all with key/title/authors/year/venue/url
- learningPath.json has 6 blocks with 16 topics in reading order
- Eleventy build succeeds with no errors
- Sidebar renders correctly with expanded block structure
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-01-SUMMARY.md`
</output>
