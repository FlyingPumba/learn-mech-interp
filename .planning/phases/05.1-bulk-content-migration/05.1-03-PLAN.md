---
phase: 05.1-bulk-content-migration
plan: 03
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/what-is-mech-interp/index.md
  - src/topics/linear-representation-hypothesis/index.md
  - src/topics/induction-heads/index.md
autonomous: true

must_haves:
  truths:
    - "what-is-mech-interp article covers the MI landscape, black-box vs white-box, and the three claims (features, circuits, universality)"
    - "linear-representation-hypothesis article covers the LRH, why linear, empirical evidence, and polysemanticity"
    - "induction-heads article covers one-layer models, two-layer composition, the induction head mechanism, and the phase change"
    - "All three articles have difficulty: foundational or intermediate and block: foundations-of-mi"
  artifacts:
    - path: "src/topics/what-is-mech-interp/index.md"
      provides: "What is Mechanistic Interpretability? article"
      contains: "block: \"foundations-of-mi\""
    - path: "src/topics/linear-representation-hypothesis/index.md"
      provides: "Linear Representation Hypothesis article"
      contains: "block: \"foundations-of-mi\""
    - path: "src/topics/induction-heads/index.md"
      provides: "Induction Heads article"
      contains: "block: \"foundations-of-mi\""
  key_links:
    - from: "src/topics/induction-heads/index.md"
      to: "src/topics/composition-and-virtual-heads/index.md"
      via: "prerequisite link"
      pattern: "/topics/composition-and-virtual-heads/"
---

<objective>
Create 3 articles for Block 2 Part 1 (Foundations of MI): what-is-mech-interp from Week 3, linear-representation-hypothesis from Week 3, and induction-heads from Week 4.

Purpose: These articles establish the conceptual foundation of MI -- what the field is, the linear representation hypothesis, and the landmark induction head discovery.
Output: 3 new articles covering Weeks 3-4 (partial).
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md
@src/_data/references.json (updated by Plan 01)
@src/_data/learningPath.json (updated by Plan 01)

Typst source files:
- /Users/ivan/latex/mech-interp-course/week-03/week-03.typ
- /Users/ivan/latex/mech-interp-course/week-04/week-04.typ
- /Users/ivan/latex/mech-interp-course/globals.typ

Course research notes (READ THESE for deeper content, additional references, and pedagogical context beyond the Typst slides):
- /Users/ivan/latex/mech-interp-course/.planning/phases/03-core-concepts/03-RESEARCH.md (Weeks 3-5 domain research -- covers MI landscape, features, circuits, induction heads, probing)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create what-is-mech-interp and linear-representation-hypothesis articles</name>
  <files>
    src/topics/what-is-mech-interp/index.md
    src/topics/linear-representation-hypothesis/index.md
  </files>
  <action>
**Article A: what-is-mech-interp**

Create `src/topics/what-is-mech-interp/index.md` from Week 3 sections 2-3. Cover:
- The Interpretability Landscape: black-box vs white-box, post-hoc vs intrinsic, correlation vs causation
- The Three Claims of MI: features (networks represent features), circuits (features connected by weights), universality (similar features/circuits across models)
- Why MI matters for AI safety (brief motivation, not the full safety argument -- that comes in Block 7)

Front matter:
```yaml
title: "What is Mechanistic Interpretability?"
description: "The landscape of neural network interpretability approaches and the three core claims that define mechanistic interpretability as a field."
prerequisites:
  - title: "The Attention Mechanism"
    url: "/topics/attention-mechanism/"
difficulty: "foundational"
block: "foundations-of-mi"
category: "core-concepts"
```

References: bereska2024review, olah2020zoom. Cross-link to attention-mechanism (prereq), forward to linear-representation-hypothesis and superposition.

**Article B: linear-representation-hypothesis**

Create `src/topics/linear-representation-hypothesis/index.md` from Week 3 sections 4-5. Cover:
- The Linear Representation Hypothesis: features as directions in activation space
- Why linear? (computational tractability, empirical success, the linearity prior)
- Empirical evidence for LRH (probing results, feature visualization)
- Connection to superposition (foreshadowing)
- Polysemanticity: Why Neurons Fail (individual neurons are polysemantic, features != neurons)

Front matter:
```yaml
title: "The Linear Representation Hypothesis"
description: "Why neural networks appear to represent concepts as linear directions in activation space, and why individual neurons fail as units of analysis."
prerequisites:
  - title: "What is Mechanistic Interpretability?"
    url: "/topics/what-is-mech-interp/"
difficulty: "foundational"
block: "foundations-of-mi"
category: "core-concepts"
```

References: park2023lrh, olah2020zoom, elhage2022toy (for superposition foreshadowing). Cross-link forward to superposition.

Content guidelines for both articles:
- Target 1,500-2,500 words each
- Convert Typst math to KaTeX using the conversion table
- Blockquote definitions for formal terms
- 2 pause-and-think prompts per article
- 2-4 sidenotes per article
- Narrative prose, not bullet fragments
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify both files exist with valid front matter.
  </verify>
  <done>what-is-mech-interp and linear-representation-hypothesis articles exist with complete front matter, narrative prose, citations, and cross-links.</done>
</task>

<task type="auto">
  <name>Task 2: Create induction-heads article</name>
  <files>src/topics/induction-heads/index.md</files>
  <action>
Create `src/topics/induction-heads/index.md` from Week 4 sections 2-5. Cover:

- One-layer models: end-to-end QK/OV analysis, what a single attention layer can compute
- Two-layer models: composition in action, how the second layer builds on the first
- Induction Heads: the discovery, the two-step mechanism ([A][B]...[A] -> predict [B]), the previous-token head + induction head pair
- The phase change: the sudden emergence of induction heads during training, connection to in-context learning

This is one of the most important articles in the course as it demonstrates the circuit analysis methodology on a real, discovered circuit.

Front matter:
```yaml
title: "Induction Heads and In-Context Learning"
description: "How the discovery of induction heads revealed a two-step circuit for in-context learning, demonstrating that compositional circuits emerge during training."
prerequisites:
  - title: "Composition and Virtual Attention Heads"
    url: "/topics/composition-and-virtual-heads/"
difficulty: "intermediate"
block: "foundations-of-mi"
category: "core-concepts"
```

References: olsson2022context (primary), elhage2021mathematical. Cross-link to composition-and-virtual-heads (where composition was introduced), forward to direct-logit-attribution and ioi-circuit.

Content guidelines:
- Target 1,500-2,500 words
- Convert Typst math to KaTeX
- Blockquote definitions (e.g., Induction Head, Previous Token Head)
- 2 pause-and-think prompts (the induction pattern is great for "can you trace the attention pattern?" exercises)
- 2-4 sidenotes
- Narrative prose explaining the discovery as a story
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify file exists with valid front matter.
  </verify>
  <done>induction-heads article exists with complete front matter, narrative prose covering the induction head discovery and mechanism, KaTeX math, citations, and cross-links.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy --dryrun` succeeds
2. All 3 articles exist with valid front matter (block: "foundations-of-mi")
3. Each article has 2 pause-and-think prompts, 2+ sidenotes, 1+ citations
4. Cross-links connect appropriately: what-is-mech-interp -> linear-representation-hypothesis -> superposition (forward), induction-heads -> composition-and-virtual-heads (back)
</verification>

<success_criteria>
- 3 new articles covering Week 3 (sections 2-5) and Week 4 (sections 2-5)
- Each article is 1,500-2,500 words of narrative prose
- All front matter valid with correct block slug
- Build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-03-SUMMARY.md`
</output>
