---
phase: 05.1-bulk-content-migration
plan: 03
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/what-is-mech-interp/index.md
  - src/topics/induction-heads/index.md
autonomous: true

must_haves:
  truths:
    - "what-is-mech-interp article explains the MI landscape, features, circuits, universality, and the Linear Representation Hypothesis"
    - "induction-heads article covers the induction head mechanism, mathematical framework, direct logit attribution, and in-context learning"
    - "Both articles have complete front matter, valid KaTeX math, and working citations"
  artifacts:
    - path: "src/topics/what-is-mech-interp/index.md"
      provides: "Week 3 content as narrative article"
      contains: "block: \"foundations-of-mi\""
    - path: "src/topics/induction-heads/index.md"
      provides: "Week 4 content as narrative article"
      contains: "block: \"foundations-of-mi\""
  key_links:
    - from: "src/topics/what-is-mech-interp/index.md"
      to: "src/topics/attention-mechanism/index.md"
      via: "prerequisite cross-link"
      pattern: "/topics/attention-mechanism/"
    - from: "src/topics/induction-heads/index.md"
      to: "src/topics/transformer-circuits/index.md"
      via: "prerequisite cross-link"
      pattern: "/topics/transformer-circuits/"
---

<objective>
Create Block 2 Part 1: the "What is Mechanistic Interpretability?" and "Induction Heads" articles from Weeks 3-4.

Purpose: These two articles establish the conceptual foundation of MI and introduce the canonical circuit discovery (induction heads). They bridge from transformer architecture knowledge to MI-specific concepts and methods.
Output: 2 new articles covering Weeks 3-4
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md

Source content:
@/Users/ivan/latex/mech-interp-course/week-03/week-03.typ
@/Users/ivan/latex/mech-interp-course/week-04/week-04.typ
@/Users/ivan/latex/mech-interp-course/globals.typ

Data files (already updated by Plan 01):
@src/_data/references.json
@src/_data/learningPath.json

Existing articles (for cross-linking):
@src/topics/attention-mechanism/index.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create what-is-mech-interp article from Week 3</name>
  <files>src/topics/what-is-mech-interp/index.md</files>
  <action>
Create directory and article:
```bash
mkdir -p src/topics/what-is-mech-interp
```

Convert week-03.typ (747 lines) into a narrative long-form article. Week 3 covers:

1. **What is Mechanistic Interpretability?** - Definition, goals, relationship to broader interpretability/XAI landscape. How MI differs from behavioral analysis.

2. **Features and Representations** - What constitutes a "feature" in MI. Polysemanticity vs monosemanticity. The Linear Representation Hypothesis (Park et al., 2023).

3. **Circuits** - What circuits are: subgraphs of the computational graph that implement specific behaviors. The Olah et al. (2020) "Zoom In" framework. Circuit-level claims.

4. **Universality** - Do different models learn the same features/circuits? Evidence for and against universality across architectures and training runs.

5. **The MI Research Landscape** - Observational vs causal methods. Bottom-up vs top-down approaches. Where the field currently stands.

Front matter:
```yaml
---
title: "What is Mechanistic Interpretability?"
description: "An introduction to the goals, concepts, and methods of mechanistic interpretability -- understanding neural networks by reverse-engineering their internal computations."
prerequisites:
  - title: "The Attention Mechanism"
    url: "/topics/attention-mechanism/"
difficulty: "foundational"
block: "foundations-of-mi"
category: "core-concepts"
---
```

Content guidelines:
- Narrative prose, not bullet fragments
- Cite: bereska2024review, olah2020zoom, park2023lrh, elhage2021mathematical (all in references.json from Plan 01)
- 2 pause-and-think prompts
- 2-4 sidenotes
- Cross-link to attention-mechanism (residual stream, attention heads)
- Cross-link forward to induction-heads, observational-tools, superposition where concepts are previewed
- No diagrams (Week 3 has no PNGs)
- Target 1,500-2,500 words
  </action>
  <verify>Run `npx @11ty/eleventy` and check build succeeds. Verify the article appears in sidebar under "Foundations of MI." Check math renders, citations resolve, cross-links work.</verify>
  <done>what-is-mech-interp article exists with complete front matter and narrative prose covering MI definition, features, circuits, universality, and landscape. All citations resolve. Cross-links work.</done>
</task>

<task type="auto">
  <name>Task 2: Create induction-heads article from Week 4</name>
  <files>src/topics/induction-heads/index.md</files>
  <action>
Create directory and article:
```bash
mkdir -p src/topics/induction-heads
```

Convert week-04.typ (790 lines) into a narrative long-form article. Week 4 covers:

1. **The Induction Head Mechanism** - What induction heads do: [A][B]...[A] -> [B] pattern completion. Previous token heads + induction heads as a two-layer circuit. How this enables in-context learning.

2. **Mathematical Framework Applied** - Using the QK/OV circuit framework from Elhage et al. (2021) to analyze induction heads. The attention pattern as a function of position and token identity.

3. **Direct Logit Attribution (DLA)** - Decomposing the model's output logits into contributions from individual attention heads and MLP layers. The logit lens connection. How DLA reveals which components contribute to specific predictions.

4. **In-Context Learning** - The Olsson et al. (2022) paper's findings: induction heads as the mechanistic basis of in-context learning. Phase change during training. Evidence from loss analysis.

5. **Beyond Simple Induction** - Fuzzy/abstract induction heads. Translation heads. The generalization from exact copy to semantic similarity.

Front matter:
```yaml
---
title: "Induction Heads and In-Context Learning"
description: "How induction heads implement in-context learning through a two-layer circuit, and what their discovery reveals about mechanistic interpretability methodology."
prerequisites:
  - title: "The Attention Mechanism"
    url: "/topics/attention-mechanism/"
  - title: "Transformer Circuits: QK, OV, and Composition"
    url: "/topics/transformer-circuits/"
difficulty: "intermediate"
block: "foundations-of-mi"
category: "core-concepts"
---
```

Content guidelines:
- Narrative prose converting slide content to flowing paragraphs
- Cite: olsson2022context, elhage2021mathematical (both already in references.json)
- Translate all math from Typst to KaTeX (attention patterns, DLA formulas, composition equations)
- 2 pause-and-think prompts (e.g., "Why do induction heads need two layers?" or "What would happen if...")
- 2-4 sidenotes
- Cross-link to attention-mechanism (attention patterns), transformer-circuits (QK/OV framework, composition)
- Cross-link forward to observational-tools (logit lens), activation-patching (causal methods)
- No diagrams (Week 4 has no PNGs)
- Target 1,500-2,500 words
  </action>
  <verify>Run `npx @11ty/eleventy` and check build succeeds. Verify the article appears in sidebar under "Foundations of MI" after what-is-mech-interp. Check math renders, citations resolve, cross-links work.</verify>
  <done>induction-heads article exists with complete front matter and narrative prose covering induction mechanism, DLA, in-context learning, and extensions. All math is valid KaTeX. Citations and cross-links work.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds without errors
2. Both articles appear in sidebar under "Foundations of MI" block
3. what-is-mech-interp appears first, induction-heads second in the block
4. No KaTeX red error boxes in either article
5. All {% cite %} shortcodes resolve (no [??] markers)
6. Cross-links to existing articles (attention-mechanism, transformer-circuits) are valid paths
</verification>

<success_criteria>
- what-is-mech-interp is a complete, readable article covering all Week 3 topics
- induction-heads is a complete, readable article covering all Week 4 topics
- Both have complete front matter, valid KaTeX, working citations, and cross-links
- Both appear correctly in sidebar under Foundations of MI
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-03-SUMMARY.md`
</output>
