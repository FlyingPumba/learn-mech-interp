---
phase: 05.1-bulk-content-migration
plan: 06
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/superposition/index.md
  - src/topics/sparse-autoencoders/index.md
  - src/topics/sparse-autoencoders/images/sae_architecture.png
  - src/topics/sae-interpretability/index.md
autonomous: true

must_haves:
  truths:
    - "superposition pilot reviewed and confirmed complete against Week 9 Typst"
    - "sparse-autoencoders article covers the SAE architecture, training, and Towards Monosemanticity results with the architecture diagram"
    - "sae-interpretability article covers feature dashboards and automated interpretability"
    - "All 3 articles have block: superposition-and-feature-extraction"
  artifacts:
    - path: "src/topics/sparse-autoencoders/index.md"
      provides: "Sparse Autoencoders article"
      contains: "block: \"superposition-and-feature-extraction\""
    - path: "src/topics/sparse-autoencoders/images/sae_architecture.png"
      provides: "SAE architecture diagram"
    - path: "src/topics/sae-interpretability/index.md"
      provides: "Feature Dashboards and Automated Interpretability article"
      contains: "block: \"superposition-and-feature-extraction\""
  key_links:
    - from: "src/topics/sparse-autoencoders/index.md"
      to: "src/topics/superposition/index.md"
      via: "prerequisite link"
      pattern: "/topics/superposition/"
---

<objective>
Create Block 4 Part 1 (Superposition & Feature Extraction): review the superposition pilot, then create sparse-autoencoders and sae-interpretability articles from Week 10.

Purpose: These articles bridge superposition (the problem) to SAEs (the solution) and show how SAE features are interpreted.
Output: 3 articles (1 reviewed pilot + 2 new) + 1 diagram.
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md
@src/topics/superposition/index.md (existing pilot -- review target)
@src/_data/references.json (updated by Plan 01)
@src/_data/learningPath.json (updated by Plan 01)

Typst source files:
- /Users/ivan/latex/mech-interp-course/week-09/week-09.typ (for pilot review)
- /Users/ivan/latex/mech-interp-course/week-10/week-10.typ (sections 2-7)
- /Users/ivan/latex/mech-interp-course/globals.typ

Course research notes (READ THESE for deeper content, additional references, and pedagogical context beyond the Typst slides):
- /Users/ivan/latex/mech-interp-course/.planning/phases/05-superposition-decomposition/05-RESEARCH.md (Weeks 9-11 domain research -- covers superposition theory, SAE architecture, monosemanticity, scaling)

Diagram to copy:
- /Users/ivan/latex/mech-interp-course/week-10/assets/sae_architecture.png
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review superposition pilot and create sparse-autoencoders article</name>
  <files>
    src/topics/superposition/index.md
    src/topics/sparse-autoencoders/index.md
    src/topics/sparse-autoencoders/images/sae_architecture.png
  </files>
  <action>
**Part A: Review superposition pilot**

Read the existing pilot at `src/topics/superposition/index.md` and compare against Week 9 Typst source. Per the research assessment, the pilot is "likely complete" covering: fundamental tension, toy model setup, phase diagrams, geometry of superposition (all polytopes), interference and its cost, why superposition makes MI hard. It already has 6 placed PNGs.

Verify: all 6 diagrams referenced, 2 pause-and-think prompts, 2+ sidenotes, front matter with block: "superposition-and-feature-extraction". Add a forward cross-link to sparse-autoencoders if not present: "The solution to superposition comes from [sparse autoencoders](/topics/sparse-autoencoders/)."

Make only minor fixes if needed. Do not rewrite the pilot.

**Part B: Create sparse-autoencoders article**

Copy the diagram:
```bash
mkdir -p src/topics/sparse-autoencoders/images
cp /Users/ivan/latex/mech-interp-course/week-10/assets/sae_architecture.png src/topics/sparse-autoencoders/images/
```

Create `src/topics/sparse-autoencoders/index.md` from Week 10 sections 2-5. Cover:

- From Superposition to Dictionary Learning: the key insight that we need an overcomplete dictionary to decompose superposed representations
- The SAE Architecture: encoder projects to wider latent space, ReLU sparsity, decoder reconstructs. The loss function (reconstruction + sparsity penalty).
- Training SAEs: training data (model activations), practical considerations, hyperparameter choices
- Towards Monosemanticity: Bricken et al.'s results demonstrating that SAE features are more monosemantic than neurons

Place the SAE architecture diagram:
`![The SAE architecture: encoder projects activations to a wider latent space, ReLU enforces sparsity, decoder reconstructs](/topics/sparse-autoencoders/images/sae_architecture.png "Figure 1: The sparse autoencoder architecture. The encoder projects d_model activations into a wider m-dimensional latent space with ReLU sparsity, and the decoder reconstructs the original activations.")`

Front matter:
```yaml
title: "Sparse Autoencoders: Decomposing Superposition"
description: "How sparse autoencoders learn an overcomplete dictionary of monosemantic features, decomposing the polysemantic activations that superposition creates."
prerequisites:
  - title: "The Superposition Hypothesis"
    url: "/topics/superposition/"
difficulty: "intermediate"
block: "superposition-and-feature-extraction"
category: "methods"
```

References: bricken2023monosemanticity (primary), elhage2022toy (for superposition connection). Cross-link to superposition (prereq), forward to sae-interpretability and scaling-monosemanticity.

Content guidelines: 1,500-2,500 words, KaTeX math (the SAE equations are central), blockquote definitions, 2 pause-and-think, 2-4 sidenotes.
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify sae_architecture.png exists. Verify both files have valid front matter.
  </verify>
  <done>Superposition pilot confirmed complete. sparse-autoencoders article created with architecture diagram, SAE math, and monosemanticity results.</done>
</task>

<task type="auto">
  <name>Task 2: Create sae-interpretability article</name>
  <files>src/topics/sae-interpretability/index.md</files>
  <action>
Create `src/topics/sae-interpretability/index.md` from Week 10 sections 6-7. Cover:

- Feature Dashboards: what monosemantic features look like when visualized (max activating examples, activation distributions, logit effects)
- Interpretation of specific features: examples of features that represent clear concepts (Golden Gate Bridge, programming languages, sentiment, etc.)
- Automated Interpretability: using LLMs to automatically describe SAE features (the Anthropic approach of having a model explain what activates a feature)
- The promise and limits of automated interpretation

Front matter:
```yaml
title: "Feature Dashboards and Automated Interpretability"
description: "How researchers visualize and interpret the features SAEs discover, from manual feature dashboards to automated LLM-based interpretation."
prerequisites:
  - title: "Sparse Autoencoders: Decomposing Superposition"
    url: "/topics/sparse-autoencoders/"
difficulty: "intermediate"
block: "superposition-and-feature-extraction"
category: "methods"
```

References: bricken2023monosemanticity. Cross-link to sparse-autoencoders (prereq), forward to scaling-monosemanticity (where automated interpretability is applied at scale).

Content guidelines: 1,500-2,000 words, narrative prose, 2 pause-and-think, 2-4 sidenotes. This article is lighter on math but heavier on concrete examples of features.
  </action>
  <verify>
Run `npx @11ty/eleventy --dryrun` -- no build errors. Verify file exists with valid front matter.
  </verify>
  <done>sae-interpretability article created covering feature dashboards and automated interpretation.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy --dryrun` succeeds
2. superposition pilot confirmed complete with forward link to sparse-autoencoders
3. sparse-autoencoders article has SAE architecture diagram placed
4. sae-interpretability article covers feature dashboards and automated interpretation
5. All 3 articles have block: "superposition-and-feature-extraction"
</verification>

<success_criteria>
- 3 articles (1 reviewed + 2 new) covering Weeks 9-10
- SAE architecture diagram placed with alt text and caption
- Narrative arc: superposition problem -> SAE solution -> interpreting features
- Build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-06-SUMMARY.md`
</output>
