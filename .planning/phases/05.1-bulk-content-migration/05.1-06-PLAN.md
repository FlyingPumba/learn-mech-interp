---
phase: 05.1-bulk-content-migration
plan: 06
type: execute
wave: 2
depends_on: ["05.1-01"]
files_modified:
  - src/topics/superposition/index.md
  - src/topics/sparse-autoencoders/index.md
  - src/topics/sparse-autoencoders/images/sae_architecture.png
  - src/topics/scaling-saes/index.md
autonomous: true

must_haves:
  truths:
    - "superposition pilot covers all Week 9 content with no significant gaps"
    - "sparse-autoencoders article explains SAE architecture, training objectives, and monosemanticity results"
    - "scaling-saes article covers scaling laws, Golden Gate Claude, SAE variants (Gated, JumpReLU), and limitations"
  artifacts:
    - path: "src/topics/superposition/index.md"
      provides: "Complete Week 9 coverage"
      contains: "block: \"superposition-and-feature-extraction\""
    - path: "src/topics/sparse-autoencoders/index.md"
      provides: "Week 10 content as narrative article"
      contains: "block: \"superposition-and-feature-extraction\""
    - path: "src/topics/scaling-saes/index.md"
      provides: "Week 11 content as narrative article"
      contains: "block: \"superposition-and-feature-extraction\""
  key_links:
    - from: "src/topics/sparse-autoencoders/index.md"
      to: "src/topics/superposition/index.md"
      via: "prerequisite cross-link"
      pattern: "/topics/superposition/"
    - from: "src/topics/scaling-saes/index.md"
      to: "src/topics/sparse-autoencoders/index.md"
      via: "prerequisite cross-link"
      pattern: "/topics/sparse-autoencoders/"
---

<objective>
Complete Block 4 (Superposition & Feature Extraction): review the superposition pilot and create the sparse-autoencoders and scaling-saes articles from Weeks 10-11.

Purpose: Block 4 covers the superposition problem and the SAE solution. The superposition pilot establishes the problem; the two new articles cover the main technical approach (SAEs) and its scaling/variants. This is one of the most active areas of MI research.
Output: 2 new articles + 1 reviewed article, covering Weeks 9-11
</objective>

<execution_context>
@/Users/ivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05.1-bulk-content-migration/05.1-RESEARCH.md

Source content:
@/Users/ivan/latex/mech-interp-course/week-09/week-09.typ
@/Users/ivan/latex/mech-interp-course/week-10/week-10.typ
@/Users/ivan/latex/mech-interp-course/week-11/week-11.typ
@/Users/ivan/latex/mech-interp-course/globals.typ

Existing article to review:
@src/topics/superposition/index.md

Data files (already updated by Plan 01):
@src/_data/references.json
@src/_data/learningPath.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review superposition pilot and create sparse-autoencoders article</name>
  <files>
    src/topics/superposition/index.md
    src/topics/sparse-autoencoders/index.md
    src/topics/sparse-autoencoders/images/sae_architecture.png
  </files>
  <action>
**Part A: Review superposition pilot**

Compare the existing superposition pilot article (~160 lines) against week-09.typ (1077 lines). The pilot covers the toy model, phase diagrams, geometric packing, and interference. The research says it is "likely complete."

Check these Week 9 topics are covered:
- The fundamental tension (more features than dimensions)
- Toy model setup (ReLU hidden layer, importance-weighted features)
- Phase transitions (dense -> sparse superposition as sparsity increases)
- Geometric packing (antipodal, triangle, pentagon, octahedron)
- Interference and loss
- Why superposition makes MI hard

If complete, make minimal changes. Add cross-links to sparse-autoencoders (the solution to superposition).

**Part B: Create sparse-autoencoders article**

```bash
mkdir -p src/topics/sparse-autoencoders/images
cp /Users/ivan/latex/mech-interp-course/week-10/assets/sae_architecture.png src/topics/sparse-autoencoders/images/
```

Convert week-10.typ (817 lines) into a narrative article. Week 10 covers:

1. **The Dictionary Learning Framing** - Superposition means activations are mixtures of features. SAEs learn an overcomplete dictionary to decompose these mixtures.

2. **SAE Architecture** - Encoder: $\mathbf{f} = \text{ReLU}(\mathbf{W}_{\text{enc}} \mathbf{x} + \mathbf{b}_{\text{enc}})$. Decoder: $\hat{\mathbf{x}} = \mathbf{W}_{\text{dec}} \mathbf{f} + \mathbf{b}_{\text{dec}}$. Overcomplete basis ($m \gg n$). Tied vs untied weights.

3. **Training Objective** - Reconstruction loss + L1 sparsity penalty. The L1 coefficient as a knob between reconstruction quality and sparsity. Dead features problem.

4. **Towards Monosemanticity** - Bricken et al. (2023) results: extracting interpretable, monosemantic features from MLP activations. Evidence that SAE features correspond to human-understandable concepts.

5. **Practical Considerations** - Where to apply SAEs (residual stream, MLP, attention). Feature dashboard inspection. Automated interpretability.

Front matter for sparse-autoencoders:
```yaml
---
title: "Sparse Autoencoders: Decomposing Superposition"
description: "How sparse autoencoders learn an overcomplete dictionary of monosemantic features, decomposing the polysemantic activations that superposition creates."
prerequisites:
  - title: "The Superposition Hypothesis"
    url: "/topics/superposition/"
difficulty: "intermediate"
block: "superposition-and-feature-extraction"
category: "methods"
---
```

Image placement:
```markdown
![Diagram showing the SAE architecture with encoder projecting from model dimension to a wider latent space, ReLU activation, and decoder projecting back](/topics/sparse-autoencoders/images/sae_architecture.png "Figure 1: The sparse autoencoder architecture. The encoder projects activations to a wider latent space with ReLU sparsity, and the decoder reconstructs the original activations from the sparse features.")
```

Content guidelines for sparse-autoencoders:
- Cite: bricken2023monosemanticity, elhage2022toy (both in references.json)
- Translate all math from Typst to KaTeX
- 2 pause-and-think prompts
- 2-4 sidenotes
- Cross-link to superposition (the problem SAEs solve)
- Cross-link forward to scaling-saes (scaling and variants)
- Target 1,500-2,500 words
  </action>
  <verify>Run `npx @11ty/eleventy`. Verify superposition article renders correctly with all 6 existing images. Verify sparse-autoencoders appears in sidebar, SAE architecture diagram renders, citations resolve.</verify>
  <done>superposition covers all Week 9 content. sparse-autoencoders is complete with SAE architecture diagram, covering dictionary learning, architecture, training, and monosemanticity results. Both have working cross-links.</done>
</task>

<task type="auto">
  <name>Task 2: Create scaling-saes article from Week 11</name>
  <files>src/topics/scaling-saes/index.md</files>
  <action>
Create directory and article:
```bash
mkdir -p src/topics/scaling-saes
```

Convert week-11.typ (1086 lines) into a narrative article. Week 11 covers:

1. **Scaling Monosemanticity** - Templeton et al. (2024) results on Claude 3 Sonnet. Finding interpretable features at scale (the Golden Gate Bridge feature). Multi-level features across abstraction.

2. **Golden Gate Claude** - The viral demonstration: clamping a single feature to steer model behavior. What this shows about feature interpretability and control.

3. **SAE Evaluation** - How to evaluate SAE quality: reconstruction fidelity (loss recovered), sparsity metrics (L0), feature interpretability. The SAEBench benchmark (Karvonen et al., 2025).

4. **SAE Variants:**
   - Gated SAEs (Rajamanoharan et al., 2024) - Separate gating and magnitude estimation
   - JumpReLU SAEs (Rajamanoharan et al., 2024) - Learnable threshold for activation
   - TopK SAEs (Gao et al., 2024) - Fixed sparsity via top-k selection

5. **Limitations and Challenges** - Feature absorption (Chanin et al., 2024). Feature splitting. The gap between reconstruction quality and interpretability. Whether SAEs actually find "the" features.

Front matter:
```yaml
---
title: "Scaling SAEs and Feature Geometry"
description: "How sparse autoencoders scale to frontier models, the SAE variants that improve training, and the open challenges of feature evaluation and absorption."
prerequisites:
  - title: "Sparse Autoencoders: Decomposing Superposition"
    url: "/topics/sparse-autoencoders/"
difficulty: "advanced"
block: "superposition-and-feature-extraction"
category: "methods"
---
```

Content guidelines:
- Cite: templeton2024scaling, gao2024scaling, rajamanoharan2024gated, rajamanoharan2024jumprelu, karvonen2025saebench, chanin2024absorption (all in references.json)
- No diagrams (Week 11 has no PNGs)
- 2 pause-and-think prompts
- 2-4 sidenotes
- Cross-link to sparse-autoencoders (base SAE architecture), superposition (the underlying problem)
- Cross-link forward to circuit-tracing (SAEs used in circuit analysis), steering (feature-based steering)
- Target 1,500-2,500 words
  </action>
  <verify>Run `npx @11ty/eleventy`. Verify scaling-saes appears in sidebar under "Superposition & Feature Extraction." Check citations resolve (templeton2024scaling, etc.). Check math renders.</verify>
  <done>scaling-saes article exists with complete front matter covering scaling results, Golden Gate Claude, SAE evaluation, variants (Gated, JumpReLU, TopK), and limitations. All citations and cross-links work.</done>
</task>

</tasks>

<verification>
1. `npx @11ty/eleventy` builds without errors
2. All 3 articles appear in sidebar under "Superposition & Feature Extraction" in order: superposition, sparse-autoencoders, scaling-saes
3. SAE architecture diagram renders in sparse-autoencoders
4. 6 existing superposition diagrams still render
5. No KaTeX errors, no [??] citation markers
6. Prev/next navigation flows through all 3 Block 4 articles
</verification>

<success_criteria>
- superposition covers all Week 9 content (verified against Typst source)
- sparse-autoencoders is complete with diagram covering all Week 10 topics
- scaling-saes is complete covering all Week 11 topics
- All have complete front matter, valid KaTeX, working citations and cross-links
</success_criteria>

<output>
After completion, create `.planning/phases/05.1-bulk-content-migration/05.1-06-SUMMARY.md`
</output>
